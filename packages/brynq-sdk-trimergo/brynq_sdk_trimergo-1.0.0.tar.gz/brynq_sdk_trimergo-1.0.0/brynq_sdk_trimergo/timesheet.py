"""timesheet.py — Hybrid client for Trimergo timesheets.
Hybrid in the sense that it supports both pythonic keyword arguments or a PageInfo object for precise query construction.
Client because it is a client for the Trimergo timesheet API.

This module provides the Timesheet class, which allows interaction with the
Trimergo timesheet API. It supports querying timesheets using either legacy
keyword arguments or a PageInfo object that mirrors Trimergo's JavaScript
PageInfo for precise query construction.

Key Goals:
1.  Easy and flexible to use: The client is easy to use in both a pythonic way and a more close to API way.
2.  Consistent query generation: Utilizes a Python port of Trimergo's
    PageInfo, Criterion, and Sorter to ensure query payloads match those
    generated by browser interactions.
3.  Advanced usage: Offers direct access to the PageInfo builder for crafting
    complex AND/OR filter combinations.

Usage Examples:
Pythonic Keyword Arguments (kwargs):
```python
from trimergo_client import Trimergo # Assuming Trimergo client is available
tgo_client = Trimergo(interface_id='1', test_environment=False, system_type='source')
sheet = Timesheet(tgo_client)
df = sheet.get(tdate__gt="20250101")
```

PageInfo Builder (similar to Trimergo's JS documentation):
```python
from .pageinfo import PageInfo, Criterion # Assuming pageinfo.py is in the same directory

pi = PageInfo()
pi.addCriterion(Criterion(field="last_name", operator="=", value="Doe"))
# pi.addSorter(Sorter(field="last_name", direction=Sorter.ASC)) # Sorter not used in this file
df = sheet.get(pageinfo=pi)
```
"""
# Standard library imports
from __future__ import annotations
import re
import urllib.parse
from typing import Any, Dict, List, Optional

# Third-party imports
import pandas as pd
import requests

# Local imports
from .schemas.timesheet import TimeSheetSchema
from . pageinfo_wrapper.pageinfo import PageInfo, Criterion
from brynq_sdk_functions import Functions

class Timesheet:
    """Service wrapper for Trimergo timesheets with dual query styles."""
    _OP_SUFFIX: Dict[str, str] = {
        "eq": "=",
        "neq": "!=",
        "gt": ">",
        "lt": "<",
        "gte": ">=",
        "lte": "<=",
        "contains": "LIKE",
        "startswith": "STARTSWITH",
        "endswith": "ENDSWITH",
    }

    _DEFAULT_FIELD_STRINGS: List[str] = [
        'ts.tdate', 'ts.time_from', 'ts.time_to', 'ts.notes',
        'ts.show_on_reportready', 'ts.amount', 'pl.user_type',
        'pl.installation_key', 'pr.type_name', 'pr.project_number',
        'pr.description', 'r.relation_key', 'wt.description',
        'res.description', 'sys.fn_varbintohexstr(ts.row_versionid)'
    ]

    def __init__(self, trimergo_client: Any) -> None:
        self._tgo: Any = trimergo_client  # expects .trimergo_host & .trimergo_session
        self._allowed: List[Dict[str, str]] = self._load_allowed_fields()

    def get(
        self,
        *,
        pageinfo: Optional[PageInfo] = None,
        page: int = 1,
        rpp: int = 1000,
        map_type: str = "tiny",
        max_pages: int = int(1e9),
        **filters: Any,
    ) -> pd.DataFrame:
        """Retrieves timesheet data from Trimergo.

        If `pageinfo` is supplied, `filters` are ignored. Otherwise, `filters`
        are used to construct a PageInfo object internally.

        Args:
            pageinfo: An optional PageInfo object for precise query control.
            page: The page number to start fetching from (1-based).
            rpp: Records per page.
            map_type: The map type for the API request (e.g., "tiny").
            max_pages: Maximum number of pages to fetch.
            **filters: Keyword arguments for filtering (legacy method).

        Returns:
            A pandas DataFrame containing the timesheet records.

        Raises:
            ValueError: If both `pageinfo` and `filters` are provided.
        """
        if pageinfo and filters:
            raise ValueError("Pass either 'pageinfo' or filter kwargs, not both.")

        current_pageinfo: PageInfo
        if pageinfo is None:
            current_pageinfo = self._kwargs_to_pageinfo(filters, page, rpp)
        else:
            current_pageinfo = pageinfo
            current_pageinfo.pageNo = page
            current_pageinfo.rpp = rpp

        params = {"map": map_type, **current_pageinfo.getProperties()}
        df = self._fetch_pages(params=params, start_page=page, max_pages=max_pages)
        valid_timesheet, invalid_timesheet = Functions.validate_data(df=df, schema=TimeSheetSchema, debug=True)
        return valid_timesheet, invalid_timesheet

    # Private helpers
    def _kwargs_to_pageinfo(
        self, filters: Dict[str, Any], page: int, rpp: int
    ) -> PageInfo:
        """Translates legacy keyword arguments into a PageInfo object."""
        allowed_vars = {d["variable"]: d for d in self._allowed}
        pi = PageInfo()
        pi.pageNo = page
        pi.rpp = rpp

        for idx, (raw_key, value) in enumerate(filters.items()):
            op = "="
            field_name = raw_key
            if "__" in raw_key:
                base, suffix = raw_key.rsplit("__", 1)
                if suffix in self._OP_SUFFIX:
                    op = self._OP_SUFFIX[suffix]
                    field_name = base

            base_var = field_name.split(".")[-1]
            if base_var not in allowed_vars:
                raise ValueError(f"Illegal filter field: {raw_key!r}")

            prefix = allowed_vars[base_var]["prefix"]
            if "." not in field_name and prefix:
                field_name = f"{prefix}.{field_name}"

            crit = Criterion(field_name, op, value)
            if idx == 0:
                pi.addCriterion(crit)  # seed
            else:
                pi.wrapCriterion(crit)  # nest → logical AND
        return pi

    def _fetch_and_validate_page_data(
        self,
        url: str,
        params: Dict[str, str],
        page_num: int
    ) -> Optional[List[Dict[str, Any]]]:
        """Fetches a single page of data and performs initial validation."""
        try:
            resp = self._tgo.trimergo_session.get(url, params=params)
            resp.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
            data = resp.json()
        except requests.exceptions.HTTPError as http_err:
            print(f"HTTP error fetching page {page_num} ({url}): {http_err}")
            return None
        except requests.exceptions.JSONDecodeError as json_err:
            print(f"JSON decode error for page {page_num} ({url}): {json_err}")
            return None
        except requests.exceptions.RequestException as req_err:  # Catches other request-related errors
            print(f"Request error fetching page {page_num} ({url}): {req_err}")
            return None

        if not isinstance(data, list):
            print(f"API Error: Expected list, got {type(data)} for page {page_num}.")
            return None
        return data

    def _fetch_pages(
        self,
        params: Dict[str, str],
        *,
        start_page: int = 1,
        max_pages: int = int(1e9),
    ) -> pd.DataFrame:
        """Crawls paginated Trimergo /timesheet endpoint; returns flat DataFrame."""
        rpp = int(params.get("rpp", 1000))
        page_num = start_page
        records: List[Dict[str, Any]] = []

        while page_num <= max_pages:
            url = urllib.parse.urljoin(
                self._tgo.trimergo_host,
                f"/tri-server-gateway-ws/api/rest/timesheet/page/{page_num}",
            )

            page_data = self._fetch_and_validate_page_data(url, params, page_num)

            if page_data is None: # Error occurred or invalid data structure
                break

            if not page_data: # Empty list, means no more data
                break

            for item in page_data:
                if isinstance(item, dict) and "timesheet" in item:
                    records.append(self._flatten_dict(item["timesheet"]))
                else:
                    # Log or handle malformed item if necessary
                    print(f"Warning: Malformed item on page {page_num}: {item}")

            if len(page_data) < rpp:
                break  # Last page fetched
            page_num += 1
        return pd.DataFrame.from_records(records)

    def _get_nested_value(
        self, data: Dict[str, Any], path: List[str], default: Any = None
    ) -> Any:
        """Safely retrieves a value from a nested dictionary path."""
        current_level = data
        for key in path:
            if not isinstance(current_level, dict):
                # print(f"Warning: Path traversal failed. Expected dict at segment "
                #       f"{key_index} ('{key}'), got {type(current_level)}.")
                return default
            current_level = current_level.get(key)
            if current_level is None:
                # print(f"Warning: Path key '{key}' (segment {key_index}) not found.")
                return default
        return current_level

    def _flatten_dict(
        self, d: Dict[str, Any], parent: str = "", sep: str = "_"
    ) -> Dict[str, Any]:
        """Recursively flattens nested dicts."""
        items: Dict[str, Any] = {}
        for k, v in d.items():
            new_key = f"{parent}{sep}{k}" if parent else k
            if isinstance(v, dict) and v:
                items.update(self._flatten_dict(v, new_key, sep))
            elif isinstance(v, list):
                items[new_key] = str(v)
            elif not isinstance(v, dict): # Add if not dict (to include non-empty values)
                items[new_key] = v
        return items

    def _parse_raw_field_strings(self, raw_strings: List[str]) -> List[Dict[str, str]]:
        """Parses a list of raw field strings into dictionary descriptors."""
        parsed_fields: List[Dict[str, str]] = []
        for field_str in raw_strings:
            try:
                parsed_fields.append(self._parse_field_descriptor(field_str))
            except ValueError as ve:
                print(
                    f"Warning: Failed to parse field descriptor '{field_str}': {ve}. "
                    f"Skipping."
                )
        return parsed_fields

    def _extract_fields_from_openapi_desc(self) -> List[str]:
        """Attempts to extract filter field strings from OpenAPI description."""
        if not (
            self._tgo
            and hasattr(self._tgo, 'openapi_json')
            and isinstance(self._tgo.openapi_json, dict)
        ):
            return []

        description_path: List[str] = [
            "paths", "/api/rest/*/timesheet/page/{pag}", "get", "description"
        ]
        description: Any = self._get_nested_value(
            self._tgo.openapi_json, description_path, default=""
        )
        if not isinstance(description, str):
            description = "" # Ensure description is a string for .split later

        marker: str = "You can filter on these fields:"
        if marker in description:
            fields_part = description.split(marker, 1)[1]
            return [f.strip() for f in fields_part.split(",") if f.strip()]
        return []

    def _load_allowed_fields(self) -> List[Dict[str, str]]:
        """Loads allowed filter fields for Trimergo timesheets."""
        raw_field_strings: List[str] = []
        try:
            raw_field_strings = self._extract_fields_from_openapi_desc()
        except Exception as e: # Catch any error during OpenAPI extraction
            print(
                f"Error: Unexpected issue extracting fields from OpenAPI: {e}. "
                f"Using default field strings."
            )
            # Fallback to default strings if OpenAPI extraction fails
            raw_field_strings = self._DEFAULT_FIELD_STRINGS

        if not raw_field_strings:
            # print(
            #     "Info: No fields from OpenAPI or extraction failed cleanly. "
            #     "Using default field strings."
            # )
            raw_field_strings = self._DEFAULT_FIELD_STRINGS

        parsed_fields = self._parse_raw_field_strings(raw_field_strings)

        if not parsed_fields:
            # This means all attempts (OpenAPI or default) yielded no parsable fields.
            # This is critical if even defaults fail.
            print(
                "Critical Warning: All field strings (OpenAPI or default) failed to parse. "
                "Attempting to parse hardcoded defaults again as a final fallback."
            )
            # Try parsing default strings one last time if all else failed
            parsed_fields = self._parse_raw_field_strings(self._DEFAULT_FIELD_STRINGS)
            if not parsed_fields:
                # If even this fails, something is fundamentally wrong with parsing logic or defaults
                raise RuntimeError(
                    "Failed to parse any field descriptors, including hardcoded defaults. "
                    "Timesheet filtering will be severely impacted."
                )
        return parsed_fields

    @staticmethod
    def _parse_field_descriptor(d: str) -> Dict[str, str]:
        """Parses a Trimergo field descriptor string into its prefix and variable name."""
        desc_str = d.strip()
        match = re.search(r'\((.*)\)', desc_str)
        if match:
            inner_content = match.group(1)
            if '.' in inner_content:
                desc_str = inner_content.strip()

        if "." not in desc_str:
            raise ValueError(
                f"Field '{d}' malformed: expected 'prefix.variable' (got '{desc_str}')."
            )

        prefix, variable_part = desc_str.split(".", 1)
        variable_name = variable_part.split(".")[0]

        if not prefix.strip() or not variable_name.strip():
            raise ValueError(
                f"Field '{d}' -> empty prefix/var (pf='{prefix}',var='{variable_name}')."
            )
        return {"prefix": prefix.strip(), "variable": variable_name.strip()}
