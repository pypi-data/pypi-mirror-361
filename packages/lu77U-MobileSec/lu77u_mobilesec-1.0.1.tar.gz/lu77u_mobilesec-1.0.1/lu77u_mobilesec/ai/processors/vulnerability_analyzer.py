#!/usr/bin/env python3
"""
Vulnerability Analyzer for AI-based code analysis

Provides unified vulnerability analysis functionality across different frameworks.
"""

import re
import json
import asyncio
from typing import Dict, List, Optional, Any
from ...ai.providers.ollama_provider import OllamaProvider
from ...ai.providers.groq_provider import GroqProvider


class VulnerabilityAnalyzer:
    """Unified AI-based vulnerability analyzer for different frameworks"""
    
    def __init__(self, debug: bool = False):
        """Initialize the vulnerability analyzer"""
        self.ollama_provider = OllamaProvider()
        self.groq_provider = GroqProvider()
        self.debug = debug
    
    async def analyze_code_with_ai(
        self, 
        framework_type: str,
        code_content: str = None,
        context: Dict[str, Any] = None,
        prompts: Dict[str, str] = None,
        use_local_llm: bool = True,
        llm_preference: str = 'ollama'
    ) -> List[Dict]:
        """
        Analyze code with AI for vulnerabilities
        
        Args:
            framework_type: The framework type ('flutter', 'react-native', 'java', etc.)
            code_content: The code content to analyze
            context: Additional context like manifest, dependencies, etc.
            prompts: Custom prompts for analysis
            use_local_llm: Whether to use local LLM
            llm_preference: Preferred LLM ('ollama' or 'groq')
            
        Returns:
            List of vulnerabilities found by AI analysis
        """
        try:
            if framework_type == 'flutter':
                return await self._analyze_flutter_with_ai(
                    code_content, context, prompts, use_local_llm, llm_preference
                )
            elif framework_type == 'react-native':
                return await self._analyze_react_native_with_ai(
                    code_content, context, prompts, use_local_llm, llm_preference
                )
            elif framework_type in ['java', 'kotlin']:
                return await self._analyze_java_kotlin_with_ai(
                    code_content, context, prompts, use_local_llm, llm_preference
                )
            else:
                # Generic analysis
                return await self._analyze_generic_with_ai(
                    code_content, context, prompts, use_local_llm, llm_preference
                )
        except Exception as e:
            print(f"‚ùå Error in AI vulnerability analysis: {e}")
            return []
    
    FLUTTER_PROMPTS = {
        'manifest': '''You are an Android security expert. Analyze this AndroidManifest.xml for security vulnerabilities:

**Critical Security Checks:**
1. **Dangerous Permissions**: Location, camera, contacts, SMS, phone access
2. **Debug/Test Flags**: android:debuggable="true", android:testOnly="true"
3. **Exported Components**: Activities, services, receivers without proper permissions
4. **Data Protection**: android:allowBackup="true", missing FLAG_SECURE
5. **Network Security**: android:usesCleartextTraffic="true", missing network config
6. **Custom URL Schemes**: Deep links without validation, intent filter issues
7. **Intent Vulnerabilities**: Exported components with dangerous actions
8. **Missing Security Attributes**: Missing android:exported declarations

**Analysis Instructions:**
- Examine all components (activities, services, receivers, providers)
- Check permission declarations and usage
- Identify security configuration issues
- Look for development/debug artifacts

Return ONLY a JSON array of vulnerabilities. Each finding must include:
[{
  "title": "Brief vulnerability title",
  "severity": "HIGH/MEDIUM/LOW",
  "description": "Technical explanation of the security issue",
  "location": "XML element or line reference",
  "impact": "Specific security consequences",
  "recommendation": "Concrete remediation steps",
  "code": "Relevant XML snippet"
}]

If no vulnerabilities are found, return an empty array: []''',

        'main_dart': '''You are a mobile security expert analyzing Flutter Dart code. Look for security vulnerabilities in this code:

**CRITICAL: Focus on identifying actual security issues, not just code structure!**

**Key Security Patterns to Find:**
1. **Hardcoded Secrets**: API keys, passwords, tokens, encryption keys
2. **Authentication Flaws**: Weak validation, bypasses, hardcoded credentials
3. **Cryptographic Issues**: Weak algorithms, hardcoded keys, improper implementation
4. **Input Validation**: SQL injection, XSS, command injection vulnerabilities
5. **Authorization Problems**: Missing access controls, privilege escalation
6. **Data Exposure**: Sensitive data logging, insecure storage
7. **Debug/Test Code**: Development artifacts in production
8. **Network Security**: Insecure HTTP, certificate pinning bypass

**SCAN FOR THESE SPECIFIC PATTERNS:**
- Hardcoded strings that look like: passwords, API keys, tokens, secrets
- Base64 encoded strings that might contain credentials (look for patterns like "U3VwM3JTM2NyM3RmMHJNeVMzY3VSM0wwZ2luQXBw")
- Hex encoded strings that might be keys (look for patterns like "10263367342860162200063963412e0c01563c1962547d5e38585c72440d")
- Weak cryptographic algorithms (MD5, SHA1, DES)
- Hardcoded URLs, especially admin/debug endpoints
- Authentication bypass logic
- Sensitive data in logs or debug output
- Unsafe HTTP requests or certificate validation bypass
- Assignment statements like: `r0 = "long_base64_string"` or `password = "hardcoded_value"`

**IMPORTANT:** Only report actual security vulnerabilities. Ignore normal code structure.

Return ONLY a JSON array of actual security vulnerabilities:
[{
  "title": "Specific vulnerability name",
  "severity": "HIGH/MEDIUM/LOW", 
  "description": "Clear security issue explanation",
  "location": "Line number or function name",
  "impact": "What attacker can achieve",
  "recommendation": "How to fix this issue",
  "code": "Vulnerable code snippet"
}]

Return empty array [] if no actual security vulnerabilities found.''',

        'objs': '''You are a mobile security expert analyzing Flutter object definitions and memory structures. Examine this object dump/definition file for security vulnerabilities:

**Security Analysis Focus:**
1. **Type Safety Issues**: Type confusion, unsafe casts, missing type checks
2. **Memory Layout Vulnerabilities**: Buffer overflows in object fields, unsafe unions
3. **Access Control**: Exposed sensitive fields, missing access modifiers
4. **State Management**: Race conditions, inconsistent state, shared mutable state
5. **Serialization Issues**: Unsafe deserialization, data validation bypass
6. **Reference Management**: Dangling pointers, circular references, memory leaks
7. **Inheritance Problems**: Virtual function table corruption, method override issues
8. **Debug/Test Objects**: Development objects in production builds

**Look for suspicious patterns:**
- Objects with sensitive data fields (keys, tokens, passwords)
- Inconsistent field sizes or types
- Missing validation or bounds checking
- Objects that handle external input
- State machines with exploitable transitions
- Objects with unsafe native method calls

Return findings as a JSON array. For each vulnerability:
[{
  "title": "Specific issue name",
  "severity": "HIGH/MEDIUM/LOW",
  "description": "Technical explanation of the vulnerability",
  "location": "Object name or field reference",
  "impact": "Security consequence (memory corruption, data exposure, etc.)",
  "recommendation": "Specific fix or mitigation",
  "code": "Relevant object definition snippet"
}]

If no security issues are found, return: []''',

        'pp': '''You are a security expert analyzing Flutter application strings and data. Find actual security vulnerabilities in this content:

**CRITICAL: Only report real security issues, not normal application strings!**

**High-Priority Security Patterns:**
1. **Credentials**: API keys, passwords, tokens, database credentials
2. **Hardcoded Secrets**: Encryption keys, certificates, private keys
3. **Debug/Admin Access**: Debug URLs, admin endpoints, test accounts
4. **Sensitive URLs**: Internal APIs, staging servers, admin panels
5. **Cryptographic Material**: Keys, salts, initialization vectors
6. **Configuration Leaks**: Database URLs, server addresses
7. **Authentication Bypasses**: Debug flags, test credentials

**SPECIFIC PATTERNS TO FIND:**
- API keys (starting with "sk_", "pk_", "AIza", etc.)
- Database connection strings
- Base64 strings containing credentials
- URLs with "admin", "debug", "test", "internal"
- Hardcoded passwords or tokens
- Private keys (-----BEGIN PRIVATE KEY-----)
- JWT tokens or OAuth secrets

**IGNORE NORMAL FLUTTER STRINGS:**
- UI text, button labels, error messages
- Standard Flutter framework strings
- Font names, color names
- Normal configuration keys
- Method names, class names

Return ONLY actual security vulnerabilities as JSON:
[{
  "title": "Specific security issue",
  "severity": "HIGH/MEDIUM/LOW",
  "description": "Why this is a security risk",
  "location": "String identifier or pattern",
  "impact": "What attacker can do",
  "recommendation": "How to secure this",
  "code": "Sensitive string found"
}]

Return [] if no actual security issues found.''',

        'pubspec': '''You are a dependency security auditor specializing in Flutter/Dart package security. Analyze this pubspec.lock file for vulnerabilities:

**Dependency Security Assessment:**
1. **Known Vulnerable Packages**: CVE-listed packages, deprecated packages
2. **Outdated Dependencies**: Packages with known security fixes in newer versions
3. **Development Dependencies**: Dev/test packages included in production builds
4. **Insecure Package Sources**: Non-HTTPS sources, unofficial repositories
5. **Version Pinning Issues**: Overly permissive version constraints
6. **Compatibility Conflicts**: Package combinations with known security issues
7. **Minimum SDK Issues**: Deprecated SDK versions with security vulnerabilities
8. **Third-party Risks**: Packages with poor security track records

**Analysis Focus:**
- Check each package against known vulnerability databases
- Identify packages that should be updated
- Look for suspicious or unofficial package sources
- Evaluate dependency chain security risks
- Check for packages with excessive permissions

Return findings as a JSON array:
[{
  "title": "Security issue with dependency",
  "severity": "HIGH/MEDIUM/LOW",
  "description": "Specific security risk or vulnerability",
  "location": "Package name and version",
  "impact": "Potential security consequences",
  "recommendation": "Update instructions or alternative packages",
  "code": "Package dependency definition"
}]

If no security issues are found, return: []'''
    }

    async def _analyze_flutter_with_ai(self, code_content: str = None, context: Dict[str, Any] = None, prompts: Dict[str, str] = None, use_local_llm: bool = True, llm_preference: str = 'ollama') -> List[Dict]:
        """Analyze Flutter code with AI using specialized prompts"""
        try:
            filename = context.get('filename', '') if context else ''
            if not filename or not code_content:
                if self.debug:
                    print(f"[DEBUG] Missing filename ({filename}) or content ({len(code_content) if code_content else 0} chars)")
                return []

            # Select appropriate prompt based on file type
            prompt_key = None
            if 'AndroidManifest.xml' in filename:
                prompt_key = 'manifest'
            elif 'main.dart' in filename:
                prompt_key = 'main_dart'
            elif 'objs.txt' in filename:
                prompt_key = 'objs'
            elif 'pp.txt' in filename:
                prompt_key = 'pp'
            elif 'pubspec.lock' in filename:
                prompt_key = 'pubspec'

            if not prompt_key:
                if self.debug:
                    print(f"[DEBUG] No matching prompt for filename: {filename}")
                return []

            # Truncate content if too long, but be smarter about it
            max_content_length = 15000
            if len(code_content) > max_content_length:
                # For different file types, truncate more intelligently
                if prompt_key == 'main_dart':
                    # For Dart code (often assembly), extract security-relevant parts
                    code_content = self._extract_security_relevant_dart_content(code_content, max_content_length)
                elif prompt_key == 'pp':
                    # For pp content, try to keep strings that look like secrets
                    code_content = self._extract_security_relevant_pp_content(code_content, max_content_length)
                elif prompt_key == 'objs':
                    # For object definitions, focus on suspicious objects
                    code_content = self._extract_security_relevant_objs_content(code_content, max_content_length)
                else:
                    # For other content, just truncate
                    code_content = code_content[:max_content_length]
                
                if self.debug:
                    print(f"[DEBUG] Content truncated to {len(code_content)} characters for {prompt_key}")

            # Build the full prompt with context
            analysis_prompt = f"{self.FLUTTER_PROMPTS[prompt_key]}\n\nAnalyze this {prompt_key} content:\n```\n{code_content}\n```"

            if self.debug:
                print(f"üîç Flutter AI Analysis for {filename}:")
                print(f"   - Prompt type: {prompt_key}")
                print(f"   - Content length: {len(code_content)} chars")
                print(f"   - LLM preference: {llm_preference}")
                print(f"   - Use local LLM: {use_local_llm}")
                print(f"   - Prompt preview (first 500 chars):")
                print(f"     {analysis_prompt[:500]}...")

            # Get AI analysis using Ollama only
            response = None
            provider_used = None
            
            try:
                if use_local_llm and llm_preference == 'ollama':
                    # Ensure Ollama is running first
                    if not self.ollama_provider.is_ollama_running():
                        print("‚ö†Ô∏è  Ollama not running, attempting to start...")
                        if self.ollama_provider.start_ollama_service():
                            print("‚úÖ Ollama service started successfully")
                        else:
                            print("‚ùå Failed to start Ollama service")
                            return []
                    
                    response = await self.ollama_provider.analyze_with_local_llm(analysis_prompt)
                    provider_used = 'ollama'
                    
                    # Check if Ollama failed and try fallback to Groq
                    if isinstance(response, dict) and 'error' in response:
                        print(f"‚ö†Ô∏è  Ollama not running, falling back to Groq")
                        try:
                            response = await self.groq_provider.analyze_code(analysis_prompt, code_content)
                            provider_used = 'groq (fallback)'
                            # Wrap response in expected format
                            if isinstance(response, str):
                                response = {'response': response}
                        except Exception as groq_error:
                            print(f"‚ùå Groq fallback also failed: {groq_error}")
                            return []
                else:
                    print("‚ùå Only Ollama provider is currently supported")
                    return []
            except Exception as provider_error:
                print(f"‚ùå Error with provider: {provider_error}")
                # Try Groq as fallback for any provider errors
                try:
                    print("‚ö†Ô∏è  Trying Groq as fallback...")
                    response = await self.groq_provider.analyze_code(analysis_prompt, code_content)
                    provider_used = 'groq (fallback)'
                    if isinstance(response, str):
                        response = {'response': response}
                except Exception as groq_error:
                    print(f"‚ùå Groq fallback failed: {groq_error}")
                    return []

            if self.debug:
                print(f"ü§ñ AI Response from {provider_used}:")
                print(f"   - Response type: {type(response)}")
                if isinstance(response, str):
                    print(f"   - Response length: {len(response)} chars")
                    print(f"   - First 200 chars: {response[:200]}...")
                elif isinstance(response, dict):
                    print(f"   - Response keys: {list(response.keys())}")
                    if 'response' in response:
                        print(f"   - Inner response length: {len(response['response'])} chars")
                        print(f"   - First 200 chars: {response['response'][:200]}...")
                else:
                    print(f"   - Full response: {response}")

            # Parse and return vulnerabilities
            vulnerabilities = self._parse_ai_vulnerabilities(response)
            
            if self.debug:
                print(f"‚úÖ Parsed {len(vulnerabilities)} vulnerabilities from AI response")
                for i, vuln in enumerate(vulnerabilities[:3]):  # Show first 3
                    print(f"   {i+1}. {vuln.get('title', 'No title')} ({vuln.get('severity', 'No severity')})")
                print(f"[AI DEBUG] Raw response for {filename}: {vulnerabilities}")
            
            return vulnerabilities
            
        except Exception as e:
            print(f"‚ùå Error in Flutter AI analysis: {e}")
            if self.debug:
                import traceback
                print(f"[DEBUG] Full traceback: {traceback.format_exc()}")
            return []
    
    async def _analyze_react_native_with_ai(
        self,
        code_content: str = None,
        context: Dict[str, Any] = None,
        prompts: Dict[str, str] = None,
        use_local_llm: bool = True,
        llm_preference: str = 'ollama'
    ) -> List[Dict]:
        """Analyze React Native code with AI for security vulnerabilities"""
        print("ü§ñ Analyzing React Native code with AI...")
        
        if self.debug:
            print(f"üêõ DEBUG: Starting React Native AI analysis with LLM preference: {llm_preference}")
        
        all_vulnerabilities = []
        
        # Handle different input types
        if isinstance(context, dict) and 'decompiled_modules' in context:
            # Analyzing decompiled modules
            decompiled_modules = context['decompiled_modules']
            return await self._analyze_decompiled_modules_with_ai(
                decompiled_modules, use_local_llm, llm_preference
            )
        elif code_content:
            # Analyzing single code content
            return await self._analyze_single_react_native_code(
                code_content, use_local_llm, llm_preference
            )
        else:
            print("‚ö†Ô∏è  No code content or decompiled modules provided for React Native analysis")
            return []
    
    async def _analyze_decompiled_modules_with_ai(
        self, 
        decompiled_modules: Dict[str, str], 
        use_local_llm: bool = True, 
        llm_preference: str = 'ollama'
    ) -> List[Dict]:
        """AI-powered analysis of decompiled React Native modules"""
        print("ü§ñ Analyzing decompiled React Native modules with AI...")
        
        if self.debug:
            print(f"üêõ DEBUG: Analyzing {len(decompiled_modules)} decompiled modules")
        
        all_vulnerabilities = []
        
        # Analyze modules in batches to avoid token limits
        module_items = list(decompiled_modules.items())
        batch_size = 5  # Analyze 5 modules at a time
        
        for i in range(0, len(module_items), batch_size):
            batch = module_items[i:i + batch_size]
            print(f"üîç Analyzing batch {i//batch_size + 1}/{(len(module_items) + batch_size - 1)//batch_size}")
            
            # Prepare batch analysis prompt
            batch_content = ""
            for module_name, module_content in batch:
                batch_content += f"\n=== MODULE: {module_name} ===\n{module_content[:5000]}\n"  # Limit content
            
            analysis_prompt = f"""REACT NATIVE DECOMPILED CODE SECURITY ANALYSIS

Analyze these decompiled React Native JavaScript modules for security vulnerabilities.

FOCUS AREAS:
1. React Native Bridge vulnerabilities - unsafe NativeModules usage
2. JavaScript injection in WebViews, eval(), Function constructor  
3. Insecure AsyncStorage/SecureStore usage with sensitive data
4. Hardcoded API keys, tokens, secrets, credentials
5. HTTP requests instead of HTTPS, SSL bypass
6. Debug code, console.log with sensitive data, __DEV__ checks
7. Deep link vulnerabilities in Linking API, navigation
8. Input validation issues - unsafe parsing, innerHTML injection
9. Cryptographic vulnerabilities - weak algorithms, Math.random()
10. Authentication/authorization bypass patterns

RESPONSE FORMAT: Return ONLY a JSON array with detailed analysis.

Example:
[{{"vulnerability_type":"JavaScript Injection","file":"module.js","line_number":45,"code_snippet":"eval(userInput + code)","description":"User input concatenated with eval() call","severity":"Critical","context":"Full function context"}}]

DECOMPILED MODULES TO ANALYZE:
{batch_content}

Return JSON array only:"""

            try:
                if llm_preference == 'ollama' and use_local_llm:
                    if self.debug:
                        print(f"üêõ DEBUG: Using Ollama for batch {i//batch_size + 1}")
                    result = await self.ollama_provider.analyze_with_local_llm(analysis_prompt)
                else:
                    if self.debug:
                        print(f"üêõ DEBUG: Using GROQ for batch {i//batch_size + 1}")
                    result = await self.groq_provider.analyze_with_groq(analysis_prompt)
                
                if result:
                    # Parse the result using the existing parser
                    batch_vulnerabilities = self._parse_ai_vulnerabilities(result)
                    all_vulnerabilities.extend(batch_vulnerabilities)
                    if self.debug:
                        print(f"üêõ DEBUG: Successfully parsed {len(batch_vulnerabilities)} vulnerabilities from batch {i//batch_size + 1}")
                else:
                    print(f"‚ùå No response from AI for batch {i//batch_size + 1}")
                    
            except Exception as e:
                print(f"‚ùå Analysis error for batch {i//batch_size + 1}: {e}")
                continue
        
        if self.debug:
            print(f"üêõ DEBUG: Total vulnerabilities from AI analysis: {len(all_vulnerabilities)}")
        
        return all_vulnerabilities
    
    async def _analyze_single_react_native_code(
        self, 
        code_content: str, 
        use_local_llm: bool = True, 
        llm_preference: str = 'ollama'
    ) -> List[Dict]:
        """Analyze single React Native code content with AI"""
        analysis_prompt = f"""REACT NATIVE CODE SECURITY ANALYSIS

Analyze this React Native code for security vulnerabilities.

FOCUS AREAS:
1. React Native Bridge vulnerabilities - unsafe NativeModules usage
2. JavaScript injection in WebViews, eval(), Function constructor  
3. Insecure AsyncStorage/SecureStore usage with sensitive data
4. Hardcoded API keys, tokens, secrets, credentials
5. HTTP requests instead of HTTPS, SSL bypass
6. Debug code, console.log with sensitive data, __DEV__ checks
7. Deep link vulnerabilities in Linking API, navigation
8. Input validation issues - unsafe parsing, innerHTML injection
9. Cryptographic vulnerabilities - weak algorithms, Math.random()
10. Authentication/authorization bypass patterns

CODE TO ANALYZE:
{code_content}

RESPONSE FORMAT: Return ONLY a JSON array with detailed analysis.

Example:
[{{"vulnerability_type":"JavaScript Injection","file":"code.js","line_number":45,"code_snippet":"eval(userInput + code)","description":"User input concatenated with eval() call","severity":"Critical","context":"Full function context"}}]

Return JSON array only:"""

        try:
            if llm_preference == 'ollama' and use_local_llm:
                result = await self.ollama_provider.analyze_with_local_llm(analysis_prompt)
            else:
                result = await self.groq_provider.analyze_with_groq(analysis_prompt)
            
            if result:
                return self._parse_ai_vulnerabilities(result)
            else:
                print("‚ùå No response from AI for single code analysis")
                return []
                
        except Exception as e:
            print(f"‚ùå Single code analysis error: {e}")
            return []
    
    async def _analyze_java_kotlin_with_ai(
        self,
        code_content: str = None,
        context: Dict[str, Any] = None,
        prompts: Dict[str, str] = None,
        use_local_llm: bool = True,
        llm_preference: str = 'ollama'
    ) -> List[Dict]:
        """Analyze Java/Kotlin code with AI (placeholder for future implementation)"""
        # This would be implemented similar to Flutter analysis
        # but with Java/Kotlin specific security patterns
        return []
    
    async def _analyze_generic_with_ai(
        self,
        code_content: str = None,
        context: Dict[str, Any] = None,
        prompts: Dict[str, str] = None,
        use_local_llm: bool = True,
        llm_preference: str = 'ollama'
    ) -> List[Dict]:
        """Generic AI analysis for unknown frameworks"""
        # Basic security analysis without framework-specific patterns
        return []
    
    def _parse_ai_vulnerabilities(self, ai_response: str) -> List[Dict]:
        """Parse AI response into structured vulnerability data"""
        try:
            if self.debug:
                print(f"[DEBUG] Parsing AI response: {type(ai_response)}")
                
            # Handle dict response (e.g., {'response': ...})
            if isinstance(ai_response, dict):
                if 'response' in ai_response:
                    ai_response = ai_response['response']
                    if self.debug:
                        print(f"[DEBUG] Extracted response from dict: {ai_response[:200]}...")
                else:
                    print(f"‚ö†Ô∏è  AI response is a dict without 'response' key: {ai_response}")
                    return []
            elif not isinstance(ai_response, str):
                print(f"‚ö†Ô∏è  Unexpected AI response type: {type(ai_response)}. Value: {ai_response}")
                return []

            # Clean up the response string
            response_text = ai_response.strip()
            
            # Multiple JSON extraction strategies
            json_str = None
            
            # Strategy 1: Look for JSON code blocks
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL | re.IGNORECASE)
            if json_match:
                json_str = json_match.group(1).strip()
                if self.debug:
                    print("[DEBUG] Found JSON in code block")
            
            # Strategy 2: Look for JSON array patterns
            if not json_str:
                json_match = re.search(r'\[\s*\{.*?\}\s*\]', response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    if self.debug:
                        print("[DEBUG] Found JSON array pattern")
            
            # Strategy 3: Look for single JSON object and wrap in array
            if not json_str:
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
                if json_match:
                    json_str = f"[{json_match.group(0)}]"
                    if self.debug:
                        print("[DEBUG] Found single JSON object, wrapped in array")
            
            # Strategy 4: Look for empty array
            if not json_str and ('[]' in response_text or 'no vulnerabilities' in response_text.lower()):
                json_str = '[]'
                if self.debug:
                    print("[DEBUG] Found empty array or no vulnerabilities statement")
            
            if json_str:
                try:
                    vulnerabilities = json.loads(json_str)
                    if self.debug:
                        print(f"[DEBUG] Successfully parsed JSON: {len(vulnerabilities) if isinstance(vulnerabilities, list) else 1} items")
                except json.JSONDecodeError as je:
                    if self.debug:
                        print(f"[DEBUG] JSON decode error: {je}")
                        print(f"[DEBUG] Attempted to parse: {json_str[:200]}...")
                    return self._parse_text_vulnerabilities(response_text)
            else:
                if self.debug:
                    print("[DEBUG] No JSON found in response, trying text parsing")
                return self._parse_text_vulnerabilities(response_text)
            
            # Ensure it's a list
            if isinstance(vulnerabilities, dict):
                vulnerabilities = [vulnerabilities]
            elif not isinstance(vulnerabilities, list):
                if self.debug:
                    print(f"[DEBUG] Unexpected vulnerabilities type: {type(vulnerabilities)}")
                return []
            
            # Validate and clean up vulnerabilities
            cleaned_vulns = []
            for i, vuln in enumerate(vulnerabilities):
                if isinstance(vuln, dict):
                    # Ensure required fields exist
                    if 'title' not in vuln:
                        vuln['title'] = f'AI-detected vulnerability #{i+1}'
                    
                    # Set defaults for missing fields
                    vuln.setdefault('severity', 'MEDIUM')
                    vuln.setdefault('description', 'AI-detected vulnerability')
                    vuln.setdefault('location', 'Unknown')
                    vuln.setdefault('impact', 'Potential security risk')
                    vuln.setdefault('recommendation', 'Review and remediate')
                    vuln.setdefault('code', '')
                    
                    # Clean up values
                    vuln['severity'] = vuln['severity'].upper()
                    if vuln['severity'] not in ['HIGH', 'MEDIUM', 'LOW']:
                        vuln['severity'] = 'MEDIUM'
                    
                    cleaned_vulns.append(vuln)
                    
            if self.debug:
                print(f"[DEBUG] Cleaned vulnerabilities: {len(cleaned_vulns)}")
                
            return cleaned_vulns
            
        except Exception as e:
            print(f"‚ùå Error parsing AI vulnerabilities: {e}")
            if self.debug:
                import traceback
                print(f"[DEBUG] Full traceback: {traceback.format_exc()}")
            return self._parse_text_vulnerabilities(ai_response)
    
    def _parse_text_vulnerabilities(self, text_response: str) -> List[Dict]:
        """Parse plain text AI response for vulnerabilities when JSON parsing fails"""
        try:
            vulnerabilities = []
            
            # Check for explicit "no vulnerabilities" statements
            no_vuln_indicators = [
                'no vulnerabilities', 'no security issues', 'no issues found',
                'appears secure', 'no obvious security', 'no significant risks',
                '[]', 'empty array', 'no findings'
            ]
            
            response_lower = text_response.lower()
            if any(indicator in response_lower for indicator in no_vuln_indicators):
                if self.debug:
                    print("[DEBUG] Found 'no vulnerabilities' indicator in text")
                return []
            
            # Split response into potential vulnerability sections
            # Look for numbered lists, bullet points, or clear sections
            sections = []
            
            # Try different splitting patterns
            patterns = [
                r'\n\d+\.\s+',  # Numbered lists (1. 2. 3.)
                r'\n-\s+',      # Dash bullets
                r'\n\*\s+',     # Asterisk bullets
                r'\n#{1,3}\s+', # Markdown headers
                r'\n[A-Z][^.!?]*[.!?]\s*\n', # Sentences that might be titles
            ]
            
            for pattern in patterns:
                potential_sections = re.split(pattern, text_response)
                if len(potential_sections) > 1:
                    sections = potential_sections
                    break
            
            # If no clear sections found, split by paragraphs
            if not sections:
                sections = [s.strip() for s in text_response.split('\n\n') if s.strip()]
            
            # Analyze each section for vulnerability information
            for i, section in enumerate(sections):
                section = section.strip()
                if len(section) < 20:  # Skip very short sections
                    continue
                
                # Security-related keywords that suggest a vulnerability
                security_keywords = [
                    'vulnerability', 'security', 'risk', 'exploit', 'attack',
                    'dangerous', 'unsafe', 'insecure', 'exposed', 'hardcoded',
                    'buffer overflow', 'injection', 'bypass', 'privilege',
                    'authentication', 'authorization', 'encryption', 'credential'
                ]
                
                # Check if section contains security-related content
                section_lower = section.lower()
                if not any(keyword in section_lower for keyword in security_keywords):
                    continue
                
                # Extract title from section
                title = None
                title_patterns = [
                    r'^([^.\n]+)\.?\s*\n',  # First line ending with period
                    r'^([^:\n]+):',         # First line ending with colon
                    r'(\b[A-Z][^.!?\n]{10,}[.!?])', # Sentence that looks like a title
                ]
                
                for pattern in title_patterns:
                    match = re.search(pattern, section)
                    if match:
                        title = match.group(1).strip()
                        break
                
                if not title:
                    title = f'AI-detected security issue #{i+1}'
                
                # Extract severity if mentioned
                severity = 'MEDIUM'
                severity_patterns = [
                    r'\b(high|critical)\b.*?(?:severity|risk|priority)',
                    r'\b(medium|moderate)\b.*?(?:severity|risk|priority)',
                    r'\b(low|minor)\b.*?(?:severity|risk|priority)',
                    r'(?:severity|risk|priority).*?\b(high|critical|medium|moderate|low|minor)\b'
                ]
                
                for pattern in severity_patterns:
                    match = re.search(pattern, section_lower)
                    if match:
                        severity_text = match.group(1).lower()
                        if severity_text in ['high', 'critical']:
                            severity = 'HIGH'
                        elif severity_text in ['medium', 'moderate']:
                            severity = 'MEDIUM'
                        elif severity_text in ['low', 'minor']:
                            severity = 'LOW'
                        break
                
                # Create vulnerability entry
                vuln = {
                    'title': title[:100],  # Limit title length
                    'severity': severity,
                    'description': section[:500],  # Limit description length
                    'location': 'AI Analysis',
                    'impact': 'Potential security risk identified by AI analysis',
                    'recommendation': 'Review and investigate this potential vulnerability',
                    'code': ''
                }
                vulnerabilities.append(vuln)
            
            if self.debug:
                print(f"[DEBUG] Text parsing found {len(vulnerabilities)} potential vulnerabilities")
            
            return vulnerabilities
            
        except Exception as e:
            print(f"‚ùå Error parsing text vulnerabilities: {e}")
            if self.debug:
                import traceback
                print(f"[DEBUG] Text parsing traceback: {traceback.format_exc()}")
            return []
    
    def _extract_security_relevant_dart_content(self, content: str, max_length: int) -> str:
        """Extract security-relevant content from Dart/Assembly code"""
        lines = content.split('\n')
        security_lines = []
        current_length = 0
        
        # Patterns that indicate security-relevant assembly code
        security_patterns = [
            r'r\d+\s*=\s*["\'][^"\']{10,}["\']',  # Register assignments with strings
            r'ldr\s+x\d+,.*["\'][^"\']{10,}["\']', # Load instructions with strings
            r'["\'][A-Za-z0-9+/=]{20,}["\']',     # Base64-like strings
            r'["\'][a-f0-9]{20,}["\']',           # Hex strings
            r'password|secret|key|token|api',      # Security keywords
            r'encode|decode|crypt|hash',          # Crypto operations
            r'login|auth|admin|debug',            # Authentication keywords
            r'http|url|endpoint|server',          # Network-related
            r'0x[a-f0-9]{6,}',                    # Memory addresses that might point to secrets
        ]
        
        # First pass: collect security-relevant lines
        for line in lines:
            if current_length + len(line) > max_length:
                break
            if any(re.search(pattern, line, re.IGNORECASE) for pattern in security_patterns):
                security_lines.append(line)
                current_length += len(line) + 1  # +1 for newline
        
        # Second pass: add context around security lines
        if current_length < max_length * 0.8:
            context_lines = set()
            for i, line in enumerate(lines):
                if line in security_lines:
                    # Add 2 lines before and after for context
                    for j in range(max(0, i-2), min(len(lines), i+3)):
                        if current_length + len(lines[j]) < max_length:
                            context_lines.add(lines[j])
                            current_length += len(lines[j]) + 1
            
            security_lines.extend(context_lines)
        
        if security_lines:
            return '\n'.join(security_lines)
        else:
            # Fallback to beginning if no security patterns found
            return content[:max_length]
    
    def _extract_security_relevant_pp_content(self, content: str, max_length: int) -> str:
        """Extract security-relevant content from pp.txt (string pools)"""
        lines = content.split('\n')
        important_lines = []
        current_length = 0
        
        # Patterns for potentially sensitive strings
        priority_patterns = [
            r'["\'][A-Za-z0-9+/]{20,}["\']',      # Base64-like strings
            r'["\'][a-f0-9]{32,}["\']',           # Hex strings  
            r'["\'].*(?:key|secret|token|password|api).*["\']',  # Credential keywords
            r'["\']https?://[^"\']+["\']',        # URLs
            r'["\'][A-Z0-9_]{10,}["\']',          # Constants that might be keys
            r'["\'].*(?:admin|debug|test|dev).*["\']',  # Debug/admin strings
            r'sk_|pk_|AIza|bearer|jwt',           # Common API key prefixes
            r'-----BEGIN.*-----',                 # Certificate/key headers
            r'mysql://|postgres://|mongodb://',   # Database URLs
        ]
        
        # First pass: collect lines that might contain secrets
        for line in lines:
            if current_length + len(line) > max_length:
                break
            if any(re.search(pattern, line, re.IGNORECASE) for pattern in priority_patterns):
                important_lines.append(line)
                current_length += len(line) + 1
        
        # If we have space, add some additional context
        if current_length < max_length * 0.8:
            for line in lines:
                if current_length + len(line) > max_length:
                    break
                if line not in important_lines:
                    important_lines.append(line)
                    current_length += len(line) + 1
        
        if important_lines:
            return '\n'.join(important_lines)
        else:
            return content[:max_length]
    
    def _extract_security_relevant_objs_content(self, content: str, max_length: int) -> str:
        """Extract security-relevant content from object definitions"""
        lines = content.split('\n')
        important_lines = []
        current_length = 0
        
        # Patterns for suspicious object definitions
        security_patterns = [
            r'class.*(?:Auth|Login|Crypt|Secret|Key|Token|Password)',  # Security-related classes
            r'field.*(?:password|secret|key|token|api)',              # Sensitive fields
            r'method.*(?:encrypt|decrypt|hash|auth|login)',           # Security methods
            r'type.*(?:String|Buffer|Array).*(?:key|secret|password)', # Sensitive data types
            r'native.*method',                                        # Native methods (potential JNI vulnerabilities)
            r'unsafe',                                               # Unsafe operations
        ]
        
        # Collect security-relevant object definitions
        for line in lines:
            if current_length + len(line) > max_length:
                break
            if any(re.search(pattern, line, re.IGNORECASE) for pattern in security_patterns):
                important_lines.append(line)
                current_length += len(line) + 1
        
        # Add some general context if we have space
        if current_length < max_length * 0.8:
            for line in lines:
                if current_length + len(line) > max_length:
                    break
                if line not in important_lines and line.strip():  # Skip empty lines
                    important_lines.append(line)
                    current_length += len(line) + 1
        
        if important_lines:
            return '\n'.join(important_lines)
        else:
            return content[:max_length]

    # ...existing code...
