from pydantic import BaseModel, Field
from typing import Optional, List, Literal, Union


class OpenaiInputTokensDetails(BaseModel):
    """Input tokens detailed information for gpt-image-1."""
    prompt_tokens: int = Field(
        ...,
        description="The number of tokens in the input prompt."
    )
    
    image_tokens: int = Field(
        ...,
        description="The number of tokens in the input image."
    )


class OpenaiUsage(BaseModel):
    """Token usage information for gpt-image-1 image generation."""
    input_tokens: int = Field(
        ...,
        description="The number of tokens (images and text) in the input prompt."
    )
    
    input_tokens_details: Optional[OpenaiInputTokensDetails] = Field(
        default=None,
        description="The input tokens detailed information for the image generation."
    )
    
    output_tokens: int = Field(
        ...,
        description="The number of image tokens in the output image."
    )
    
    total_tokens: int = Field(
        ...,
        description="The total number of tokens (images and text) used for the image generation."
    )


class OpenaiImageFile(BaseModel):
    """OpenAI image file output schema."""
    b64_json: Optional[str] = Field(
        default=None, 
        description="The base64-encoded JSON of the generated image. Default value for gpt-image-1, and only present if response_format is set to b64_json for dall-e-2 and dall-e-3."
    )
    
    url: Optional[str] = Field(
        default=None, 
        description="When using dall-e-2 or dall-e-3, the URL of the generated image if response_format is set to url (default value). Unsupported for gpt-image-1."
    )


class OpenaiTextToImageInput(BaseModel):
    """OpenAI input schema for text-to-image generation via images/create endpoint."""
    
    prompt: str = Field(
        ...,
        min_length=1,
        max_length=32000,
        description="A text description of the desired image(s). The maximum length is 32000 characters for gpt-image-1, 1000 characters for dall-e-2 and 4000 characters for dall-e-3."
    )
    
    background: Optional[Literal["transparent", "opaque", "auto"]] = Field(
        default="auto",
        description="Allows to set transparency for the background of the generated image(s). This parameter is only supported for gpt-image-1. Must be one of transparent, opaque or auto (default value). When auto is used, the model will automatically determine the best background for the image. If transparent, the output format needs to support transparency, so it should be set to either png (default value) or webp."
    )
    
    model: Optional[Literal["gpt-image-1"]] = Field(
        default="gpt-image-1",
        description="The model to use for image generation. Only gpt-image-1 is supported."
    )
    
    moderation: Optional[Literal["low", "auto"]] = Field(
        default="auto",
        description="Control the content-moderation level for images generated by gpt-image-1. Must be either low for less restrictive filtering or auto (default value)."
    )
    
    n: Optional[int] = Field(
        default=1,
        ge=1,
        le=10,
        description="The number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported."
    )
    
    output_compression: Optional[int] = Field(
        default=100,
        ge=0,
        le=100,
        description="The compression level (0-100%) for the generated images. This parameter is only supported for gpt-image-1 with the webp or jpeg output formats, and defaults to 100."
    )
    
    output_format: Optional[Literal["png", "jpeg", "webp"]] = Field(
        default="png",
        description="The format in which the generated images are returned. This parameter is only supported for gpt-image-1. Must be one of png, jpeg, or webp."
    )
    
    quality: Optional[Literal["auto", "high", "medium", "low"]] = Field(
        default="auto",
        description="The quality of the image that will be generated. auto (default value) will automatically select the best quality for the given model. high, medium and low are supported for gpt-image-1."
    )
    
    size: Optional[Literal["auto", "1024x1024", "1536x1024", "1024x1536"]] = Field(
        default="auto",
        description="The size of the generated images. Must be one of 1024x1024, 1536x1024 (landscape), 1024x1536 (portrait), or auto (default value) for gpt-image-1."
    )
    
    user: Optional[str] = Field(
        default=None,
        description="A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."
    )


class OpenaiImageEditingInput(BaseModel):
    """OpenAI input schema for image editing via images/createEdit endpoint."""
    
    image: Union[str, List[str]] = Field(
        ...,
        description="The image(s) to edit. Must be a supported image file or an array of images. For gpt-image-1, each image should be a png, webp, or jpg file less than 25MB. You can provide up to 16 images. For dall-e-2, you can only provide one image, and it should be a square png file less than 4MB."
    )
    
    prompt: str = Field(
        ...,
        min_length=1,
        max_length=32000,
        description="A text description of the desired image(s). The maximum length is 1000 characters for dall-e-2, and 32000 characters for gpt-image-1."
    )
    
    background: Optional[Literal["transparent", "opaque", "auto"]] = Field(
        default="auto",
        description="Allows to set transparency for the background of the generated image(s). This parameter is only supported for gpt-image-1. Must be one of transparent, opaque or auto (default value). When auto is used, the model will automatically determine the best background for the image. If transparent, the output format needs to support transparency, so it should be set to either png (default value) or webp."
    )
    
    mask: Optional[str] = Field(
        default=None,
        description="An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where image should be edited. If there are multiple images provided, the mask will be applied on the first image. Must be a valid PNG file, less than 4MB, and have the same dimensions as image."
    )
    
    model: Optional[Literal["gpt-image-1"]] = Field(
        default="gpt-image-1",
        description="The model to use for image generation. Only dall-e-2 and gpt-image-1 are supported. Defaults to dall-e-2 unless a parameter specific to gpt-image-1 is used."
    )
    
    n: Optional[int] = Field(
        default=1,
        ge=1,
        le=10,
        description="The number of images to generate. Must be between 1 and 10."
    )
    
    quality: Optional[Literal["auto", "high", "medium", "low"]] = Field(
        default="auto",
        description="The quality of the image that will be generated. high, medium and low are only supported for gpt-image-1."
    )
    
    size: Optional[Literal["auto", "1024x1024", "1536x1024", "1024x1536"]] = Field(
        default="1024x1024",
        description="The size of the generated images. Must be one of 1024x1024, 1536x1024 (landscape), 1024x1536 (portrait), or auto (default value) for gpt-image-1."
    )
    
    user: Optional[str] = Field(
        default=None,
        description="A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."
    )


class OpenaiImageOutput(BaseModel):
    """OpenAI output schema for image generation, editing, and variation operations."""
    
    created: int = Field(
        ...,
        description="The Unix timestamp (in seconds) of when the image was created."
    )
    
    data: List[OpenaiImageFile] = Field(
        ...,
        description="The list of generated images."
    )
     
    usage: Optional[OpenaiUsage] = Field(
        default=None,
        description="For gpt-image-1 only, the token usage information for the image generation."
    )
