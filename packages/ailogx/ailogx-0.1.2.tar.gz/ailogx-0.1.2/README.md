# ğŸ§  LLM Logger

**LLM Logger** is a Python logging framework designed for seamless integration with **Large Language Models (LLMs)**.  
It produces structured, LLM-friendly logs that can be easily summarized or reasoned about â€” even across massive, deeply nested codebases.

---

## ğŸš€ Features

- ğŸªµ **Structured JSON logs** (timestamped, contextual, machine-readable)
- ğŸ§  **LLM-optimized format** with `reason`, `inputs`, `outputs`, and semantic tags
- ğŸ“‚ **Log grouping** (`start_group`, `end_group`) and function spans
- ğŸ”Œ **Modular LLM backend** support via environment variable:
  - `Ollama` (local models)
  - `Groq` (LLama, Gemma via API)
  - `OpenAI` (GPT-3.5 / GPT-4)
- ğŸ“Š **Summarization CLI** with smart filtering and token-aware chunking
- ğŸ’¾ **Cache** for LLM calls with expiration/cleanup
- ğŸ§ª **Test harness** to simulate deeply nested logs

---

## ğŸ“¦ Installation

```bash
pip install llm-logger
```

## ğŸ› ï¸ Basic Usage

```python
from llm_logger.core import LLMLogger

log = LLMLogger("my-service")

log.llm_info("User login started", inputs={"username": "admin"})
log.llm_decision("Using 2FA", reason="high-risk user")
log.llm_error("Login failed", reason="Invalid OTP")
```

## ğŸ” Function Span

```python
with log.function_span("process_payment", reason="checkout flow"):
    # your logic
    pass
```

## ğŸ“‚ Grouping Logs

```python
log.start_group("req-42", reason="incoming API request")
# your logs here
log.end_group("req-42")
```

## ğŸ“Š LLM Summarization

### ğŸ§  Environment-based Backend Selection

Supports:

- `LLM_LOGGER_BACKEND=ollama` (default)
- `LLM_LOGGER_BACKEND=groq`
- `LLM_LOGGER_BACKEND=openai`

### ğŸ§¾ Example

```bash
export LLM_LOGGER_BACKEND=groq
python -m llm_logger.summarize simulated_logs/deep_nested_logs.jsonl --filter=smart --fast
```

Or call from Python:

```python
from llm_logger.summarizer.summarizer import multi_pass_summarize
from llm_logger.backends.registry import get_analyzer
import json

with open("llm_logs.jsonl") as f:
    logs = [json.loads(line) for line in f]

summary = multi_pass_summarize(logs, get_analyzer())
print(summary)
```

## ğŸ§ª Test Harness

Generate deep, nested logs for benchmarking:

```bash
python llm_logger/core.py
```

Outputs:

- `llm_simulated_logs.jsonl` (LLMLogger)
- `standard_simulated_logs.log` (Python logging)

## ğŸ” Cache & Optimization

- âœ… LLM responses cached to `.cache/`
- ğŸ§  Token-aware chunking
- ğŸ” Smart filtering (`--filter=smart`, `--intent="auth errors"`)
- âš¡ `--fast` mode for shallow summaries before full deep dives