{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory and Diffusion\n",
    "> From Memorization to Generalization with Diffusion Models\n",
    "\n",
    ":::{.callout-warning}\n",
    "## ðŸš§ Under construction\n",
    "\n",
    "This notebook is under construction. It will be completed by July 14, 2025.\n",
    ":::\n",
    "\n",
    "This notebook is a simplified, step-by-step walkthrough of the 2D toy example from the paper: [\"Memorization to Generalization: Emergence of Diffusion Models from Associative Memory\"]((https://arxiv.org/abs/2505.21777)).\n",
    "\n",
    "We will train a score-based diffusion model on a small dataset of points lying on a circle. Our goal is to understand how the model learns the data distribution and to visualize its learned \"energy landscape,\" which reveals how it behaves like an Associative Memory system initially to later transition into a generative model.\n",
    "\n",
    "For more details, please read the [paper](https://arxiv.org/abs/2505.21777) and the [code repository](https://github.com/Lemon-cmd/Diffusion-Models-and-Associative-Memory/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Imports and Setup\"\n",
    "# --- Essential Libraries ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import cycle\n",
    "import os\n",
    "from IPython.display import display\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# --- SciPy for specific math functions ---\n",
    "from scipy.special import i0, i1 # Modified Bessel functions for analytical energy\n",
    "import scipy.integrate as integrate # For ODE solving (likelihood calculation)\n",
    "\n",
    "# --- Scikit-learn for Clustering ---\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Set a nice plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import OrderedDict\n",
    "\n",
    "# For caching trained models\n",
    "CACHE_DIR = Path(\"./cache/02_diffusion_as_memory\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_MODELS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data\n",
    "\n",
    "The paper uses a simple dataset: points sampled from the circumference of a unit circle. This helps us easily visualize how the model learns.\n",
    "\n",
    "We'll define a function to generate these points and a PyTorch `Dataset` class to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circle_data(num_samples=50000, radius=1, seed=59):\n",
    "    \"\"\"Generates data points that lie on a unit circle.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    # Sample angles uniformly from 0 to 2*pi\n",
    "    angles = np.random.uniform(0, 2 * np.pi, num_samples)\n",
    "\n",
    "    # Convert polar coordinates (angles, radius) to Cartesian (x, y)\n",
    "    x = radius * np.cos(angles)\n",
    "    y = radius * np.sin(angles)\n",
    "    return np.stack([x, y], axis=1)\n",
    "\n",
    "class CircleDataset(Dataset):\n",
    "    \"\"\"A PyTorch Dataset to wrap our circle data.\"\"\"\n",
    "    def __init__(self, num_samples=50000, radius=1, seed=9):\n",
    "        # Generate and store the data as a torch tensor\n",
    "        self.data = torch.from_numpy(generate_circle_data(num_samples, radius, seed)).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def create_subset(dataset, sample_size, seed=42):\n",
    "    \"\"\"Create a subset of the dataset based on the specified sample size. \"\"\"\n",
    "    max_size = len(dataset)\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    if not 1 <= sample_size <= len(dataset):\n",
    "        raise ValueError(\"Sample size must be between 1 and the size of the dataset inclusive.\")\n",
    "    subset, _ = torch.utils.data.random_split(\n",
    "        dataset, [sample_size, max_size - sample_size], generator=generator\n",
    "    )\n",
    "    return subset\n",
    "\n",
    "def prepare_datasets(sample_size, train_size=60000, test_size=10000, seed=9):\n",
    "    dataset = CircleDataset(num_samples=train_size, seed=seed)\n",
    "    test_dataset = CircleDataset(num_samples=test_size, seed=seed)\n",
    "    train_subset = create_subset(dataset, sample_size)\n",
    "    test_subset = create_subset(test_dataset, sample_size)\n",
    "    return train_subset, test_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion models can learn from very few samples. In the paper, this is referred to as memorizing \"patterns\". Let's create a tiny dataset with just 9 data points (patterns) to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "SAMPLE_SIZE = 9 # The number of data points (patterns) to memorize\n",
    "BATCH_SIZE = min(500, SAMPLE_SIZE)  # Use all data points in each batch\n",
    "SEED = 9       # For reproducibility\n",
    "\n",
    "# Create the full dataset\n",
    "train_subset, _ = prepare_datasets(SAMPLE_SIZE, seed=SEED)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Extract the training data points for visualization\n",
    "patterns = train_subset.dataset[train_subset.indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our small dataset. These are the specific points we want our model to learn and remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "def plot_circle_data(data, radius=1, figsize=(6, 6)):\n",
    "    \"\"\"Helper function to visualize our data on the circle.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Plot the \"continuous manifold\" (the full circle)\n",
    "    theta = np.linspace(0, 2 * np.pi, 200)\n",
    "    circle_x = radius * np.cos(theta)\n",
    "    circle_y = radius * np.sin(theta)\n",
    "    plt.plot(circle_x, circle_y, label='Continuous Manifold', color='gray', alpha=0.6)\n",
    "\n",
    "    # Plot our training data points (\"patterns\")\n",
    "    plt.scatter(data[:, 0], data[:, 1], color='red', s=150, zorder=3, marker=\"*\",\n",
    "                edgecolor='black', label='Training Data (Patterns)')\n",
    "\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Training Data: {len(data)} Patterns')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize our training data\n",
    "plot_circle_data(patterns.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Diffusion Architecture\n",
    "\n",
    "The diffusion model is simply a model $s_\\theta(\\mathbf{x}_t, t)$ which approximates the `score` function:\n",
    "$$\n",
    "s_\\theta(\\mathbf{x}_t, t) â‰ˆ \\nabla_{\\mathbf{x}_t} \\log p_t (\\mathbf{x}_t)\n",
    "$$\n",
    "over a series of timesteps.\n",
    "\n",
    "In this tutorial, we will be using Variance Exploding `(VE) SDE`, which defines how data is gradually noised over time ranging from $t \\in [\\epsilon, 1]$:\n",
    "$$\n",
    "  \\mathrm{d} \\mathbf{x}_t = \\sigma  \\mathrm{d} \\mathbf{w}_t\n",
    "$$\n",
    "and the corresponding reverse process:\n",
    "$$\n",
    "  \\mathrm{d} \\mathbf{x}_t = \\big [ -\\sigma^2 \\nabla_{\\mathbf{x}_t} \\log p_t (\\mathbf{x}_t) \\big ] \\mathrm{d}t + \\sigma^2 \\mathrm{d} \\mathbf{w}_t\n",
    "$$\n",
    "where $g(t) = \\sigma$ is the `diffusion coefficient` and $\\mathbf{w}_t$ is `brownian motion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VESDETerms:\n",
    "    \"\"\"Defines the terms for the Variance Exploding SDE.\"\"\"\n",
    "    def __init__(self, sigma_max, device=None):\n",
    "        self.sigma = sigma_max\n",
    "        self.device = device\n",
    "\n",
    "    def marginal_prob_std(self, t):\n",
    "        t = torch.as_tensor(t, device=self.device, dtype=torch.float32)\n",
    "        return self.sigma * torch.sqrt(t)\n",
    "\n",
    "    def diffusion_coeff(self, t):\n",
    "        t = torch.as_tensor(t, device=self.device, dtype=torch.float32)\n",
    "        return self.sigma * torch.ones_like(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our score network is a simple Multi-Layer Perceptron (`MLP`). It takes a noisy data point `x` and a time step `t` as inputs, and returns the `estimated score`. The conditioning on time step `t` is performed via the `Fourier embedding`, a standard method of time conditioning in diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        if param.requires_grad == True:\n",
    "            ema_params[name].mul_(decay).add_(param.data, alpha=1. - decay)\n",
    "\n",
    "class FourierEmbedding(torch.nn.Module):\n",
    "    \"\"\"Embeds time `t` into a high-dimensional feature space.\"\"\"\n",
    "    def __init__(self, embed_dim, scale=16):\n",
    "        super().__init__()\n",
    "        self.register_buffer('freqs', torch.randn(embed_dim // 2) * scale)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.ger((2. * torch.pi * self.freqs).to(x.dtype))\n",
    "        x = torch.cat([x.cos(), x.sin()], dim=1)\n",
    "        return x\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    \"\"\"The score-based model (a simple MLP).\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=2,\n",
    "        num_layers=4,\n",
    "        hidden_dim=128,\n",
    "        embed_dim=128,\n",
    "        marginal_prob_std=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = nn.SiLU()\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            FourierEmbedding(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Project combined (x + time-embedding) to hidden dimension\n",
    "        self.input_proj = nn.Linear(input_dim + embed_dim, hidden_dim)\n",
    "\n",
    "        # Hidden MLP layers\n",
    "        layers = []\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "            if _ == num_layers - 2:\n",
    "                layers.append(nn.LayerNorm(hidden_dim))\n",
    "                layers.append(nn.SiLU())\n",
    "            else:\n",
    "                layers.append(nn.SiLU())\n",
    "\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "\n",
    "        # Final output to 2 dimensions\n",
    "        self.output = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Generate time embedding and concatenate with x\n",
    "        t_emb = self.time_embed(t)\n",
    "        h = torch.cat([x, t_emb], dim=1)\n",
    "\n",
    "        # Pass through MLP\n",
    "        h = self.input_proj(h)\n",
    "        h = self.hidden(h)\n",
    "        h = self.output(h)\n",
    "\n",
    "        # Scale by 1 / marginal_prob_std(t)\n",
    "        return h / self.marginal_prob_std(t)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We use the denoising score matching (`DSM`) loss. The goal is to train the `ScoreNet` model so that its output, the score, matches the direction of the noise `z` that was added to the clean data `x` at each time step `t`.\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{\\mathbf{x}_0, \\mathbf{x}_t, t} \\, \\bigg  [ \\lambda(t) \\lVert s_\\theta (\\mathbf{x}_t, t) -  \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t | \\mathbf{x}_0) \\rVert^2 \\bigg ]\n",
    "$$\n",
    "where $\\mathbf{x}_0$ denotes the clean data point and $\\mathbf{x}_t$ is the perturbed data point.\n",
    "\n",
    "For example, assume $\\tilde{\\mathbf{x}} \\sim \\mathcal{N} (\\tilde{\\mathbf{x}} | \\mathbf{x}, \\sigma^2 \\mathbf{I})$ for the simple case of `DSM`. We have the following:\n",
    "$$\n",
    "\\nabla_{\\tilde{\\mathbf{x}}} \\log p(\\tilde{\\mathbf{x}} | \\mathbf{x}) = \\nabla_\\tilde{\\mathbf{x}} \\bigg ( -\\frac{1}{2\\sigma^2} (\\tilde{\\mathbf{x}} - \\mathbf{x})^2 \\bigg ) = -\\frac{\\tilde{\\mathbf{x}} - \\mathbf{x}}{\\sigma^2} = -\\frac{\\epsilon}{\\sigma}\n",
    "$$\n",
    "as the score function, which we have to learn for a single timestep of denoising. In the case of diffusion, our mean $\\mu_\\theta (\\mathbf{x}_t, t)$ is conditioned by time, where $\\mu_\\theta (\\mathbf{x}_0, 0) = \\mathbf{x}$ at `t = 0` when we are back at the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, marginal_prob_std, eps=1e-5):\n",
    "    \"\"\"The denoising score matching loss function.\"\"\"\n",
    "    # Sample a random time t\n",
    "    random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps\n",
    "\n",
    "    # Sample a random noise vector\n",
    "    z = torch.randn_like(x)\n",
    "    std = marginal_prob_std(random_t)[:, None]\n",
    "\n",
    "    # Create the noisy data point\n",
    "    perturbed_x = x + z * std\n",
    "\n",
    "    # Get the model's score prediction\n",
    "    score = model(perturbed_x, random_t)\n",
    "\n",
    "    # Calculate the loss\n",
    "    #loss = torch.mean(torch.sum((score * std + z)**2, dim=1))\n",
    "    loss = torch.mean(torch.square(score * std + z))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader, vesde, iterations=100_000, lr=1e-4, device='cuda', log_freq=10_000):\n",
    "    # create our score model\n",
    "    score_model = ScoreNet(\n",
    "        marginal_prob_std=vesde.marginal_prob_std\n",
    "    )\n",
    "\n",
    "    # create an exponential moving average version of the model\n",
    "    ema = deepcopy(score_model).to(device)\n",
    "    score_model = score_model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(score_model.parameters(), lr=lr)\n",
    "\n",
    "    # Use an infinite data loader to cycle through our small dataset\n",
    "    infinite_loader = iter(cycle(train_loader))\n",
    "\n",
    "    # --- Training ---\n",
    "    score_model.train()\n",
    "    running_loss = 0.\n",
    "    pbar = tqdm(range(iterations))\n",
    "    for iteration in pbar:\n",
    "        # Get a batch of data\n",
    "        x = next(infinite_loader).to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(score_model, x, vesde.marginal_prob_std)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        update_ema(ema, score_model) # udpate ema\n",
    "\n",
    "        # Log progress\n",
    "        if iteration % log_freq == 0 and iteration > 0:\n",
    "            pbar.set_description(f\"Loss: {running_loss / log_freq:.4f}\")\n",
    "            running_loss = 0.\n",
    "\n",
    "    # return the exponential moving average model\n",
    "    ema.eval()\n",
    "    return ema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train! The following code takes a few minutes to run, but the results are cached after the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our SDE-based diffusion models for training data sizes: 2, 9, and 1000.\n",
    "SEED = 9       # For reproducibility\n",
    "LR = 1e-4\n",
    "SIGMA_MAX = 1.\n",
    "ITERATIONS = 50_000\n",
    "\n",
    "# Instantiate the SDE and the Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "vesde = VESDETerms(sigma_max=SIGMA_MAX, device=device)\n",
    "\n",
    "CACHE_MODELS = True\n",
    "\n",
    "def make_cache_name(sample_size, seed, lr, sigma_max, iterations):\n",
    "    return f\"ema_model_{sample_size}_{seed}_{lr}_{sigma_max}_{iterations}.pth\"\n",
    "\n",
    "def get_ema_model(cache_name):\n",
    "    cache_path = CACHE_DIR / cache_name\n",
    "    if cache_path.exists():\n",
    "        model = ScoreNet(marginal_prob_std=vesde.marginal_prob_std)\n",
    "        state_dict = torch.load(cache_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "    else:\n",
    "        return None # Will need to train it\n",
    "\n",
    "data_sizes = [2, 9, 1000] # Takes ~5 min on an M1 Pro CPU\n",
    "ema_set, pattern_set = [], [] # store our ema models and training patterns into two separate lists\n",
    "\n",
    "for sample_size in data_sizes:\n",
    "    ema = None        \n",
    "    cache_name = make_cache_name(sample_size, SEED, LR, SIGMA_MAX, ITERATIONS)\n",
    "    if CACHE_MODELS:\n",
    "        ema = get_ema_model(cache_name)\n",
    "    if ema is None or not CACHE_MODELS: \n",
    "        # Train the model if no cache exists\n",
    "        batch_size = min(500, sample_size)  # Use all data points in each batch\n",
    "        train_subset, _ = prepare_datasets(sample_size, seed=SEED)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # ensure the initialization of our model remains the same\n",
    "        torch.manual_seed(SEED)\n",
    "        ema = train_loop(train_loader, vesde, ITERATIONS, LR, device=device)\n",
    "        torch.save(ema.state_dict(), CACHE_DIR / f\"{cache_name}\")\n",
    "        \n",
    "        \n",
    "    # Extract the training data points for visualization\n",
    "    patterns = train_subset.dataset[train_subset.indices]\n",
    "    pattern_set.append(patterns)\n",
    "    ema_set.append(ema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Trained Model\n",
    "\n",
    "To generate new samples, we run the diffusion process in reverse. We start with pure random noise (sampled at `t=1`) and use our trained score model to guide it back towards the data distribution (towards `t=0`). This is done using a numerical SDE solver, like the Euler-Maruyama method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euler_Maruyama_sampler(score_model,\n",
    "                           sde,\n",
    "                           batch_size=64,\n",
    "                           num_steps=1000,\n",
    "                           device='cuda',\n",
    "                           eps=1e-5):\n",
    "    \"\"\"Generate samples from the score-based model using the Euler-Maruyama solver.\"\"\"\n",
    "    score_model.eval()\n",
    "    t_end = torch.ones(batch_size, device=device)\n",
    "\n",
    "    # Start with orthogonalized random noise ~ N(0, sigma_max^2 * I)\n",
    "    init_x = torch.randn(batch_size, 2, device=device)\n",
    "    init_x = init_x  / torch.norm(init_x, dim = (1), keepdim=True)\n",
    "    init_x = init_x * sde.marginal_prob_std(t_end)[:, None]\n",
    "\n",
    "    time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
    "    step_size = time_steps[0] - time_steps[1] #dt\n",
    "    x = init_x\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for time_step in tqdm(time_steps, desc=\"Sampling\"):\n",
    "            batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "            g = sde.diffusion_coeff(batch_time_step)\n",
    "            # This is the reverse SDE update step\n",
    "            mean_x = x + (g**2)[:, None] * score_model(x, batch_time_step) * step_size\n",
    "            eps = torch.randn_like(x)\n",
    "            noise = torch.sqrt(step_size) * g[:, None] * eps\n",
    "            x = mean_x + noise\n",
    "\n",
    "    score_model.train()\n",
    "    return mean_x # Return the final denoised sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sample from the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_set = []\n",
    "\n",
    "for ema in ema_set:\n",
    "    generated_samples = Euler_Maruyama_sampler(ema, vesde, batch_size=1_000, device=device)\n",
    "    generated_samples = generated_samples.detach().cpu().numpy()\n",
    "    generated_set.append(generated_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Potential Energy of Generated vs. Data Samples\n",
    "\n",
    "Recall the relationship between energy and probability denoted by the `Boltzmann distribution`:\n",
    "$$\n",
    "    p_\\theta(\\mathbf{x}) = \\frac{\\exp{(-E_\\theta(\\mathbf{x}))} }{Z_\\theta}\n",
    "$$\n",
    "This indicates that our energy (up to a constant) is obtained by computing the negative log-likelihood:\n",
    "$$\n",
    "    -\\log p_\\theta(\\mathbf{x}) = E_\\theta(\\mathbf{x}) + C\n",
    "$$\n",
    "\n",
    "Since we are dealing with a non-equilibrium system, that is our diffusion model, we follow the formulations and codes provided in [Song et al. (2021)](https://arxiv.org/abs/2011.13456) to compute the log-likelihood:\n",
    "$$\n",
    "    \\log p_0(\\mathbf{x}_0;\\mathbf{\\theta}) = \\log p_T(\\mathbf{x}_T; \\theta) + \\int_0^T \\nabla \\cdot \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t)  \\mathrm{d}t\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t) =-\\frac{1}{2}\\sigma^2 \\nabla_{\\mathbf{x}_{t}} \\log p_t(\\mathbf{x}_{t}; \\theta)\n",
    "$$\n",
    "for this setting. Keep in mind, $\\nabla \\cdot ()$ denotes the laplacian operation.\n",
    "\n",
    "To derive the above equation, we start with the `Fokker-Planck` equation, as did in [Chen et al. (2018)](https://arxiv.org/abs/1806.07366) and [Song et al. (2021)](https://arxiv.org/abs/2011.13456), which yields the following general probability flow ODE (derived from the forward process SDE):\n",
    "$$\n",
    "    \\mathrm{d} \\mathbf{x}_t = \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t)\\mathrm{d} t + \\tilde{\\mathbf{g}} (\\mathbf{x}_t, t) \\mathrm{d} \\mathbf{w}_t\n",
    "$$\n",
    "where\n",
    "$$\\tilde{\\mathbf{f}}(\\mathbf{x}_t, t) = \\mathbf{f} (\\mathbf{x}_t, t) - \\frac{1}{2} \\nabla \\cdot \\big[\\mathbf{g} (\\mathbf{x}_t, t) \\mathbf{g} (\\mathbf{x}_t, t)^\\top \\big] - \\frac{1}{2}  \\big [ \\mathbf{g} (\\mathbf{x}_t, t) \\mathbf{g} (\\mathbf{x}_t, t)^\\top \\big ] \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)$$\n",
    "and $\\tilde{\\mathbf{g}} (\\mathbf{x}_t, t) = 0$.\n",
    "\n",
    "\n",
    "Here, $\\mathbf{f}(\\mathbf{x}_t, t) = 0$ denotes the drift term which vanishes in the VE setting while $\\mathbf{g} (\\mathbf{x}_t, t) = \\sigma$ which is constant in this setting. Thus, the probability flow ODE for our setting is simply:\n",
    "$$\n",
    "    \\mathrm{d} \\mathbf{x}_t = \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t)\\mathrm{d} t = -\\frac{1}{2}\\sigma^2 \\nabla_{\\mathbf{x}_{t}} \\log p_t(\\mathbf{x}_{t}) \\, \\mathrm{d}t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Laplacian and Log-Likelihood\n",
    "def compute_laplacian(score_fn, x, t):\n",
    "    \"\"\"Compute the Laplacian of the score function.\"\"\"\n",
    "    laplacian = torch.zeros(x.size(0), device=x.device)\n",
    "    with torch.enable_grad():\n",
    "        x.requires_grad_(True)\n",
    "        score = score_fn(x, t)\n",
    "        for i in range(x.shape[1]):\n",
    "            grad_score_i = torch.autograd.grad(score[:, i].sum(), x, create_graph=True)[0][:, i]\n",
    "            laplacian += grad_score_i\n",
    "    x.requires_grad_(False)\n",
    "    return laplacian.detach()\n",
    "\n",
    "\n",
    "def ode_likelihood_with_laplacian(x, score_model, sde, device='cuda', eps=1e-5):\n",
    "    \"\"\"Compute the log-likelihood of x by solving the probability flow ODE.\"\"\"\n",
    "    shape = x.shape\n",
    "\n",
    "    def score_eval_wrapper(sample, time_steps):\n",
    "        \"\"\"A wrapper for evaluating the score-based model for the ODE solver.\"\"\"\n",
    "        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n",
    "        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n",
    "        with torch.no_grad():\n",
    "            score = score_model(sample, time_steps)\n",
    "        return score.cpu().numpy().reshape((-1, shape[1])).astype(np.float64)\n",
    "\n",
    "    def laplacian_eval_wrapper(sample, time_steps):\n",
    "        \"\"\"A wrapper for evaluating the Laplacian of the score function.\"\"\"\n",
    "        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n",
    "        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n",
    "        laplacian = compute_laplacian(score_model, sample, time_steps)\n",
    "        return laplacian.cpu().numpy().reshape((-1,)).astype(np.float64)\n",
    "\n",
    "    def ode_func(t, x_and_logp):\n",
    "        time_steps = torch.from_numpy(np.ones((shape[0],)) * t).to(device, torch.float32)\n",
    "        sample = torch.from_numpy(x_and_logp[:-shape[0]].reshape(shape)).to(device, torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            g = sde.diffusion_coeff(time_steps).cpu().numpy()\n",
    "\n",
    "            score = score_eval_wrapper(sample, time_steps)\n",
    "            laplacian = laplacian_eval_wrapper(sample, time_steps)\n",
    "\n",
    "        drift = -0.5 * g[:, None]**2 * score\n",
    "        logp_grad = -0.5 * g**2 * laplacian\n",
    "        return np.concatenate([drift.flatten(), logp_grad], axis = 0)\n",
    "\n",
    "    init = np.concatenate([x.cpu().numpy().flatten(), np.zeros((shape[0],))])\n",
    "    res = integrate.solve_ivp(ode_func, (eps, 1.), init, rtol=1e-5, atol=1e-5, method='RK45')\n",
    "    zp = torch.tensor(res.y[:, -1], device=device)\n",
    "    z = zp[:-shape[0]].reshape(shape)\n",
    "    delta_logp = zp[-shape[0]:]\n",
    "\n",
    "    sigma_max = sde.marginal_prob_std(torch.tensor(1.))\n",
    "\n",
    "    prior_logp = -shape[1] / 2. * torch.log(2 * np.pi * sigma_max ** 2)\n",
    "    prior_logp = prior_logp - torch.sum(z ** 2, dim=-1) / (2 * sigma_max ** 2)\n",
    "    return (prior_logp + delta_logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Energy\n",
    "\n",
    "Since we are generating quite a lot of synthetic data points, we use `hierarchical clustering` to get a sense of where the concentrations of these new points are at. To be more informative, we are also displaying the energy profile of these concentrations alongside that of the training data points.\n",
    "\n",
    "At `K = 2`, we can see that the concentrations of generated points are pretty much surrounding the data points and their energy profile are similar to that of the training data points.\n",
    "\n",
    "Meanwhile, at `K = 9`, we now see local minima of the energy that devitate drastically from the training points. These new local minima of the energy are called `spurious patterns`.\n",
    "\n",
    "Finally, when `K = 1000`, the energy now very closely matches that of the DenseAM's derived `exact energy`, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_energy(loglikelihood, normalize=True):\n",
    "    # nll = E + C\n",
    "    energy = -loglikelihood\n",
    "    if normalize:\n",
    "         # normalize energy by its minimum\n",
    "        return energy - energy.min()\n",
    "    return energy\n",
    "\n",
    "\n",
    "def plot_combined_landscape(fig, ax, score_model, sde, patterns, samples, labels, centers, t_eval=1e-5, device='cpu', annotate=True):\n",
    "    \"\"\"Visualize the energy landscape with annotated points.\"\"\"\n",
    "    score_model.eval()\n",
    "\n",
    "    # 1. Calculate Energy for Patterns and Centers\n",
    "    # Energy for original patterns\n",
    "    patterns_tensor = patterns.to(device)\n",
    "    logp_patterns = ode_likelihood_with_laplacian(patterns_tensor, score_model, sde, device, eps=t_eval)\n",
    "    energy_patterns = to_energy(logp_patterns).cpu().numpy()\n",
    "\n",
    "    # Energy for found cluster centers\n",
    "    centers_tensor = torch.from_numpy(centers).float().to(device)\n",
    "    logp_centers = ode_likelihood_with_laplacian(centers_tensor, score_model, sde, device, eps=t_eval)\n",
    "    energy_centers = to_energy(logp_centers).cpu().numpy()\n",
    "\n",
    "    # 2. Create a grid of points\n",
    "    bounds=(-1.5, 1.5); resolution=75\n",
    "    x_ = torch.linspace(bounds[0], bounds[1], resolution)\n",
    "    y_ = torch.linspace(bounds[0], bounds[1], resolution)\n",
    "    X, Y = torch.meshgrid(x_, y_, indexing='ij')\n",
    "    grid_tensor = torch.stack([X.flatten(), Y.flatten()], dim=1).to(device)\n",
    "\n",
    "    # 3. Calculate the energy and score for the grid\n",
    "    logp_grid = ode_likelihood_with_laplacian(grid_tensor, score_model, sde, device, eps=t_eval)\n",
    "    energy_grid = to_energy(logp_grid).cpu().numpy().reshape(resolution, resolution)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vec_t = torch.ones(grid_tensor.shape[0], device=device) * t_eval\n",
    "        scores = score_model(grid_tensor, vec_t).cpu().numpy()\n",
    "\n",
    "    # 4. Create the comprehensive plot\n",
    "    #fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.set_title(f'K = {len(patterns)}', fontsize=16)\n",
    "    ax.set_aspect('equal'); ax.grid(False)\n",
    "\n",
    "    # Plot energy contour\n",
    "    contour = ax.contourf(X.cpu(), Y.cpu(), energy_grid, levels=100, cmap='inferno', zorder=0)\n",
    "\n",
    "    # Plot the unit circle\n",
    "    theta = np.linspace(0, 2 * np.pi, 200)\n",
    "    ax.plot(np.cos(theta), np.sin(theta), color='white', linestyle='--', alpha=0.6, zorder=1, label='Unit Circle')\n",
    "\n",
    "    # Plot score field\n",
    "    ax.quiver(grid_tensor[:, 0].cpu(), grid_tensor[:, 1].cpu(),\n",
    "              scores[:, 0], scores[:, 1], color='white', alpha=0.5,\n",
    "              width=0.003, headwidth=3, zorder=2)\n",
    "\n",
    "    # Plot original patterns\n",
    "    ax.scatter(patterns[:, 0].cpu(), patterns[:, 1].cpu(), marker=\"*\", alpha=0.75,\n",
    "               s=400, color=\"deeppink\", label=\"Original Patterns\", edgecolor='black', zorder=5)\n",
    "\n",
    "    if annotate: # turn off annotation and plotting center since there are too many patterns at this point\n",
    "        # Plot generated samples\n",
    "        ax.scatter(samples[:, 0], samples[:, 1], c=labels, cmap='viridis',\n",
    "                   s=15, alpha=0.2, zorder=3, label='Generated Samples')\n",
    "\n",
    "        # Plot found cluster centers\n",
    "        ax.scatter(centers[:, 0], centers[:, 1], marker='X', s=250,\n",
    "                    color='aqua', edgecolor='black', zorder=4, label='Cluster Centers')\n",
    "\n",
    "        # Add energy annotations for patterns\n",
    "        for i, p in enumerate(patterns.cpu().numpy()):\n",
    "            ax.annotate(f'{energy_patterns[i]:.2f}', (p[0], p[1]),\n",
    "                        xytext=(15, -15), textcoords='offset points', color='deeppink', fontsize=10,\n",
    "                        weight='bold', bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"black\", ec=\"lime\", lw=1, alpha=0.6))\n",
    "\n",
    "        # Add energy annotations for cluster centers\n",
    "        for i, c in enumerate(centers):\n",
    "            ax.annotate(f'{energy_centers[i]:.2f}', (c[0], c[1]),\n",
    "                        xytext=(-20, 15), textcoords='offset points', color='aqua', fontsize=10,\n",
    "                        weight='bold', bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"black\", ec=\"yellow\", lw=1, alpha=0.6))\n",
    "\n",
    "    ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "    ax.set_xlim(bounds); ax.set_ylim(bounds)\n",
    "    return contour\n",
    "\n",
    "\n",
    "def plot_energy_surface_3d(fig, ax, X, Y, energy, view_angle=(60, -60), title=None):\n",
    "    \"\"\"\n",
    "    Creates a 3D surface plot of the energy landscape.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Meshgrid for X coordinates.\n",
    "        Y (np.ndarray): Meshgrid for Y coordinates.\n",
    "        energy (np.ndarray): 2D array of energy values.\n",
    "        view_angle (tuple): Tuple of (elevation, azimuth) for the plot's camera angle.\n",
    "    \"\"\"\n",
    "    # Plot the 3D surface\n",
    "    surface = ax.plot_surface(X, Y, energy, cmap='inferno', rstride=1, cstride=1,\n",
    "                              linewidth=0, antialiased=True, alpha=0.9)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('y', fontsize=12)\n",
    "    ax.set_title(title, fontsize=18)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "\n",
    "    # Set a nice viewing angle\n",
    "    ax.view_init(elev=view_angle[0], azim=view_angle[1])\n",
    "    return surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization of the Energy Landscape Across K training Sizes in 2D\n",
    "# This threshold determines how close points need to be to be considered in the same cluster.\n",
    "# You may need to tune this value based on your results.\n",
    "dist_thresholds = [3, 2, 0.45]\n",
    "annotates = [True, True, False]\n",
    "t_evals = [0.15] * len(dist_thresholds)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 8), constrained_layout=True)\n",
    "for i, (patterns, generated_samples, ema, threshold, t_eval, annotate) in enumerate(zip(pattern_set, generated_set, ema_set, dist_thresholds, t_evals, annotates)):\n",
    "    # Perform Agglomerative (Hierarchical) Clustering\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,                                      # We let the algorithm find the clusters based on the threshold\n",
    "        distance_threshold=threshold\n",
    "    ).fit(generated_samples)\n",
    "\n",
    "    # Find the center of each identified cluster\n",
    "    cluster_labels = clustering.labels_\n",
    "    n_clusters_found = len(np.unique(cluster_labels))\n",
    "    #print(f\"Found {n_clusters_found} clusters!\")\n",
    "\n",
    "    cluster_centers = np.array([\n",
    "        generated_samples[cluster_labels == i].mean(axis=0)\n",
    "        for i in range(n_clusters_found)\n",
    "    ])\n",
    "\n",
    "    contour = plot_combined_landscape(\n",
    "        fig, axs[i],\n",
    "        ema,\n",
    "        vesde,\n",
    "        patterns,\n",
    "        generated_samples,\n",
    "        cluster_labels,\n",
    "        cluster_centers,\n",
    "        device=device,\n",
    "        t_eval=t_eval,\n",
    "        annotate=annotate,\n",
    "    )\n",
    "\n",
    "axs[0].legend(loc='lower left', fontsize=12)\n",
    "# Set the ticks to only be at the min and max\n",
    "cbar = fig.colorbar(contour, ax=axs[-1], label='Energy (Lower is Better)', shrink=0.8)\n",
    "vmin, vmax = contour.get_clim()\n",
    "cbar.set_ticks([vmin, vmax])\n",
    "cbar.set_ticklabels(['Low', 'High'], fontsize=14)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization of the Energy Landscape Across K training Sizes in 3D\n",
    "# This code is slow to run, so we cache the figure\n",
    "CACHE_FIG = True\n",
    "\n",
    "bounds = (-1.5, 1.5)\n",
    "resolution = 100\n",
    "X_grid, Y_grid = torch.meshgrid(\n",
    "    torch.linspace(bounds[0], bounds[1], resolution),\n",
    "    torch.linspace(bounds[0], bounds[1], resolution),\n",
    "    indexing='ij'\n",
    ")\n",
    "grid_tensor = torch.stack([X_grid.ravel(), Y_grid.ravel()], dim=1).to(device)\n",
    "\n",
    "fig_fname = CACHE_DIR / \"slow_fig.png\"\n",
    "if CACHE_FIG and fig_fname.exists():\n",
    "    img = PIL.Image.open(str(fig_fname))\n",
    "    display(img)\n",
    "else:\n",
    "    # 1. Create a grid of points (same as before)\n",
    "\n",
    "    # Make Figure\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 8), constrained_layout=True, subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "    for i, (patterns, score_model) in enumerate(zip(pattern_set, ema_set)):\n",
    "        # 2. Calculate the energy for the grid (this is the slow step)\n",
    "        logp_grid = ode_likelihood_with_laplacian(grid_tensor, score_model, vesde, device=device, eps=0.05)\n",
    "        energy_grid = to_energy(logp_grid).cpu().numpy()\n",
    "        energy_grid = energy_grid.reshape(resolution, resolution)\n",
    "\n",
    "        # 3. Create the plot\n",
    "        contour = plot_energy_surface_3d(fig, axs[i], X_grid.cpu().numpy(), Y_grid.cpu().numpy(), energy_grid, title=f'K = {len(patterns)}')\n",
    "\n",
    "    # Set the ticks to only be at the min and max\n",
    "    cbar = fig.colorbar(contour, ax=axs[-1], label='Energy (Lower is Better)', shrink=0.75, pad=0.25)\n",
    "    vmin, vmax = contour.get_clim()\n",
    "    cbar.set_ticks([vmin, vmax])\n",
    "    cbar.set_ticklabels(['Low', 'High'], fontsize=14)\n",
    "    plt.savefig(fig_fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact Energy from Dense Associative Memory\n",
    "\n",
    "Consider the typical `DenseAM`'s energy function which involves the `logsumexp` function:\n",
    "$$\n",
    "E^\\text{AM}(\\mathbf{x}) = -\\beta^{-1} \\log \\bigg[\\sum\\limits_{\\mu=1}^K \\exp\\Big(- \\beta \\lVert \\mathbf{x} - \\boldsymbol{\\xi}^\\mu \\rVert^2_2\\Big) \\bigg]\n",
    "$$\n",
    "which is related to the energy of the diffusion model, derived in [our work](https://arxiv.org/abs/2505.21777):\n",
    "$$\n",
    "E^\\text{DM}(\\mathbf{x}_t, t) = -2 \\sigma^2 t \\log\\bigg[\\sum\\limits_{\\mu=1}^K \\exp \\Big(- \\frac{\\lVert \\mathbf{x}_t - \\boldsymbol{\\xi}^\\mu \\rVert^2_2}{2 \\sigma^2 t}\\Big) \\bigg]\n",
    "$$\n",
    "\n",
    "Both of these energies expressed competitions among the memories (or stored data patterns). But, the main difference is in the value of the `inverse temperature` $\\beta$. In the case of diffusion models, it is alternating over time, i.e., $\\beta_t = \\frac{1}{2 \\sigma^2 t}$, but for DenseAM, this variable is fixed. Nonetheless, although their dynamical trajetories are slightly different, the fixed points (obtained at $t \\approx 0$) of both equations are still the same.\n",
    "\n",
    "Using DenseAM energy, for the case of $K = 2$, we have the following:\n",
    "$$\n",
    "E^\\text{AM}(\\mathbf{x}) = -\\beta^{-1} \\log \\Big[\\exp \\Big(- \\beta \\lVert \\mathbf{x} - \\boldsymbol{\\xi}^1 \\rVert^2_2\\Big) + \\exp \\Big(- \\beta \\lVert \\mathbf{x} - \\boldsymbol{\\xi}^2 \\rVert^2_2 \\Big) \\Big]\n",
    "$$\n",
    "\n",
    "For small finite values of $\\beta$, it is possible for a minimum to exist:\n",
    "$$\n",
    "\\boldsymbol{\\eta} = \\underset{\\mathbf{x}}{\\text{arg min}} \\, E^\\text{AM}(\\mathbf{x})\n",
    "$$\n",
    "such that $\\boldsymbol{\\eta} \\neq \\xi^1$ and $\\boldsymbol{\\eta} \\neq \\xi^2$. This minimum is the `spurious pattern`.\n",
    "\n",
    "Assume that the empirical data distribution is $p(\\mathbf{y}) = \\frac{1}{K}\\sum\\limits_{\\mu=1}^K \\delta^{(N)}(\\mathbf{y} - \\boldsymbol{\\xi}^\\mu)$ where $\\boldsymbol{\\xi}^\\mu$ represents an individual data point (with data size $K$).\n",
    "\n",
    "When the training data size $K \\rightarrow \\infty$, this `empirical data distirbution` becomes a continuous density of states:\n",
    "$$\n",
    "p(\\mathbf{y}) = \\frac{1}{\\pi} \\delta\\big(y_1^2+y_2^2-1\\big)\n",
    "$$\n",
    "The probability of the generated data is then proportional (up to terms independent of the state $\\mathbf{x}$) to\n",
    "$$\n",
    "p(\\mathbf{x}) \\sim \\int\\limits_{-\\infty}^{+\\infty} dy_1 dy_2\\ p(\\mathbf{y})\\ e^{- \\beta \\lVert \\mathbf{x} - \\mathbf{y} \\rVert^2_2} = e^{-\\beta (R^2+1)} I_0(2\\beta R)\n",
    "$$\n",
    "where $I_0(\\cdot)$ is a `modified Bessel function` of the first kind and $R$ is the radius of the circle. Then, the exact energy of our toy model is the following:\n",
    "$$\n",
    "E^\\text{AM}(R, \\phi) =  R^2+1 - \\frac{1}{\\beta} \\log\\big[I_0(2\\beta R)\\big] \\underset{\\beta\\rightarrow\\infty}{\\approx} (R - 1)^2\n",
    "$$\n",
    "given the polar angle $\\phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title The Empirical Energy and Score Function for the Toy Model\n",
    "def cartesian_to_polar(samples):\n",
    "    x, y = samples[:, 0], samples[:, 1]\n",
    "    r = np.sqrt(x ** 2 + y ** 2)\n",
    "    angles = np.arctan2(y, x)\n",
    "    return r, angles\n",
    "\n",
    "def energy_am(samples, beta, normalize=True):\n",
    "    \"\"\"\n",
    "    Computes the energy function\n",
    "        E^AM(R, phi) = R^2 + 1 - (1 / beta) * log(I_0(2 * beta * R))\n",
    "\n",
    "    See Eq. (13) in the paper.\n",
    "    \"\"\"\n",
    "    r, _ = cartesian_to_polar(samples)\n",
    "    energy = r**2 + 1 - (1 / beta) * np.log(i0(2 * beta * r))\n",
    "\n",
    "    # Shift so that the lowest energy is\n",
    "    if normalize:\n",
    "        energy = energy - energy.min()\n",
    "    return energy\n",
    "\n",
    "def score_am(samples, beta, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the score function\n",
    "        S^AM(R, phi) = -2 * R - (2 / beta) * I_1(2 * beta * R) / I_0(2 * beta * R)\n",
    "\n",
    "    See Eq. (18) in the paper.\n",
    "    \"\"\"\n",
    "    r, _ = cartesian_to_polar(samples)\n",
    "\n",
    "    # Ensure r is not zero to avoid division by zero\n",
    "    r = np.clip(r, epsilon, np.inf)\n",
    "\n",
    "    bessel_ratio = i1(2 * beta * r) / i0(2 * beta * r)\n",
    "\n",
    "    score_r = 2 * (bessel_ratio - r)\n",
    "    score = score_r[:, None] * samples / r[:, None]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization of the Empirical Energy Landscape and its Score Function\n",
    "# Inverse Temperature -- Beta\n",
    "beta = 1 / 0.05\n",
    "\n",
    "grid_square = torch.stack([X_grid, Y_grid], dim=1)\n",
    "scores = score_am(grid_square, beta)\n",
    "energy = energy_am(grid_square, beta)\n",
    "energy_grid = energy.reshape(resolution, resolution)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8), constrained_layout=True)\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Exact', fontsize=16)\n",
    "ax.set_aspect('equal'); ax.grid(False)\n",
    "\n",
    "# Plot energy contour\n",
    "contour = ax.contourf(X_grid.cpu(), Y_grid.cpu(), energy_grid, levels=100, cmap='inferno', zorder=0)\n",
    "\n",
    "# Plot the unit circle\n",
    "theta = np.linspace(0, 2 * np.pi, 200)\n",
    "ax.plot(np.cos(theta), np.sin(theta), color='white', linestyle='--', alpha=0.6, zorder=1, label='Unit Circle')\n",
    "\n",
    "# Plot score field\n",
    "ax.quiver(grid_square[:, 0].cpu(), grid_square[:, 1].cpu(),\n",
    "          scores[:, 0], scores[:, 1], color='white', alpha=0.5,\n",
    "          width=0.003, headwidth=3, zorder=2)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "\n",
    "ax = fig.add_subplot(122, projection=\"3d\")\n",
    "plot_energy_surface_3d(fig, ax, X_grid, Y_grid, energy_grid, view_angle=(60, -60), title='Exact')\n",
    "\n",
    "cbar = fig.colorbar(contour, ax=ax, label='Energy (Lower is Better)', shrink=0.8, pad=0.08)\n",
    "vmin, vmax = contour.get_clim()\n",
    "cbar.set_ticks([vmin, vmax])\n",
    "cbar.set_ticklabels(['Low', 'High'], fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "path": "nbs/tutorial/02_diffusion_as_memory.qmd"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
