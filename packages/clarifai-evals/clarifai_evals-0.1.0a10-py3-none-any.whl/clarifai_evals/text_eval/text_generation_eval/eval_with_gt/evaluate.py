from typing import Dict, List, Tuple

from uptrain import EvalLLM, ResponseMatching

from clarifai_evals.constant import DEFAULT_TEXT_GEN_JUDGE_LLM_URL, DataTypes, VisualizationTypes
from clarifai_evals.text_eval.text_generation_eval.base_text_gen_eval import \
    BaseTextGenerationEvaluator


class TextGenMatchEvaluator(BaseTextGenerationEvaluator):
  """Grades how well the response generated by the LLM aligns with the provided ground truth using precision"""

  def __init__(
      self,
      clarifai_pat: str,
      clarifai_model_url: str = DEFAULT_TEXT_GEN_JUDGE_LLM_URL,
  ):
    """Initializes the evaluator object"""
    super().__init__(
        clarifai_pat=clarifai_pat,
        description=
        "Grades how well the response generated by the LLM aligns with the provided ground truth using precision & recall",
        has_explanation=False,
        aggregation_type="mean",
        graph_type=VisualizationTypes.undefined,
        corrective_action=
        "1. Include explicit instructions in the prompt about what information is required and how it should be presented. \n\
          2. Request answers in a structured format that makes it easier for the model to generate precise responses \n\
          3. Apply post-processing techniques to refine the model's output, ensuring it matches the expected response more closely \n\
          4. Include examples of correct responses in the prompt to set a standard for the model.",
        examples=None,
        library="uptrain",
        unit=DataTypes.float_type,
        clarifai_model_url=clarifai_model_url,
    )

  def evaluate(
      self,
      query_batch: List[str],
      predictions_batch: List[str],
      ground_truth_batch: List[str] = None,
      context_batch: List[str] = None,
  ) -> Tuple[Dict, Dict, Dict]:
    """
        This method evaluates the match precision & recall of the predicted text with the ground truth text
        Args:
            ground_truth_batch (List[str]): A list of lists containing the ground truth labels
            query_batch (List[str]): A list of queries
            predictions_batch (List[str]): A list of predictions
        Returns:
            List: Per sample scores
            float: The average scores
            List: Per sample explanations
        """
    data = self.batch_to_uptrain_inputs(query_batch, predictions_batch, ground_truth_batch)
    eval_llm = EvalLLM(self.settings)
    results = eval_llm.evaluate(data=data, checks=[ResponseMatching(method="llm")])
    metric_scores, explanations, summary_scores = {}, {}, {}
    metric_names = ["precision", "recall", "llm"]
    metric_scores = {name: [] for name in metric_names}
    explanations = {name: [] for name in metric_names}
    summary_scores = {name: 0 for name in metric_names}
    for result in results:
      for name in metric_names:
        s = round(result[f"score_response_match_{name}"],
                  4) if result[f"score_response_match_{name}"] else 0
        metric_scores[name].append(s)
        summary_scores[name] += s
      if name == "llm" and "explanation_response_matching" in result:
        explanations[name].append(result["explanation_response_matching"])
    summary_scores = {
        metric: round(summary_scores[metric] / len(results), 4)
        for metric in metric_names
    }
    return metric_scores, summary_scores, explanations

  def get_graph_data(
      self,
      metric_scores: List,
      ground_truth_batch: List[str],
      predictions_batch: List,
  ) -> Dict:
    return None


class TextGenRougeEvaluator(BaseTextGenerationEvaluator):
  """Grades how well the response generated by the LLM aligns with the provided ground truth using rouge"""

  def __init__(
      self,
      clarifai_pat: str,
      clarifai_model_url: str = DEFAULT_TEXT_GEN_JUDGE_LLM_URL,
  ):
    super().__init__(
        clarifai_pat=clarifai_pat,
        description=
        "Grades how well the response generated by the LLM aligns with the provided ground truth using rouge",
        has_explanation=False,
        aggregation_type="mean",
        graph_type=VisualizationTypes.undefined,
        corrective_action=None,
        examples=None,
        library="uptrain",
        unit=DataTypes.float_type,
        clarifai_model_url=clarifai_model_url,
    )

  def evaluate(
      self,
      query_batch: List[str],
      predictions_batch: List[str],
      ground_truth_batch: List[str] = None,
      context_batch: List[str] = None,
  ) -> Tuple[List, float, List]:
    """
        This method evaluates the match rouge score of the predicted text with the ground truth text
        Args:
            ground_truth_batch (List[str]): A list of lists containing the ground truth labels
            query_batch (List[str]): A list of queries
            predictions_batch (List[str]): A list of predictions
        Returns:
            List: Per sample score
            float: The average score
            List: The explanations for each sample
        """
    data = self.batch_to_uptrain_inputs(query_batch, predictions_batch, ground_truth_batch)
    eval_llm = EvalLLM(self.settings)
    results = eval_llm.evaluate(data=data, checks=[ResponseMatching(method="rouge")])
    metric_scores = [
        round(result["score_response_match_rouge"], 4)
        if result["score_response_match_rouge"] else 0 for result in results
    ]
    return metric_scores, round(sum(metric_scores) / len(metric_scores), 4), None

  def get_graph_data(
      self,
      metric_scores: List,
      ground_truth_batch: List[str],
      predictions_batch: List,
  ) -> Dict:
    return None


class TextGenExactMatchEvaluator(BaseTextGenerationEvaluator):
  """Grades how well the response generated by the LLM aligns with the provided ground truth using exact match"""

  def __init__(
      self,
      clarifai_pat: str,
      clarifai_model_url: str = DEFAULT_TEXT_GEN_JUDGE_LLM_URL,
  ):
    super().__init__(
        clarifai_pat=clarifai_pat,
        description=
        "Grades how well the response generated by the LLM aligns with the provided ground truth using exact match",
        has_explanation=False,
        aggregation_type="mean",
        graph_type=VisualizationTypes.undefined,
        corrective_action=None,
        examples=None,
        library="uptrain",
        unit=DataTypes.float_type,
        clarifai_model_url=clarifai_model_url,
    )

  def evaluate(
      self,
      query_batch: List[str],
      predictions_batch: List[str],
      ground_truth_batch: List[str] = None,
      context_batch: List[str] = None,
  ) -> Tuple[List, float, List]:
    """
        This method evaluates the exact match score of the predicted text with the ground truth text
        Args:
            ground_truth_batch (List[str]): A list of lists containing the ground truth labels
            query_batch (List[str]): A list of queries
            predictions_batch (List[str]): A list of predictions
        Returns:
            List: Per sample recall
            float: The average recall
            List: The explanations for each sample
        """
    data = self.batch_to_uptrain_inputs(query_batch, predictions_batch, ground_truth_batch)
    eval_llm = EvalLLM(self.settings)
    results = eval_llm.evaluate(data=data, checks=[ResponseMatching(method="exact")])
    metric_scores = [
        round(result["score_response_match_exact"], 4)
        if result["score_response_match_exact"] else 0 for result in results
    ]
    return metric_scores, round(sum(metric_scores) / len(metric_scores), 4), None

  def get_graph_data(
      self,
      metric_scores: List,
      ground_truth_batch: List[str],
      predictions_batch: List,
  ) -> Dict:
    return None
