*This scratchpad file serves as a phase-specific task tracker and implementation planner. The Mode System on Line 1 is critical and must never be deleted. It defines two core modes: Implementation Type for new feature development and Bug Fix Type for issue resolution. Each mode requires specific documentation formats, confidence tracking, and completion criteria. Use "plan" trigger for planning phase (ðŸŽ¯) and "agent" trigger for execution phase (âš¡) after reaching 95% confidence. Follow strict phase management with clear documentation transfer process.*

`MODE SYSTEM TYPES (DO NOT DELETE!):
1. Implementation Type (New Features):
   - Trigger: User requests new implementation
   - Format: MODE: Implementation, FOCUS: New functionality
   - Requirements: Detailed planning, architecture review, documentation
   - Process: Plan mode (ðŸŽ¯) â†’ 95% confidence â†’ Agent mode (âš¡)

2. Bug Fix Type (Issue Resolution):
   - Trigger: User reports bug/issue
   - Format: MODE: Bug Fix, FOCUS: Issue resolution
   - Requirements: Problem diagnosis, root cause analysis, solution verification
   - Process: Plan mode (ðŸŽ¯) â†’ Chain of thought analysis â†’ Agent mode (âš¡)

Cross-reference with .cursor/memories.md and .cursor/rules/lessons-learned.mdc for context and best practices.`


# Mode: COMPLETED âœ…
Current Task: Fix ruff linting errors in core.py and io_operations.py

## Task Summary:
**Status**: COMPLETED âœ…  
**Result**: All 8 ruff linting errors successfully fixed

## Issues Fixed:
1. **F841 - Unused Variable**: Removed unused `logger` variable in core.py line 63
   - Solution: Removed assignment since `LOGGER` module constant already available
   
2. **B904 - Exception Chaining**: Added proper exception chaining in io_operations.py
   - Fixed 7 raise statements to use `from e` for better error tracebacks
   - Locations: YAML/JSON parsing, file I/O error handling

## Verification:
- âœ… `uv run ruff check --fix *.py llm_ci_runner/` â†’ "All checks passed!"
- âœ… `uv run pytest tests/unit/ -v` â†’ 113/113 tests passing
- âœ… Code quality maintained with all existing comments preserved
- âœ… No functional changes - only linting compliance improvements

## Next Available Tasks:
- Integration test failures (from previous scratchpad context)
- Other development tasks as needed

Confidence: 100% (task completed successfully)


# Mode: PLAN ðŸŽ¯
Current Task: Add native OpenAI support (API-key based) with automatic Azure-first fallback
Understanding: We currently only support AzureChatCompletion via setup_azure_service(). We need a generic service setup that:
1. Prefers Azure when AZURE_OPENAI_* vars are present and valid.
2. Falls back to OpenAIChatCompletion when OPENAI_API_KEY (+ OPENAI_CHAT_MODEL_ID) are set.
3. Keeps public API stable for existing code/tests.
4. Requires changes in azure_service.py, core.py, docs, and new tests.

## Comprehensive Analysis & Implementation Plan

### 1. Environment Variables (CONFIRMED via Semantic Kernel docs)
**Azure OpenAI (current):**
- AZURE_OPENAI_ENDPOINT (required)
- AZURE_OPENAI_MODEL (required) 
- AZURE_OPENAI_API_KEY (optional, fallback to RBAC)
- AZURE_OPENAI_API_VERSION (optional, defaults to "2024-12-01-preview")

**OpenAI (new):**
- OPENAI_API_KEY (required)
- OPENAI_CHAT_MODEL_ID (required - the model name like "gpt-4")
- OPENAI_ORG_ID (optional)
- OPENAI_BASE_URL (optional)

### 2. OpenAI Service Constructor (from Semantic Kernel PyPI docs)
```python
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion

service = OpenAIChatCompletion(
    ai_model_id="gpt-4",        # Required - model name
    api_key="secret-api-key",  # Required - API key
    org_id="org-...",          # Optional - organization ID
    base_url="https://...",    # Optional - custom base URL
    service_id="openai",       # Optional - service identifier
)
```

### 3. Current AI Call Method Analysis
Our current `execute_llm_task()` in `llm_execution.py` uses:
```python
result = await service.get_chat_message_contents(
    chat_history=chat_history,
    settings=settings,
    arguments=args,
)
```
âœ… **CONFIRMED**: OpenAIChatCompletion uses the SAME `get_chat_message_contents()` method as AzureChatCompletion

### 4. Execution Settings Compatibility
Current: `OpenAIChatPromptExecutionSettings()`
âœ… **CONFIRMED**: Same settings class works for both Azure and OpenAI services

### 5. Detailed Implementation Steps

#### Step 1: Rename and Refactor azure_service.py â†’ ai_service.py
```python
# llm_ci_runner/ai_service.py
async def setup_ai_service() -> tuple[AzureChatCompletion | OpenAIChatCompletion, DefaultAzureCredential | None]:
    """
    Setup AI service with automatic Azure-first, OpenAI fallback.
    
    Priority order:
    1. Azure OpenAI (if AZURE_OPENAI_ENDPOINT + AZURE_OPENAI_MODEL present)
    2. OpenAI (if OPENAI_API_KEY + OPENAI_CHAT_MODEL_ID present)
    
    Returns:
        Tuple of (ChatCompletion service, Azure credential or None)
    """
    # Check Azure first
    if os.getenv("AZURE_OPENAI_ENDPOINT") and os.getenv("AZURE_OPENAI_MODEL"):
        return await setup_azure_service()
    
    # Fallback to OpenAI
    elif os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_CHAT_MODEL_ID"):
        return await setup_openai_service()
    
    else:
        raise AuthenticationError(
            "No valid AI service configuration found. Please set either:\n"
            "Azure: AZURE_OPENAI_ENDPOINT + AZURE_OPENAI_MODEL\n"
            "OpenAI: OPENAI_API_KEY + OPENAI_CHAT_MODEL_ID"
        )

async def setup_openai_service() -> tuple[OpenAIChatCompletion, None]:
    """Setup OpenAI service with API key authentication."""
    api_key = os.getenv("OPENAI_API_KEY")
    model_id = os.getenv("OPENAI_CHAT_MODEL_ID")
    org_id = os.getenv("OPENAI_ORG_ID")
    base_url = os.getenv("OPENAI_BASE_URL")
    
    if not api_key:
        raise AuthenticationError("OPENAI_API_KEY environment variable is required")
    if not model_id:
        raise AuthenticationError("OPENAI_CHAT_MODEL_ID environment variable is required")
    
    LOGGER.info(f"ðŸŽ¯ Using OpenAI model: {model_id}")
    if org_id:
        LOGGER.info(f"ðŸŽ¯ Using OpenAI organization: {org_id}")
    if base_url:
        LOGGER.info(f"ðŸŽ¯ Using OpenAI base URL: {base_url}")
    
    try:
        service = OpenAIChatCompletion(
            ai_model_id=model_id,
            api_key=api_key,
            org_id=org_id,
            base_url=base_url,
            service_id="openai",
        )
        LOGGER.info("âœ… OpenAI service setup completed successfully")
        return service, None
        
    except Exception as e:
        raise AuthenticationError(f"Failed to setup OpenAI service: {e}") from e
```

#### Step 2: Update core.py imports and calls
```python
# llm_ci_runner/core.py
from .ai_service import setup_ai_service  # Changed from setup_azure_service

async def main(...):
    # Setup AI service (Azure-first, OpenAI fallback)
    service, credential = await setup_ai_service()  # Changed call
    
    # Rest of code unchanged - same execute_llm_task() call
```

#### Step 3: Update llm_execution.py type hints
```python
# llm_ci_runner/llm_execution.py
from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion

async def execute_llm_task(
    service: AzureChatCompletion | OpenAIChatCompletion,  # Updated type hint
    chat_history: ChatHistory,
    context: dict[str, Any] | None,
    schema_model: type[KernelBaseModel] | None,
) -> str | dict[str, Any]:
    # Implementation unchanged - both services use same get_chat_message_contents() method
```

#### Step 4: Add OpenAI import to __init__.py
```python
# llm_ci_runner/__init__.py
from .ai_service import setup_ai_service, setup_azure_service, setup_openai_service
```

### 6. Testing Strategy

#### Existing Tests (patch setup_azure_service â†’ setup_ai_service):
- tests/unit/test_setup_and_utility_functions.py
- tests/integration/test_main_function.py  
- tests/integration/test_cli_interface.py

#### New Tests to Add:
```python
# tests/unit/test_openai_service.py
@pytest.mark.asyncio
async def test_setup_openai_service_success():
    """Test successful OpenAI service setup."""
    with patch.dict(os.environ, {
        "OPENAI_API_KEY": "non-an-api-key",
        "OPENAI_CHAT_MODEL_ID": "gpt-4"
    }):
        service, credential = await setup_openai_service()
        assert isinstance(service, OpenAIChatCompletion)
        assert credential is None

@pytest.mark.asyncio 
async def test_setup_ai_service_azure_priority():
    """Test Azure takes priority when both configs present."""
    with patch.dict(os.environ, {
        "AZURE_OPENAI_ENDPOINT": "https://test.openai.azure.com/",
        "AZURE_OPENAI_MODEL": "gpt-4",
        "OPENAI_API_KEY": "non-an-api-key",
        "OPENAI_CHAT_MODEL_ID": "gpt-4"
    }):
        service, _ = await setup_ai_service()
        assert isinstance(service, AzureChatCompletion)

@pytest.mark.asyncio
async def test_setup_ai_service_openai_fallback():
    """Test OpenAI fallback when Azure not configured."""
    with patch.dict(os.environ, {
        "OPENAI_API_KEY": "non-an-api-key", 
        "OPENAI_CHAT_MODEL_ID": "gpt-4"
    }, clear=True):
        service, _ = await setup_ai_service()
        assert isinstance(service, OpenAIChatCompletion)
```

#### Integration Tests:
- Reuse existing test data/schemas
- Test both Azure and OpenAI paths with same inputs
- Verify identical output structure

### 7. Documentation Updates

#### README.md:
```markdown
## Environment Variables

The tool supports both Azure OpenAI and OpenAI services with automatic fallback:

### Azure OpenAI (Priority 1)
```bash
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
export AZURE_OPENAI_MODEL="gpt-4"
export AZURE_OPENAI_API_KEY="your-api-key"  # Optional, uses RBAC if not set
```

### OpenAI (Fallback)
```bash
export OPENAI_API_KEY="your-very-secret-api-key" 
export OPENAI_CHAT_MODEL_ID="gpt-4"
export OPENAI_ORG_ID="org-your-org-id"      # Optional
export OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional
```

The tool automatically detects which service to use based on available environment variables.
```

#### examples/ updates:
- Add OpenAI examples alongside Azure examples
- Show both env var configurations
- Demonstrate identical usage patterns

### 8. Confidence Assessment: 95%

**Confirmed Facts:**
âœ… OpenAIChatCompletion uses identical `get_chat_message_contents()` method
âœ… Same OpenAIChatPromptExecutionSettings for both services  
âœ… Semantic Kernel provides both AzureChatCompletion and OpenAIChatCompletion
âœ… Environment variable names confirmed from official docs
âœ… Constructor parameters confirmed from PyPI documentation
âœ… No breaking changes to public API

**Remaining 5% uncertainty:**
- Potential minor differences in error handling between services
- Edge cases in credential management
- Integration test validation needed

### 9. Next Steps for Agent Mode:
1. Rename azure_service.py â†’ ai_service.py with new setup_ai_service()
2. Add setup_openai_service() function
3. Update core.py imports and calls
4. Update type hints in llm_execution.py
5. Add comprehensive unit tests
6. Update documentation and examples
7. Run full test suite validation

**Ready for Agent Mode Implementation** âœ…
