# Condensed: Reranker

Summary: This tutorial provides implementation details for building a two-stage retrieval system with reranking capabilities using the FlagEmbedding library. It covers essential techniques for initial retrieval using vector embeddings with FAISS and subsequent reranking using cross-encoder models. Key implementations include embedding generation, vector index creation, reranker model initialization, and evaluation metric calculations (Recall, MRR). The tutorial demonstrates how to work with different reranker models (ranging from 278M to 2.51B parameters) and includes code for processing queries, computing relevance scores, and reranking results. It's particularly useful for tasks involving semantic search, document retrieval, and ranking optimization, with specific focus on performance tuning through FP16 optimization and batch processing.

*This is a condensed version that preserves essential implementation details and context.*

Here's the condensed tutorial focusing on key implementation details:

# Reranker Implementation Guide

## Overview
Rerankers use cross-encoder architecture to directly score query-text relevance pairs. They're typically used after initial retrieval to improve ranking precision at the cost of speed.

## Key Implementation Steps

### 1. Setup & Dependencies
```python
pip install FlagEmbedding faiss-cpu
```

### 2. Initial Data Processing & Embedding
```python
from FlagEmbedding import FlagModel
from datasets import load_dataset
import numpy as np

# Load data
data = load_dataset("namespace-Pt/msmarco", split="dev")
queries = np.array(data[:100]["query"])
corpus = sum(data[:5000]["positive"], [])

# Initialize embedding model
model = FlagModel('BAAI/bge-base-en-v1.5',
                  query_instruction_for_retrieval="Represent this sentence for searching relevant passages:",
                  use_fp16=True)

# Generate embeddings
corpus_embeddings = model.encode(corpus)
query_embeddings = model.encode_queries(queries)
```

### 3. Vector Index Creation
```python
import faiss

dim = corpus_embeddings.shape[-1]
index = faiss.index_factory(dim, 'Flat', faiss.METRIC_INNER_PRODUCT)
corpus_embeddings = corpus_embeddings.astype(np.float32)
index.train(corpus_embeddings)
index.add(corpus_embeddings)
```

### 4. Reranking Implementation

#### Available Reranker Models
- `bge-reranker-v2-m3`: 568M params, multilingual, lightweight
- `bge-reranker-v2-gemma`: 2.51B params, strong multilingual
- `bge-reranker-large`: 560M params, Chinese/English
- `bge-reranker-base`: 278M params, Chinese/English

```python
from FlagEmbedding import FlagReranker

# Initialize reranker
reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)

# Rerank results
new_ids, new_scores, new_text = [], [], []
for i in range(len(queries)):
    new_score = reranker.compute_score([[queries[i], text] for text in res_text[i]])
    new_id = [tup[1] for tup in sorted(list(zip(new_score, res_ids[i])), reverse=True)]
    new_scores.append(sorted(new_score, reverse=True))
    new_ids.append(new_id)
    new_text.append(corpus[new_id])
```

### 5. Evaluation Metrics

```python
# Recall calculation
def calc_recall(preds, truths, cutoffs):
    recalls = np.zeros(len(cutoffs))
    for text, truth in zip(preds, truths):
        for i, c in enumerate(cutoffs):
            recall = np.intersect1d(truth, text[:c])
            recalls[i] += len(recall) / max(min(len(recall), len(truth)), 1)
    recalls /= len(preds)
    return recalls

# MRR calculation
def MRR(preds, truth, cutoffs):
    mrr = [0 for _ in range(len(cutoffs))]
    for pred, t in zip(preds, truth):
        for i, c in enumerate(cutoffs):
            for j, p in enumerate(pred):
                if j < c and p in t:
                    mrr[i] += 1/(j+1)
                    break
    mrr = [k/len(preds) for k in mrr]
    return mrr
```

## Best Practices
1. Use `use_fp16=True` for faster computation with minimal performance impact
2. Implement two-stage retrieval: first retriever, then reranker
3. Choose reranker model based on language requirements and computational resources
4. Consider batch processing for large-scale reranking operations

## Important Notes
- Rerankers are slower but more accurate than pure embedding-based retrieval
- Different reranker models offer various tradeoffs between speed and accuracy
- Evaluation should consider multiple metrics (Recall, MRR, nDCG) for comprehensive assessment