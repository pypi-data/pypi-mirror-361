{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDB/DBF Conversion Test: Subprocess vs Shell Commands\n",
    "\n",
    "This notebook comprehensively tests MDB and DBF file conversions using two approaches:\n",
    "1. **Test Case 1**: Python subprocess module\n",
    "2. **Test Case 2**: Databricks %sh shell commands\n",
    "\n",
    "Each conversion is run separately to ensure clear results and error isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install PyForge CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyForge CLI with MDB/DBF support\n",
    "%pip install /Volumes/cortex_dev_catalog/sandbox_testing/pkgs/usa-sdandey@deloitte.com/pyforge_cli-1.0.9.dev7-py3-none-any.whl --no-cache-dir --quiet --index-url https://pypi.org/simple/ --trusted-host pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Python to ensure clean import\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MDB and DBF test files (only Parquet format supported)\n",
    "test_files = [\n",
    "    # MDB files\n",
    "    {\n",
    "        \"type\": \"MDB\",\n",
    "        \"name\": \"access_sakila.mdb\",\n",
    "        \"path\": \"/Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/access/small/access_sakila.mdb\",\n",
    "        \"format\": \"parquet\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"MDB\",\n",
    "        \"name\": \"sample_dibi.mdb\",\n",
    "        \"path\": \"/Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/access/small/sample_dibi.mdb\",\n",
    "        \"format\": \"parquet\"\n",
    "    },\n",
    "    # DBF files\n",
    "    {\n",
    "        \"type\": \"DBF\",\n",
    "        \"name\": \"tl_2024_us_county.dbf\",\n",
    "        \"path\": \"/Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/dbf/medium/tl_2024_us_county.dbf\",\n",
    "        \"format\": \"parquet\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"DBF\",\n",
    "        \"name\": \"tl_2024_01_place.dbf\",\n",
    "        \"path\": \"/Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/dbf/small/tl_2024_01_place.dbf\",\n",
    "        \"format\": \"parquet\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Total test files: {len(test_files)}\")\n",
    "print(f\"MDB files: {sum(1 for f in test_files if f['type'] == 'MDB')}\")\n",
    "print(f\"DBF files: {sum(1 for f in test_files if f['type'] == 'DBF')}\")\n",
    "print(f\"\\nAll conversions will output to Parquet format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Environment Verification"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\n\nprint(\"=\" * 70)\nprint(\"ENVIRONMENT VERIFICATION\")\nprint(\"=\" * 70)\nprint(f\"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Working Directory: {os.getcwd()}\")\nprint(f\"IS_SERVERLESS: {os.environ.get('IS_SERVERLESS', 'Not set')}\")\n\n# Check PyForge version\nresult = subprocess.run(['pyforge', '--version'], capture_output=True, text=True)\nprint(f\"\\nPyForge Version: {result.stdout.strip()}\")\n\n# Check Java version\nresult = subprocess.run(['java', '-version'], capture_output=True, text=True)\nif result.stderr:\n    # Extract version outside of f-string to avoid backslash issue\n    java_version = result.stderr.split()[2].strip('\"')\n    print(f\"Java Version: {java_version}\")\nelse:\n    print(\"Java Version: Not found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 1: Subprocess Approach"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Clean up output files that would be generated from test_files\nprint(\"ðŸ§¹ Cleaning up previous test output files...\")\n\n# Generate expected output file paths based on test_files variable\nexpected_outputs = []\n\nfor test_file in test_files:\n    # Extract base name without extension (e.g., \"access_sakila\" from \"access_sakila.mdb\")\n    base_name = test_file['name'].rsplit('.', 1)[0]\n    \n    # PyForge CLI creates outputs in current directory by default\n    # Format: basename.parquet (single file) or basename_parquet/ (directory for multi-table)\n    expected_outputs.extend([\n        f\"{base_name}.parquet\",      # Single parquet file\n        f\"{base_name}_parquet/\"      # Directory for multi-table outputs\n    ])\n\nprint(f\"ðŸ“‹ Checking for output files from {len(test_files)} test files...\")\nprint(f\"    Expected outputs: {len(expected_outputs)} files/directories\")\n\nfiles_removed = 0\ndirs_removed = 0\n\n# Check each expected output\nfor output_path in expected_outputs:\n    try:\n        if output_path.endswith('/'):\n            # Directory path\n            try:\n                dbutils.fs.ls(output_path)\n                # Directory exists, remove it\n                dbutils.fs.rm(output_path, recurse=True)\n                dirs_removed += 1\n                print(f\"   ðŸ—‘ï¸ Directory exists, removed: {output_path}\")\n            except:\n                # Directory doesn't exist - this is normal\n                pass\n        else:\n            # File path  \n            try:\n                dbutils.fs.ls(output_path)\n                # File exists, remove it\n                dbutils.fs.rm(output_path)\n                files_removed += 1\n                print(f\"   ðŸ—‘ï¸ File exists, removed: {output_path}\")\n            except:\n                # File doesn't exist - this is normal\n                pass\n                \n    except Exception as e:\n        print(f\"   âš ï¸ Error checking {output_path}: {e}\")\n\n# Summary\ntotal_removed = files_removed + dirs_removed\nif total_removed == 0:\n    print(\"   âœ… No existing output files found - clean workspace\")\nelse:\n    print(f\"\\nâœ… Cleanup completed: {files_removed} files + {dirs_removed} directories removed\")\n\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cleanup: Remove Previous Test Files",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.1: MDB to Parquet - access_sakila.mdb (subprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.1: Convert access_sakila.mdb to Parquet\n",
    "test_file = test_files[0]  # access_sakila.mdb\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test 1.1: {test_file['type']} to {test_file['format'].upper()} (subprocess)\")\n",
    "print(f\"File: {test_file['name']}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run conversion without verbose flag (logs go to stderr by default)\n",
    "cmd = ['pyforge', 'convert', test_file['path'], '--format', test_file['format'], '--force']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nCommand: {' '.join(cmd)}\")\n",
    "print(f\"\\nExecution Time: {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Return Code: {result.returncode}\")\n",
    "\n",
    "# Show both stdout and stderr for complete information\n",
    "if result.stdout:\n",
    "    print(\"\\n--- Standard Output ---\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "if result.stderr:\n",
    "    print(\"\\n--- Standard Error (includes warnings/logs) ---\")\n",
    "    # Show first 1000 chars to avoid too much output\n",
    "    print(result.stderr[:1000])\n",
    "    if len(result.stderr) > 1000:\n",
    "        print(f\"... (truncated, {len(result.stderr) - 1000} more characters)\")\n",
    "    \n",
    "print(f\"\\nStatus: {'âœ… SUCCESS' if result.returncode == 0 else 'âŒ FAILED'}\")\n",
    "\n",
    "# Check for generated files\n",
    "import glob\n",
    "output_files = glob.glob(\"*sakila*parquet*\")\n",
    "if output_files:\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    for f in output_files:\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.2: MDB to Parquet - sample_dibi.mdb (subprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.2: Convert sample_dibi.mdb to Parquet\n",
    "test_file = test_files[1]  # sample_dibi.mdb\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test 1.2: {test_file['type']} to {test_file['format'].upper()} (subprocess)\")\n",
    "print(f\"File: {test_file['name']}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run conversion\n",
    "cmd = ['pyforge', 'convert', test_file['path'], '--format', test_file['format'], '--force']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nCommand: {' '.join(cmd)}\")\n",
    "print(f\"\\nExecution Time: {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Return Code: {result.returncode}\")\n",
    "\n",
    "if result.stdout:\n",
    "    print(\"\\n--- Standard Output ---\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "if result.stderr:\n",
    "    print(\"\\n--- Standard Error (includes warnings/logs) ---\")\n",
    "    print(result.stderr[:1000])\n",
    "    if len(result.stderr) > 1000:\n",
    "        print(f\"... (truncated, {len(result.stderr) - 1000} more characters)\")\n",
    "    \n",
    "print(f\"\\nStatus: {'âœ… SUCCESS' if result.returncode == 0 else 'âŒ FAILED'}\")\n",
    "\n",
    "# Check for generated files\n",
    "output_files = glob.glob(\"*dibi*parquet*\")\n",
    "if output_files:\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    for f in output_files:\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.3: DBF to Parquet - tl_2024_us_county.dbf (subprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.3: Convert tl_2024_us_county.dbf to Parquet\n",
    "test_file = test_files[2]  # tl_2024_us_county.dbf\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test 1.3: {test_file['type']} to {test_file['format'].upper()} (subprocess)\")\n",
    "print(f\"File: {test_file['name']}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run conversion\n",
    "cmd = ['pyforge', 'convert', test_file['path'], '--format', test_file['format'], '--force']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nCommand: {' '.join(cmd)}\")\n",
    "print(f\"\\nExecution Time: {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Return Code: {result.returncode}\")\n",
    "\n",
    "if result.stdout:\n",
    "    print(\"\\n--- Standard Output ---\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "if result.stderr:\n",
    "    print(\"\\n--- Standard Error (includes warnings/logs) ---\")\n",
    "    print(result.stderr[:1000])\n",
    "    if len(result.stderr) > 1000:\n",
    "        print(f\"... (truncated, {len(result.stderr) - 1000} more characters)\")\n",
    "    \n",
    "print(f\"\\nStatus: {'âœ… SUCCESS' if result.returncode == 0 else 'âŒ FAILED'}\")\n",
    "\n",
    "# Check for generated files\n",
    "output_files = glob.glob(\"*county*parquet*\")\n",
    "if output_files:\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    for f in output_files:\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.4: DBF to Parquet - tl_2024_01_place.dbf (subprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.4: Convert tl_2024_01_place.dbf to Parquet\n",
    "test_file = test_files[3]  # tl_2024_01_place.dbf\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test 1.4: {test_file['type']} to {test_file['format'].upper()} (subprocess)\")\n",
    "print(f\"File: {test_file['name']}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run conversion\n",
    "cmd = ['pyforge', 'convert', test_file['path'], '--format', test_file['format'], '--force']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nCommand: {' '.join(cmd)}\")\n",
    "print(f\"\\nExecution Time: {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Return Code: {result.returncode}\")\n",
    "\n",
    "if result.stdout:\n",
    "    print(\"\\n--- Standard Output ---\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "if result.stderr:\n",
    "    print(\"\\n--- Standard Error (includes warnings/logs) ---\")\n",
    "    print(result.stderr[:1000])\n",
    "    if len(result.stderr) > 1000:\n",
    "        print(f\"... (truncated, {len(result.stderr) - 1000} more characters)\")\n",
    "    \n",
    "print(f\"\\nStatus: {'âœ… SUCCESS' if result.returncode == 0 else 'âŒ FAILED'}\")\n",
    "\n",
    "# Check for generated files\n",
    "output_files = glob.glob(\"*place*parquet*\")\n",
    "if output_files:\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    for f in output_files:\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 2: Shell Command Approach (%sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.1: MDB to Parquet - access_sakila.mdb (%sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"Command: pyforge convert /Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/access/small/access_sakila.mdb --format parquet --force\"\n",
    "echo \"\"\n",
    "# Run without verbose flag and capture all output\n",
    "pyforge convert /Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/access/small/access_sakila.mdb --format parquet --force 2>&1\n",
    "echo \"\"\n",
    "echo \"Exit code: $?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.2: MDB to Parquet - sample_dibi.mdb (%sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"Command: pyforge convert /Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/access/small/sample_dibi.mdb --format parquet --force\"\n",
    "echo \"\"\n",
    "# Run without verbose flag and capture all output\n",
    "pyforge convert /Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/access/small/sample_dibi.mdb --format parquet --force 2>&1\n",
    "echo \"\"\n",
    "echo \"Exit code: $?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.3: DBF to Parquet - tl_2024_us_county.dbf (%sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"Command: pyforge convert /Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/dbf/medium/tl_2024_us_county.dbf --format parquet --force\"\n",
    "echo \"\"\n",
    "# Run without verbose flag and capture all output\n",
    "pyforge convert /Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/dbf/medium/tl_2024_us_county.dbf --format parquet --force 2>&1\n",
    "echo \"\"\n",
    "echo \"Exit code: $?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.4: DBF to Parquet - tl_2024_01_place.dbf (%sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"Command: pyforge convert /Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/dbf/small/tl_2024_01_place.dbf --format parquet --force\"\n",
    "echo \"\"\n",
    "# Run without verbose flag and capture all output\n",
    "pyforge convert /Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/dbf/small/tl_2024_01_place.dbf --format parquet --force 2>&1\n",
    "echo \"\"\n",
    "echo \"Exit code: $?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "import glob\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GENERATED FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for Parquet files\n",
    "parquet_files = glob.glob(\"*.parquet\") + glob.glob(\"*_parquet/*.parquet\")\n",
    "if parquet_files:\n",
    "    print(\"\\nParquet files:\")\n",
    "    for f in sorted(parquet_files):\n",
    "        size = os.path.getsize(f) / 1024\n",
    "        print(f\"  ðŸ“ {f} ({size:.1f} KB)\")\n",
    "else:\n",
    "    print(\"\\nâŒ No Parquet files found\")\n",
    "\n",
    "# Check for directories\n",
    "dirs = [d for d in os.listdir('.') if os.path.isdir(d) and '_parquet' in d]\n",
    "if dirs:\n",
    "    print(\"\\nOutput directories:\")\n",
    "    for d in sorted(dirs):\n",
    "        files = len(os.listdir(d))\n",
    "        total_size = sum(os.path.getsize(os.path.join(d, f)) for f in os.listdir(d)) / 1024 / 1024\n",
    "        print(f\"  ðŸ“‚ {d} ({files} files, {total_size:.1f} MB total)\")\n",
    "        # List files in directory\n",
    "        for f in sorted(os.listdir(d))[:5]:  # Show first 5 files\n",
    "            file_size = os.path.getsize(os.path.join(d, f)) / 1024\n",
    "            print(f\"     - {f} ({file_size:.1f} KB)\")\n",
    "        if len(os.listdir(d)) > 5:\n",
    "            print(f\"     ... and {len(os.listdir(d)) - 5} more files\")\n",
    "else:\n",
    "    print(\"\\nâŒ No output directories found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST EXECUTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nEnvironment: Databricks Serverless\")\n",
    "print(f\"PyForge Version: 1.0.9.dev7\")\n",
    "print(f\"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Test Coverage:\")\n",
    "print(\"  - MDB â†’ Parquet (subprocess & %sh)\")\n",
    "print(\"  - DBF â†’ Parquet (subprocess & %sh)\")\n",
    "print(\"  - Total: 4 file types, 2 execution methods\")\n",
    "\n",
    "print(\"\\nðŸ” Key Observations:\")\n",
    "print(\"  - Both subprocess and %sh methods should produce identical results\")\n",
    "print(\"  - Subprocess captures both stdout and stderr for debugging\")\n",
    "print(\"  - %sh shows output directly but mixing stdout/stderr\")\n",
    "print(\"  - Standard logging information provided without verbose flag\")\n",
    "print(\"  - MDB files use subprocess backend in Serverless (JPype limitation)\")\n",
    "print(\"  - DBF files use native Python reader (no Java required)\")\n",
    "\n",
    "print(\"\\nðŸ“ Logging Notes:\")\n",
    "print(\"  - Warning messages (like 'Table film_text is empty') are normal\")\n",
    "print(\"  - Progress is shown on stderr (not stdout) by design\")\n",
    "print(\"  - Use 2>&1 in %sh to capture all output\")\n",
    "\n",
    "print(\"\\nâœ… All tests demonstrate PyForge CLI functionality in Databricks Serverless\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}