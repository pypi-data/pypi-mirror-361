{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c3cb93",
   "metadata": {},
   "source": [
    "# KARA Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788a7e6",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdfcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kara import KARAUpdater, RecursiveCharacterChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c620d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_chunks(chunks_text):\n",
    "    \"\"\"Pretty print a chunk of text.\"\"\"\n",
    "    for i, chunk in enumerate(chunks_text):\n",
    "        print(f\"Chunk {i + 1}: `{chunk[:90]}...`\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b16d9d",
   "metadata": {},
   "source": [
    "## Document Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76895115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document length: 1315 characters\n"
     ]
    }
   ],
   "source": [
    "# Original document\n",
    "original_doc = (\n",
    "    \"LangChain is an open-source framework built in Python that helps developers create \"\n",
    "    \"applications powered by large language models (LLMs). It allows seamless integration \"\n",
    "    \"between LLMs and external data sources like APIs, files, and databases. With LangChain, \"\n",
    "    \"developers can build dynamic workflows where a language model not only generates text but \"\n",
    "    \"also interacts with tools and environments. This makes it ideal for creating advanced \"\n",
    "    \"chatbots, agents, and AI systems that go beyond static prompting. LangChain provides both \"\n",
    "    \"low-level components for custom logic and high-level abstractions for rapid prototyping, \"\n",
    "    \"making it a versatile toolkit for AI application development.\\n\\n\"\n",
    "    \"Python is the primary language used with LangChain due to its rich ecosystem and \"\n",
    "    \"simplicity. Python's popularity in AI and data science makes it a natural fit for \"\n",
    "    \"building with LangChain. Libraries like pydantic, asyncio, and openai integrate smoothly \"\n",
    "    \"with LangChain, enabling developers to quickly build robust, scalable applications. \"\n",
    "    \"Because LangChain supports modularity, developers can extend it using Python's vast \"\n",
    "    \"collection of libraries. Whether you're building an autonomous agent or a document QA \"\n",
    "    \"tool, Python and LangChain together offer a powerful combination that lowers the barrier \"\n",
    "    \"for building intelligent, interactive systems.\"\n",
    ")\n",
    "\n",
    "print(f\"Original document length: {len(original_doc)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0830c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document length: 1300 characters\n"
     ]
    }
   ],
   "source": [
    "# Updated document (with additional content in the middle)\n",
    "updated_doc = (\n",
    "    \"LangChain is an open-source framework built in Python that helps developers create \"\n",
    "    \"applications powered by large language models (LLMs). It allows seamless integration \"\n",
    "    \"between LLMs and external data sources like APIs, files, and databases. With LangChain, \"\n",
    "    \"developers can build dynamic workflows where a language model not only generates text but \"\n",
    "    \"also interacts with tools and environments. Developers can define step-by-step workflows \"\n",
    "    \"in which an LLM can retrieve data, call APIs, and act based on context. This flexibility \"\n",
    "    \"allows LangChain to support everything from basic assistants to complex, multi-step \"\n",
    "    \"agents capable of reasoning and memory retention.\\n\\n\"\n",
    "    \"Python is the primary language used with LangChain due to its rich ecosystem and \"\n",
    "    \"simplicity. Python's popularity in AI and data science makes it a natural fit for \"\n",
    "    \"building with LangChain. Libraries like pydantic, asyncio, and openai integrate smoothly \"\n",
    "    \"with LangChain, enabling developers to quickly build robust, scalable applications. \"\n",
    "    \"Because LangChain supports modularity, developers can extend it using Python's vast \"\n",
    "    \"collection of libraries. Whether you're building an autonomous agent or a document QA \"\n",
    "    \"tool, Python and LangChain together offer a powerful combination that lowers the barrier \"\n",
    "    \"for building intelligent, interactive systems.\"\n",
    ")\n",
    "\n",
    "print(f\"Updated document length: {len(updated_doc)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1258da",
   "metadata": {},
   "source": [
    "## Initialize KARA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KARA with character-based chunking\n",
    "chunker = RecursiveCharacterChunker(chunk_size=200, separators=[\". \", \"\\n\\n\"])\n",
    "updater = KARAUpdater(chunker=chunker, imperfect_chunk_tolerance=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e818a5",
   "metadata": {},
   "source": [
    "## Step 1: Process Original Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d238c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9 chunks:\n",
      "\n",
      "Chunk 1: `LangChain is an open-source framework built in Python that helps...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2: `It allows seamless integration between LLMs and external data sou...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3: `With LangChain, developers can build dynamic workflows where a la...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4: `This makes it ideal for creating advanced chatbots, agents, and A...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5: `LangChain provides both low-level components for custom logic and...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6: `Python is the primary language used with LangChain due to its ric...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 7: `Libraries like pydantic, asyncio, and openai integrate smoothly w...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 8: `Because LangChain supports modularity, developers can extend it u...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 9: `Whether you're building an autonomous agent or a document QA tool...`\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_result = updater.create_knowledge_base([original_doc])\n",
    "\n",
    "assert initial_result.new_chunked_doc is not None\n",
    "original_chunks = [chunk.content for chunk in initial_result.new_chunked_doc.chunks]\n",
    "\n",
    "print(f\"Created {len(original_chunks)} chunks:\")\n",
    "print()\n",
    "for i, chunk in enumerate(original_chunks, 1):\n",
    "    print(f\"Chunk {i}: `{chunk[:65].strip()}...`\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf2f9e6",
   "metadata": {},
   "source": [
    "## Step 2: Process Updated Document\n",
    "\n",
    "Now let's update the knowledge base with the modified document and see how KARA reuses existing chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785d8b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 9 chunks\n",
      "\n",
      "Chunk 1 [REUSED]: `LangChain is an open-source framework built in Python t...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 [REUSED]: `It allows seamless integration between LLMs and externa...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 [REUSED]: `With LangChain, developers can build dynamic workflows...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4 [NEW]: `Developers can define step-by-step workflows in which a...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5 [NEW]: `This flexibility allows LangChain to support everything...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6 [REUSED]: `Python is the primary language used with LangChain due...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 7 [REUSED]: `Libraries like pydantic, asyncio, and openai integrate...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 8 [REUSED]: `Because LangChain supports modularity, developers can e...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 9 [REUSED]: `Whether you're building an autonomous agent or a docume...`\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = updater.update_knowledge_base(initial_result.new_chunked_doc, [updated_doc])\n",
    "\n",
    "assert result.new_chunked_doc is not None\n",
    "updated_chunks = [chunk.content for chunk in result.new_chunked_doc.chunks]\n",
    "\n",
    "print(f\"Result: {len(updated_chunks)} chunks\")\n",
    "print()\n",
    "for i, chunk in enumerate(updated_chunks, 1):\n",
    "    # Check if this chunk existed before\n",
    "    is_reused = chunk in original_chunks\n",
    "    status = \"REUSED\" if is_reused else \"NEW\"\n",
    "    print(f\"Chunk {i} [{status}]: `{chunk[:55].strip()}...`\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa0575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6abbad38",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Efficiency\n",
    "\n",
    "Let's examine the efficiency gains from using KARA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdff8cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KARA Efficiency Analysis\n",
      "========================================\n",
      "Total chunks in updated document: 9\n",
      "Chunks reused from original: 7\n",
      "Chunks added: 2\n",
      "Chunks deleted: 2\n",
      "\n",
      "Overall efficiency: 7/9 = 77.8%\n"
     ]
    }
   ],
   "source": [
    "# Show efficiency metrics\n",
    "reused_count = result.num_reused\n",
    "total_chunks = len(updated_chunks)\n",
    "efficiency_pct = result.efficiency_ratio\n",
    "\n",
    "print(\"KARA Efficiency Analysis\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total chunks in updated document: {total_chunks}\")\n",
    "print(f\"Chunks reused from original: {reused_count}\")\n",
    "print(f\"Chunks added: {result.num_added}\")\n",
    "print(f\"Chunks deleted: {result.num_deleted}\")\n",
    "print()\n",
    "print(f\"Overall efficiency: {reused_count}/{total_chunks} = {efficiency_pct:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936894e",
   "metadata": {},
   "source": [
    "## Step 4: Multi-Document Support Demo\n",
    "\n",
    "KARA also supports efficient updates across multiple documents. Let's demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6fb4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Processing multiple documents:\n",
      "Document 1: 1315 chars (LangChain overview)\n",
      "Document 2: 153 chars (Python ecosystem)\n",
      "Document 3: 145 chars (Vector databases)\n"
     ]
    }
   ],
   "source": [
    "# Multiple documents with some overlap\n",
    "doc1 = original_doc\n",
    "\n",
    "doc2 = (\n",
    "    \"Python's rich ecosystem makes it ideal for AI development. Libraries like numpy, \"\n",
    "    \"pandas, and scikit-learn integrate seamlessly with LangChain components.\"\n",
    ")\n",
    "\n",
    "doc3 = (\n",
    "    \"Vector databases enable semantic search in RAG applications. They store \"\n",
    "    \"embeddings and allow for efficient similarity-based retrieval of context.\"\n",
    ")\n",
    "\n",
    "print(\"📚 Processing multiple documents:\")\n",
    "print(f\"Document 1: {len(doc1)} chars (LangChain overview)\")\n",
    "print(f\"Document 2: {len(doc2)} chars (Python ecosystem)\")\n",
    "print(f\"Document 3: {len(doc3)} chars (Vector databases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e0f8242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 11 chunks from 3 documents:\n",
      "\n",
      "Chunk 1: `LangChain is an open-source framework built in Python that helps...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2: `It allows seamless integration between LLMs and external data sou...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3: `With LangChain, developers can build dynamic workflows where a la...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4: `This makes it ideal for creating advanced chatbots, agents, and A...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5: `LangChain provides both low-level components for custom logic and...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6: `Python is the primary language used with LangChain due to its ric...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 7: `Libraries like pydantic, asyncio, and openai integrate smoothly w...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 8: `Because LangChain supports modularity, developers can extend it u...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 9: `Whether you're building an autonomous agent or a document QA tool...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 10: `Python's rich ecosystem makes it ideal for AI development. Librar...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 11: `Vector databases enable semantic search in RAG applications. They...`\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process multiple documents\n",
    "multi_result = updater.create_knowledge_base([doc1, doc2, doc3])\n",
    "assert multi_result.new_chunked_doc is not None\n",
    "multi_chunks = [chunk.content for chunk in multi_result.new_chunked_doc.chunks]\n",
    "\n",
    "print(f\"Created {len(multi_chunks)} chunks from 3 documents:\")\n",
    "print()\n",
    "for i, chunk in enumerate(multi_chunks, 1):\n",
    "    print(f\"Chunk {i}: `{chunk[:65].strip()}...`\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6afb4e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating with modified and new documents:\n",
      "   - Document 1: Modified (content added)\n",
      "   - Document 2: Unchanged\n",
      "   - Document 3: Removed\n",
      "   - Document 4: New (RAG explanation)\n"
     ]
    }
   ],
   "source": [
    "# Update with modified documents\n",
    "doc1_updated = updated_doc  # Modified version\n",
    "doc2_updated = doc2  # Unchanged\n",
    "doc4_new = (\n",
    "    \"Retrieval-Augmented Generation (RAG) combines the power of large language \"\n",
    "    \"models with external knowledge retrieval to provide more accurate and \"\n",
    "    \"contextual responses.\"\n",
    ")\n",
    "\n",
    "print(\"Updating with modified and new documents:\")\n",
    "print(\"   - Document 1: Modified (content added)\")\n",
    "print(\"   - Document 2: Unchanged\")\n",
    "print(\"   - Document 3: Removed\")\n",
    "print(\"   - Document 4: New (RAG explanation)\")\n",
    "\n",
    "multi_update_result = updater.update_knowledge_base(\n",
    "    multi_result.new_chunked_doc, [doc1_updated, doc2_updated, doc4_new]\n",
    ")\n",
    "assert multi_update_result.new_chunked_doc is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a960642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Document Update Results:\n",
      "========================================\n",
      "Original documents: 3\n",
      "Updated documents: 3 (1 modified, 1 unchanged, 1 new)\n",
      "\n",
      "Chunks reused: 8\n",
      "Chunks added: 3\n",
      "Chunks deleted: 3\n",
      "\n",
      "Multi-doc efficiency: 72.7%\n"
     ]
    }
   ],
   "source": [
    "# Analyze multi-document efficiency\n",
    "print(\"Multi-Document Update Results:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Original documents: 3\")\n",
    "print(\"Updated documents: 3 (1 modified, 1 unchanged, 1 new)\")\n",
    "print(\"\")\n",
    "print(f\"Chunks reused: {multi_update_result.num_reused}\")\n",
    "print(f\"Chunks added: {multi_update_result.num_added}\")\n",
    "print(f\"Chunks deleted: {multi_update_result.num_deleted}\")\n",
    "print(\"\")\n",
    "print(f\"Multi-doc efficiency: {multi_update_result.efficiency_ratio:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
