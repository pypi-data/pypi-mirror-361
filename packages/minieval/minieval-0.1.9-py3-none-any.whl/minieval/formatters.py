from dataclasses import dataclass

from minieval.datatypes import Formatter, Instance, LMRequest, RequestType


def base_model_chat_template(messages, tokenize=False):
    assert not tokenize
    assert (
        len(messages) == 1
    ), "Basic chat template currently only supports single-turn conversations!"

    message = messages[0]["content"]
    prompt = f"Question: {message}\nAnswer:"
    return prompt


@dataclass
class CoT(Formatter):
    TEMPLATE = "{instruction}{question}"
    REQUEST_TYPE = RequestType.GENERATE

    def __init__(self, instruction: str = None, few_shot_alias: str = None, few_shot_n: int = None, few_shot: list[Instance] = None):
        self.instruction = instruction
        self.few_shot_alias = few_shot_alias
        self.few_shot_n = few_shot_n

    def _build_message(self, instance: Instance) -> LMRequest:
        context = self.TEMPLATE.format(instruction=self.instruction, question=instance.question)

        assert self.instruction != "", self.instruction

        messages = [{"role": "user", "content": context}]

        request = LMRequest(messages=messages)

        return request


@dataclass
class MC(Formatter):
    TEMPLATE = "{question}\nChoices:\n{choices}"
    REQUEST_TYPE = RequestType.LOGPROBS

    def __init__(self, few_shot_alias: str = None, few_shot_n: int = None, few_shot: list[Instance] = None):
        self.few_shot_alias = few_shot_alias
        self.few_shot_n = few_shot_n

    def _build_message(self, instance: Instance) -> LMRequest:
        choices_text = "\n".join(
            f"{chr(65+i)}. {choice}" for i, choice in enumerate(instance.choices)
        )
        question_text = self.TEMPLATE.format(question=instance.question, choices=choices_text)
        messages = [{"role": "user", "content": question_text}]

        # Continuations are " A", " B", ...
        choices = [f" {chr(65+i)}" for i in range(len(instance.choices))]

        request = LMRequest(messages=messages, continuation=choices)

        return request


@dataclass
class RC(Formatter):
    TEMPLATE = "{question}\nChoices:\n{choices}"
    REQUEST_TYPE = RequestType.LOGPROBS

    def __init__(self, few_shot_alias: str = None, few_shot_n: int = None, few_shot: list[Instance] = None):
        self.few_shot_alias = few_shot_alias
        self.few_shot_n = few_shot_n

    def _build_message(self, instance: Instance) -> LMRequest:
        choices_text = "\n".join(
            f"{chr(65+i)}. {choice}" for i, choice in enumerate(instance.choices)
        )
        question_text = self.TEMPLATE.format(question=instance.question, choices=choices_text)
        messages = [{"role": "user", "content": question_text}]

        # Add leading space to continuations
        choices = [" " + choice for choice in instance.choices]

        request = LMRequest(messages=messages, continuation=choices)

        return request


@dataclass
class Continuation(Formatter):
    TEMPLATE = "{question}"
    REQUEST_TYPE = RequestType.LOGPROBS

    def __init__(self, few_shot_alias: str = None, few_shot_n: int = None, few_shot: list[Instance] = None):
        self.few_shot_alias = few_shot_alias
        self.few_shot_n = few_shot_n

    def _build_message(self, instance: Instance) -> LMRequest:
        question_text = self.TEMPLATE.format(question=instance.question)
        messages = [{"role": "user", "content": question_text}]

        # Add leading space to continuations
        choices = [" " + choice for choice in instance.choices]

        request = LMRequest(messages=messages, continuation=choices)

        return request


@dataclass
class PPL(Formatter):
    REQUEST_TYPE = RequestType.LOGPROBS

    def _build_message(self, instance: Instance) -> LMRequest:
        messages = [{"role": "user", "content": ""}]

        request = LMRequest(messages=messages, continuation=instance.gold_completion)

        return request
