{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs + mockstack\n",
    "\n",
    "A few simple examples for using `mockstack` to mock various components in typical LLM-driven use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langchain dependencies with the following or using `uv` depending on your venv setup:\n",
    "\n",
    "#!pip install -q langchain langchain-openai\n",
    "# or:\n",
    "#!uv pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #1: Static template-based mocking (`filefixtures` strategy)\n",
    "\n",
    "Here we simply use the **filefixtures** strategy to route requesets coming in for a certain URL to a template file with the appropriate name.\n",
    "\n",
    "For the below example you'll want to make sure:\n",
    "\n",
    "- mockstack is running at http://localhost:8000 which are the default settings\n",
    "- `MOCKSTACK__TEMPLATES_DIR` is pointing to a valid directory with a template file called `openai-v1-chat-completions.j2`. See the [included file](./templates/openai-v1-chat-completions.j2) with same name in the ./templates sub-directory of this example for a a template with a valid response based on [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat/create).\n",
    "- the `MOCKSTACK__FILEFIXTURES_ENABLE_TEMPLATES_FOR_POST` flag is set to true (which it should be by default)\n",
    "\n",
    "\n",
    "If everything is setup correctly, you should get back the mocked response in the correct format from the template and the below assert should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"http://localhost:8000/openai/v1\",\n",
    "    api_key=\"SOME_STRING_THAT_DOES_NOT_MATTER\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "assert ai_msg.content == \"Hello! How can I assist you today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an identical example, but using OpenAI via Microsoft Azure.\n",
    "\n",
    "The only difference is that under the hood `LangChain` will hit an API URL of the form `/openai/v1/deployments/gpt-4o/chat/completions?api-version=<api-version>`.\n",
    "\n",
    "We can again handle this easily using templates by just creating a file with the name `openai-v1-deployments-gpt-4o-chat-completions.j2`.\n",
    "\n",
    "Remember you can also create conditional logic inside that file using Jinja syntax to return different mock responses depending on the request parameters, for instance using the query parameter `api-version`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"http://localhost:8000/openai/v1\",\n",
    "    api_key=\"SOME_STRING_THAT_DOES_NOT_MATTER\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "assert ai_msg.content == \"Hello! How can I assist you today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #2: Dynamic template-based mocking with ollama integration\n",
    "\n",
    "In this example we use the [ollama-python](https://github.com/ollama/ollama-python) integration to return mock responses that actually come from a real LLM running on your host! This is a great way to go a step further in development, debugging, and integration testing scenarios where LLMs and their non-determinism are important to capture.\n",
    "\n",
    "For this example you will need to make sure you have the following:\n",
    "\n",
    "* mockstack installed with the optional `llm` dependencies:\n",
    "\n",
    "    ```bash\n",
    "    uv pip install mockstack[llm]\n",
    "    ```\n",
    "\n",
    "* [ollama](https://ollama.com/) installed locally with the \"llama3.2\" model (which is typically the default model installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le mockstack est très amusant, mais les LLMs sont beaucoup plus froids.\n",
      "\n",
      "(Note: I used \"froids\" instead of \"cool\" as it's a more common translation in French. If you want to keep the exact word order and meaning, I can suggest an alternative: Le mockstack est très amusant, mais les LLMs sont vraiment cool.)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"http://localhost:8000/ollama/openai/v1\",\n",
    "    api_key=\"SOME_STRING_THAT_DOES_NOT_MATTER\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"mockstack is pretty cool. But LLMs are way cooler\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "# Output would not be deterministic, but should be a valid French translation.\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #3: mocking out a tool call\n",
    "\n",
    "** COMING SOON **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
