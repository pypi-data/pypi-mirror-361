---
description: This lessons-learned file serves as a critical knowledge base for capturing and preventing mistakes. During development, document any reusable solutions, bug fixes, or important patterns. Consult it before implementing any solution.
alwaysApply: false
---
*This lessons-learned file serves as a critical knowledge base for capturing and preventing mistakes. During development, document any reusable solutions, bug fixes, or important patterns using the format: [Timestamp] Category: Issue ‚Üí Solution ‚Üí Impact. Entries must be categorized by priority (Critical/Important/Enhancement) and include clear problem statements, solutions, prevention steps, and code examples. Only update upon user request with "lesson" trigger word. Focus on high-impact, reusable lessons that improve code quality, prevent common errors, and establish best practices. Cross-reference with .cursor\memories.md for context.*

# Lessons Learned

*Note: This file is updated only upon user request and focuses on capturing important, reusable lessons learned during development. Each entry follows format: [Timestamp] Priority: Category ‚Üí Issue: [Problem] ‚Üí Fix: [Solution] ‚Üí Why: [Impact]. Use grep/automated tools to verify changes and prevent regressions.*

## Component Development

[2024-12-30 14:30] Testing Framework: Template evaluation requires different approach than query-response evaluation ‚Üí Use template content as context instead of hardcoded fake queries, focus on output quality rather than relevance ‚Üí Prevents brittle hardcoded queries and makes evaluation more appropriate for template-based examples

[2024-12-30 14:25] Testing Framework: Auto-detection of test mode based on file extension simplifies CLI integration ‚Üí Check .hbs vs .json extension to determine --template-file vs --input-file mode ‚Üí Cleaner code and automatic mode switching without manual configuration

[2024-12-30 14:20] Testing Framework: Priority-based example discovery enables graceful fallback ‚Üí Scan for input.json first, then fall back to template.hbs + schema.yaml ‚Üí Maintains backward compatibility while adding new functionality

## 2025-01-06 - Code Quality and Refactoring

[2025-01-06 23:16] **Critical**: Code Smell Detection and Library Usage ‚Üí Issue: Manual implementation of JSON schema to Pydantic model conversion instead of using dedicated libraries creates 150+ lines of maintenance burden and incomplete feature coverage ‚Üí Fix: Replace manual implementation with json-schema-to-pydantic library, use multiple inheritance to maintain KernelBaseModel functionality ‚Üí Why: Eliminates reinventing-the-wheel anti-pattern, reduces code by 150+ lines, improves robustness with comprehensive JSON schema support including references and combiners, follows DRY principle and reduces maintenance burden. Libraries exist for common problems - use them.

```python
# ‚ùå CODE SMELL - Manual implementation
def _convert_json_schema_field(field_schema: Dict[str, Any], field_name: str) -> tuple:
    # 150+ lines of manual type mapping...
    if field_type == "string":
        if "enum" in field_schema:
            # Manual enum handling...
    # More manual mapping...

# ‚úÖ PROPER - Use dedicated library
from json_schema_to_pydantic import create_model as create_model_from_schema

def create_dynamic_model_from_schema(schema_dict: Dict[str, Any], model_name: str) -> Type[KernelBaseModel]:
    BaseGeneratedModel = create_model_from_schema(schema_dict)
    
    class DynamicKernelModel(KernelBaseModel, BaseGeneratedModel):
        pass
    
    return DynamicKernelModel
```

## 2025-01-06 - Schema Enforcement Implementation

[2025-01-06 22:48] **Critical**: Semantic Kernel ChatHistory Integration ‚Üí Issue: Using kernel.invoke_prompt() with {{$chat_history}} template variable causes "Variable not found" error when chat_history is passed separately ‚Üí Fix: Use service.get_chat_message_contents() directly with chat_history parameter instead of prompt templates ‚Üí Why: Direct service call properly handles ChatHistory objects and avoids template variable resolution issues. Critical for structured output workflows.

```python
# ‚ùå WRONG - Template variable approach fails
result = await kernel.invoke_prompt(
    prompt="{{$chat_history}}",
    arguments=args,
    chat_history=chat_history,
)

# ‚úÖ CORRECT - Direct service call works
result = await service.get_chat_message_contents(
    chat_history=chat_history,
    settings=settings,
    arguments=args,
)
```

[2025-01-06 22:47] **Critical**: Semantic Kernel Response Extraction ‚Üí Issue: Different response objects returned by kernel.invoke_prompt() vs service.get_chat_message_contents() causing extraction failures ‚Üí Fix: Handle both list[ChatMessageContent] and FunctionResult.value patterns with proper type checking ‚Üí Why: Ensures robust response handling across different Semantic Kernel execution paths. Essential for production reliability.

```python
# ‚úÖ ROBUST - Handle both response types
if isinstance(result, list) and len(result) > 0:
    # Direct service call returns list of ChatMessageContent
    response = result[0].content if hasattr(result[0], "content") else str(result[0])
elif hasattr(result, "value") and result.value:
    # Kernel invoke_prompt returns FunctionResult with value
    if isinstance(result.value, list) and len(result.value) > 0:
        response = result.value[0].content if hasattr(result.value[0], "content") else str(result.value[0])
    else:
        response = str(result.value)
else:
    response = str(result)
```

[2025-01-06 22:45] **Important**: Dynamic Pydantic Model Creation ‚Üí Issue: JSON schemas need runtime conversion to KernelBaseModel classes for 100% enforcement ‚Üí Fix: Use pydantic.create_model() with custom field mapping function to convert JSON schema properties to Pydantic field definitions ‚Üí Why: Enables true token-level constraint enforcement through settings.response_format = ModelClass. Achieves 100% schema compliance vs basic JSON mode.

```python
# ‚úÖ PATTERN - Dynamic model creation
def create_dynamic_model_from_schema(schema_dict: Dict[str, Any], model_name: str) -> Type[KernelBaseModel]:
    properties = schema_dict.get("properties", {})
    required_fields = schema_dict.get("required", [])
    field_definitions = {}
    
    for field_name, field_schema in properties.items():
        field_type, field_info = _convert_json_schema_field(field_schema, field_name)
        if field_name in required_fields:
            field_definitions[field_name] = (field_type, field_info)
        else:
            field_definitions[field_name] = (Optional[field_type], Field(default=None, **field_info.extra))
    
    return create_model(model_name, __base__=KernelBaseModel, **field_definitions)
```

[2025-01-06 22:43] **Important**: JSON Schema Field Type Mapping ‚Üí Issue: JSON schema types (string, number, array, etc.) need conversion to Python types with constraint validation ‚Üí Fix: Create comprehensive mapping function handling enums, numeric ranges, array limits, string constraints ‚Üí Why: Ensures all JSON schema validation rules are enforced at the Pydantic model level. Critical for maintaining schema integrity.

```python
# ‚úÖ COMPREHENSIVE - Handle all JSON schema types
def _convert_json_schema_field(field_schema: Dict[str, Any], field_name: str) -> tuple:
    field_type = field_schema.get("type")
    field_kwargs = {"description": field_schema.get("description", "")} if field_schema.get("description") else {}
    
    if field_type == "string":
        python_type = str
        if "enum" in field_schema:
            enum_values = field_schema["enum"]
            field_kwargs["pattern"] = f"^({'|'.join(enum_values)})$"
        if "maxLength" in field_schema:
            field_kwargs["max_length"] = field_schema["maxLength"]
        if "minLength" in field_schema:
            field_kwargs["min_length"] = field_schema["minLength"]
    elif field_type == "number":
        python_type = float
        if "minimum" in field_schema:
            field_kwargs["ge"] = field_schema["minimum"]
        if "maximum" in field_schema:
            field_kwargs["le"] = field_schema["maximum"]
    # ... handle other types
    
    return python_type, Field(**field_kwargs)
```

[2025-01-06 22:42] **Enhancement**: Structured Output Enforcement Architecture ‚Üí Issue: Basic JSON mode ({type: "json_object"}) provides no schema validation ‚Üí Fix: Use settings.response_format = KernelBaseModel for token-level constraint enforcement ‚Üí Why: Achieves 100% schema compliance through Azure OpenAI's structured outputs feature. Eliminates schema validation errors in production CI/CD pipelines.

```python
# ‚ùå WEAK - Basic JSON mode (no enforcement)
settings.response_format = {"type": "json_object"}

# ‚úÖ STRONG - Token-level constraint enforcement
settings.response_format = DynamicPydanticModel  # 100% guaranteed compliance
```


## 2025-01-06 - Comprehensive Testing Infrastructure

[2025-01-06 23:45] **Critical**: Python Metaclass Compatibility Issues ‚Üí Issue: Using Mock objects in tests for KernelBaseModel-based classes causes TypeError due to metaclass conflicts between Mock and BaseModel metaclasses ‚Üí Fix: Replace Mock objects with actual Pydantic BaseModel classes in test fixtures, use @patch decorators with proper BaseModel subclasses ‚Üí Why: Eliminates metaclass conflicts while maintaining test isolation. Critical for testing schema enforcement functionality with KernelBaseModel inheritance. Affected 7 of 18 test failures.

```python
# ‚ùå WRONG - Mock objects cause metaclass conflicts
@patch("llm_ci_runner.create_dynamic_model_from_schema")
def test_schema_function(mock_create_model):
    mock_create_model.return_value = Mock()  # Fails with metaclass conflict
    
# ‚úÖ CORRECT - Use actual BaseModel classes
@patch("llm_ci_runner.create_dynamic_model_from_schema")
def test_schema_function(mock_create_model):
    class MockModel(BaseModel):
        test_field: str = "test"
    mock_create_model.return_value = MockModel
```

[2025-01-06 23:42] **Important**: Realistic Mocking Strategy ‚Üí Issue: Synthetic mocks fail to capture actual API response complexity leading to false test confidence ‚Üí Fix: Generate mocks from actual API responses using debug mode, create mock_factory.py with realistic ChatMessageContent structure including inner_content, metadata, and usage statistics ‚Üí Why: Ensures test mocks match production behavior, catches integration issues early. Based on captured Azure OpenAI API responses with proper structure.

```python
# ‚ùå SYNTHETIC - Oversimplified mocks
def create_mock_response():
    return Mock(content="simple response")

# ‚úÖ REALISTIC - Based on actual API responses
def create_chat_message_content_mock(content: str = "Test response") -> ChatMessageContent:
    return ChatMessageContent(
        role="assistant",
        content=content,
        inner_content=TextContent(text=content),
        metadata={"usage": {"total_tokens": 150}},
        model_id="gpt-4"
    )
```

[2025-01-06 23:40] **Important**: Systematic Test Failure Resolution ‚Üí Issue: 18 failing tests across multiple categories (imports, mocking, business logic) overwhelm debugging efforts ‚Üí Fix: Categorize failures by complexity (Easy: 4 tests, Medium: 7 tests, Complex: 7 tests), tackle systematically starting with easiest wins ‚Üí Why: Achieves 100% test pass rate through organized approach, prevents debugging fatigue, ensures no failures are missed. Methodology scales to larger codebases.

```python
# ‚úÖ SYSTEMATIC APPROACH - Categorize and prioritize
"""
Easy Fixes (4 tests):
- Logger assertion issues
- Mock setup problems  
- Simple return value mismatches

Medium Fixes (7 tests):
- Exception type alignment
- Error message regex patterns
- Import statement updates

Complex Fixes (7 tests):
- Metaclass compatibility
- Schema model mocking
- Retry logic testing
"""
```

[2025-01-06 23:38] **Important**: Test Architecture Design ‚Üí Issue: Monolithic test structure lacks proper separation of concerns and makes maintenance difficult ‚Üí Fix: Implement three-tier architecture: tests/unit/ (heavy mocking), tests/integration/ (minimal mocking), acceptance/ (real API calls with LLM-as-judge) ‚Üí Why: Provides comprehensive coverage at different levels, enables faster unit tests while maintaining end-to-end validation. Follows industry best practices for test pyramid.

```python
# ‚úÖ THREE-TIER ARCHITECTURE
tests/
‚îú‚îÄ‚îÄ unit/                     # 69 tests, heavy mocking, fast execution
‚îÇ   ‚îú‚îÄ‚îÄ test_schema_functions.py
‚îÇ   ‚îú‚îÄ‚îÄ test_semantic_kernel_functions.py
‚îÇ   ‚îî‚îÄ‚îÄ test_input_output_functions.py
‚îú‚îÄ‚îÄ integration/              # End-to-end with realistic mocks
‚îÇ   ‚îú‚îÄ‚îÄ test_examples_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_cli_interface.py
‚îî‚îÄ‚îÄ acceptance/               # Real API calls, quality evaluation
    ‚îî‚îÄ‚îÄ llm_as_judge_acceptance_test.py
```

[2025-01-06 23:36] **Enhancement**: Exception Handling Alignment ‚Üí Issue: Test expectations don't match actual function behavior causing false failures ‚Üí Fix: Analyze actual exception types and messages thrown by functions, update test assertions to match reality ‚Üí Why: Ensures tests validate actual behavior rather than assumed behavior. Prevents brittle tests that break when error messages change.

```python
# ‚ùå ASSUMED - Test expects wrong exception type
def test_invalid_json_schema():
    with pytest.raises(SchemaValidationError):  # Wrong exception type
        load_json_schema("invalid.json")

# ‚úÖ ALIGNED - Test matches actual behavior  
def test_invalid_json_schema():
    with pytest.raises(InputValidationError):  # Actual exception type
        load_json_schema("invalid.json")
```


## 2025-01-07 - Acceptance Test Framework Refactoring

[2025-01-07 00:15] **Critical**: Monolithic Test Framework Anti-Pattern ‚Üí Issue: Custom AcceptanceTestFramework class with 600+ lines violates testing best practices, creates maintenance burden, and lacks proper test isolation ‚Üí Fix: Refactor to pytest-based structure with conftest.py fixtures, Rich formatting, and structured output for LLM-as-judge responses ‚Üí Why: Eliminates custom framework maintenance, follows industry standards, enables easy test extension, provides beautiful visual feedback. Achieves professional test structure with minimal boilerplate.

```python
# ‚ùå ANTI-PATTERN - Custom monolithic framework
class AcceptanceTestFramework:
    def __init__(self):
        self.results = []
        self.temp_files = []
    
    async def judge_response_with_llm(self, query, response, criteria):
        # 150+ lines of custom logic with regex parsing
        pass
    
    def run_llm_ci_runner(self, input_file, output_file):
        # Custom subprocess handling
        pass

# ‚úÖ PROPER - Pytest fixtures with Rich formatting
@pytest.fixture
def llm_judge(llm_ci_runner, temp_files, judgment_schema_path):
    async def _evaluate_response(query, response, criteria):
        # Structured output, no parsing needed
        return structured_judgment
    
    return _evaluate_response

@pytest.fixture  
def rich_test_output():
    def _format_judgment_table(judgment):
        # Beautiful Rich tables and panels
        return table
    
    return {"format_judgment_table": _format_judgment_table}
```

[2025-01-07 00:14] **Important**: String Parsing Anti-Pattern in LLM-as-Judge ‚Üí Issue: Using regex to parse LLM judgment responses causes brittle tests, false failures, and maintenance overhead ‚Üí Fix: Use structured output with JSON schema (judgment_schema.json) for guaranteed parsing reliability ‚Üí Why: Eliminates regex parsing errors, ensures consistent judgment structure, provides type safety, enables easy validation. Critical for reliable acceptance testing.

```python
# ‚ùå BRITTLE - Regex parsing of text responses
def _extract_score(self, text: str, criteria: str) -> int:
    pattern = f"{criteria}.*?(\\d+)"
    match = re.search(pattern, text, re.IGNORECASE)
    return int(match.group(1)) if match else 0

def judge_response_with_llm(self, query, response, criteria):
    # Parse text with regex - error prone!
    pass_fail_match = re.search(r"PASS/FAIL:.*?(PASS|FAIL)", judgment_text)
    actual_decision = pass_fail_match.group(1).upper() if pass_fail_match else "FAIL"

# ‚úÖ ROBUST - Structured output with schema validation
async def _evaluate_response(query, response, criteria):
    # Use judgment_schema.json for structured output
    returncode, stdout, stderr = llm_ci_runner(
        judgment_input_file, 
        judgment_output_file, 
        judgment_schema_path  # Guaranteed structure!
    )
    
    structured_judgment = judgment_result.get("response", {})
    # No parsing needed - direct access to fields
    return structured_judgment
```

[2025-01-07 00:13] **Important**: Test Extensibility Architecture ‚Üí Issue: Adding new acceptance tests requires 50+ lines of boilerplate, custom framework knowledge, and manual setup ‚Üí Fix: Create pytest fixtures (environment_check, llm_ci_runner, llm_judge, temp_files, rich_test_output) that enable new tests with ~20 lines ‚Üí Why: Dramatically reduces test creation effort, follows DRY principle, enables rapid test development, maintains consistency across test suite.

```python
# ‚ùå HIGH BOILERPLATE - Custom framework approach
async def test_new_scenario():
    with AcceptanceTestFramework() as framework:
        # 50+ lines of setup, execution, validation
        input_file = "examples/new-example.json"
        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            output_file = f.name
            framework.temp_files.append(output_file)
        
        returncode, stdout, stderr = framework.run_llm_ci_runner(input_file, output_file)
        # ... more boilerplate
        
        judgment = await framework.judge_response_with_llm(...)
        # ... manual validation

# ‚úÖ MINIMAL BOILERPLATE - Pytest fixtures approach
@pytest.mark.asyncio
async def test_new_scenario(
    self, environment_check, llm_ci_runner, temp_files, llm_judge,
    assert_execution_success, assert_judgment_passed, rich_test_output
):
    # given - just define input and criteria (~10 lines)
    input_file = temp_files(json.dumps(input_data, indent=2))
    output_file = temp_files()
    
    # when - single line execution
    returncode, stdout, stderr = llm_ci_runner(input_file, output_file)
    
    # then - single line validation with Rich output
    assert_judgment_passed(judgment, "Test Name", rich_output=rich_test_output)
```

[2025-01-07 00:12] **Enhancement**: Rich Formatting for Test Output ‚Üí Issue: Basic print statements provide poor user experience and make test results hard to interpret ‚Üí Fix: Implement Rich console formatting with tables, panels, and colored output for professional test feedback ‚Üí Why: Improves developer experience, makes test results visually appealing, enables better debugging, provides clear pass/fail indicators with detailed reasoning.

```python
# ‚ùå BASIC - Simple print statements
print(f"  ‚ùå FAIL: {test_name}")
print(f"    Details: {details}")
print(f"    Full judgment: {judgment.get('full_judgment', '')[:200]}...")

# ‚úÖ RICH - Beautiful formatted output
def _format_judgment_table(judgment: Dict[str, Any]) -> Table:
    table = Table(title="üßë‚Äç‚öñÔ∏è LLM Judge Results")
    table.add_column("Metric", style="cyan")
    table.add_column("Score", style="magenta")
    table.add_column("Status", style="green" if judgment.get("pass") else "red")
    
    for metric in ["relevance", "accuracy", "completeness", "clarity", "overall"]:
        score = judgment.get(metric, 0)
        table.add_row(metric.title(), f"{score}/10", "‚úÖ" if score >= 7 else "‚ùå")
    
    return table

console.print(table)
console.print(Panel(details, title="üìä Detailed Assessment"))
```

[2025-01-07 00:11] **Enhancement**: Pytest Parametrization for Test Efficiency ‚Üí Issue: Testing multiple scenarios requires duplicate test functions or manual iteration ‚Üí Fix: Use @pytest.mark.parametrize for efficient testing of multiple scenarios with shared logic ‚Üí Why: Reduces code duplication, enables comprehensive scenario coverage, maintains test consistency, improves test execution efficiency.

```python
# ‚ùå DUPLICATE - Multiple similar test functions
async def test_python_programming_expertise():
    # 30+ lines of test logic
    pass

async def test_data_science_expertise():
    # 30+ lines of nearly identical test logic
    pass

async def test_machine_learning_expertise():
    # 30+ lines of nearly identical test logic
    pass

# ‚úÖ EFFICIENT - Single parametrized test
@pytest.mark.parametrize("topic,min_score", [
    ("python_programming", 8),
    ("data_science", 7),
    ("machine_learning", 8),
])
@pytest.mark.asyncio
async def test_technical_expertise_topics(
    self, environment_check, llm_ci_runner, temp_files, llm_judge,
    assert_execution_success, assert_judgment_passed, rich_test_output,
    topic, min_score
):
    # Single test function handles all scenarios
    # Dynamic content based on topic parameter
    topic_questions = {
        "python_programming": "Explain list comprehensions...",
        "data_science": "What are key data science steps...",
        "machine_learning": "Explain bias-variance tradeoff..."
    }
    # ... shared test logic
```

## 2025-01-07 - Handlebars Templates and Structured Output

[2025-01-07 01:30] **Critical**: Semantic Kernel YAML Execution Settings Limitation ‚Üí Issue: response_format parameter is NOT supported in YAML execution_settings, only programmatic setting works ‚Üí Fix: Use hybrid approach - load Handlebars YAML template, render with variables, then apply schema enforcement programmatically via settings.response_format = KernelBaseModel ‚Üí Why: Maintains 100% schema enforcement while using Handlebars templates. YAML execution_settings only support temperature, max_tokens, function_choice_behavior - NOT response_format. Critical architectural constraint.

```python
# ‚ùå WRONG - YAML execution_settings don't support response_format
execution_settings:
  default:
    temperature: 0.7
    response_format: MySchema  # NOT SUPPORTED in YAML

# ‚úÖ CORRECT - Hybrid approach
config = PromptTemplateConfig.from_yaml(template_content)
template = HandlebarsPromptTemplate(prompt_template_config=config)
rendered_prompt = await template.render(kernel, KernelArguments(**template_vars))

# Apply schema enforcement programmatically
settings = OpenAIChatPromptExecutionSettings()
settings.response_format = schema_model  # Must be done in code
```

[2025-01-07 01:28] **Important**: Handlebars Template Rendering Integration ‚Üí Issue: Rendered Handlebars templates produce string output that needs conversion to ChatHistory for existing execution pipeline ‚Üí Fix: Create rendered prompt as single ChatMessageContent with user role, maintain existing service.get_chat_message_contents() flow ‚Üí Why: Preserves existing schema enforcement architecture while adding template capability. Avoids rewriting core execution logic.

```python
# ‚úÖ INTEGRATION PATTERN - Template to ChatHistory
rendered_prompt = await template.render(kernel, KernelArguments(**template_vars))

# Convert to ChatHistory for existing pipeline
chat_history = ChatHistory([
    ChatMessageContent(role="user", content=rendered_prompt)
])

# Use existing execution path with schema enforcement
result = await service.get_chat_message_contents(
    chat_history=chat_history,
    settings=settings,  # Contains response_format
    arguments=args
)
```

[2025-01-07 01:26] **Important**: Template Variables Validation Strategy ‚Üí Issue: Handlebars templates can reference variables not provided in template_vars causing rendering failures ‚Üí Fix: Validate template variable requirements against provided variables, surface clear error messages before attempting render ‚Üí Why: Prevents cryptic template rendering errors, improves user experience, enables early validation feedback.

```python
# ‚úÖ VALIDATION PATTERN - Check template variables
def validate_template_variables(template_config: PromptTemplateConfig, provided_vars: dict):
    required_vars = [var.name for var in template_config.input_variables if var.is_required]
    missing_vars = [var for var in required_vars if var not in provided_vars]
    
    if missing_vars:
        raise InputValidationError(
            f"Template requires variables {missing_vars} but only {list(provided_vars.keys())} provided"
        )
```

[2025-01-07 01:24] **Enhancement**: YAML Template Execution Settings Integration ‚Üí Issue: Handlebars YAML templates can define execution_settings (temperature, etc.) that should be merged with schema enforcement settings ‚Üí Fix: Extract execution_settings from template config, merge with programmatic response_format setting ‚Üí Why: Respects template author's intent for model behavior while maintaining schema enforcement. Enables template-specific tuning.

```python
# ‚úÖ MERGE PATTERN - Template settings + schema enforcement
config = PromptTemplateConfig.from_yaml(template_content)

# Extract template execution settings
template_settings = config.execution_settings.get("default", {})

# Create execution settings with schema enforcement
settings = OpenAIChatPromptExecutionSettings()
settings.response_format = schema_model  # Schema enforcement (not in YAML)

# Merge template settings
if "temperature" in template_settings:
    settings.temperature = template_settings["temperature"]
if "max_tokens" in template_settings:
    settings.max_tokens = template_settings["max_tokens"]
# ... other settings
```

[2025-01-17 10:15] Agent Mode Implementation: YAML Support Complete ‚Üí Full feature implementation with comprehensive testing ‚Üí Impact: Production-ready YAML/template functionality achieved

[2025-01-17 10:00] Template Function Testing: Mock PromptTemplateConfig directly ‚Üí Use `PromptTemplateConfig(**yaml_data)` instead of non-existent `from_yaml()` method ‚Üí Impact: Tests pass correctly matching actual SK Python API

[2025-01-17 09:45] Unit Test Debugging: Function name changes require test updates ‚Üí Update all references from `load_input_json` to `load_input_file` and `load_json_schema` to `load_schema_file` ‚Üí Impact: All 93 tests passing after refactoring

[2025-01-17 09:30] Main Function Testing: sys.exit mocking prevents SystemExit ‚Üí Remove sys.exit patches to allow proper exception propagation in tests ‚Üí Impact: KeyboardInterrupt tests pass correctly

[2025-01-17 09:15] Template Implementation: Direct YAML parsing needed ‚Üí Parse YAML manually with `yaml.safe_load()` then create `PromptTemplateConfig(**data)` ‚Üí Impact: Proper template loading without deprecated methods

[2025-01-17 09:00] CLI Mutual Exclusivity: argparse groups enforce constraints ‚Üí Use `add_mutually_exclusive_group()` for --input-file vs --template-file ‚Üí Impact: Clear validation and proper error messages

[2025-01-17 08:45] YAML Output: File extension determines format ‚Üí Check `output_file.suffix.lower() in [".yaml", ".yml"]` to select writer ‚Üí Impact: Seamless format switching based on user intent

[2025-01-17 08:30] Template Execution: Dual main() paths needed ‚Üí Branch execution based on `args.template_file` vs `args.input_file` ‚Üí Impact: Clean separation of template vs direct file workflows

[2025-01-17 08:15] Function Refactoring: Maintain backward compatibility ‚Üí Rename functions (`load_input_json` ‚Üí `load_input_file`) but keep same behavior for JSON ‚Üí Impact: Zero breaking changes for existing users

[2025-01-17 08:00] Code Organization: Follow existing patterns ‚Üí Maintain same error handling, logging style, and function signatures as original code ‚Üí Impact: Consistent codebase quality and maintainability

[2025-01-17 08:15] Function Refactoring: Maintain backward compatibility ‚Üí Rename functions (`load_input_json` ‚Üí `load_input_file`) but keep same behavior for JSON ‚Üí Impact: Zero breaking changes for existing users

[2025-01-17 08:00] Code Organization: Follow existing patterns ‚Üí Maintain same error handling, logging style, and function signatures as original code ‚Üí Impact: Consistent codebase quality and maintainability
