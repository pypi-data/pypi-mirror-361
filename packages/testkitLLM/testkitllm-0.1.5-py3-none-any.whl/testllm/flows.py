"""
Conversation Flow Testing - Multi-step behavioral testing for production agents
"""

import os
import time
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from enum import Enum

from .core import AgentUnderTest
from .semantic import SemanticTest, SemanticTestResult
from .evaluation_loop import EvaluationLoop, EvaluationLoopConfig, SemanticCriterion


class FlowStepType(Enum):
    """Types of flow steps"""
    USER_INPUT = "user_input"
    SYSTEM_CHECK = "system_check" 
    CONDITIONAL = "conditional"
    PARALLEL = "parallel"


@dataclass
class FlowStep:
    """A single step in a conversation flow"""
    step_id: str
    step_type: FlowStepType
    user_input: Optional[str] = None
    criteria: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Behavioral expectations
    expect_context_retention: bool = False
    expect_tool_usage_indicators: List[str] = field(default_factory=list)
    expect_business_logic: List[str] = field(default_factory=list)
    
    # Flow control
    conditional_check: Optional[Callable] = None
    parallel_steps: List['FlowStep'] = field(default_factory=list)


@dataclass
class FlowResult:
    """Result of executing a conversation flow"""
    flow_id: str
    description: str
    steps_executed: int
    total_steps: int
    passed: bool
    overall_score: float
    execution_time: float
    step_results: List[SemanticTestResult] = field(default_factory=list)
    flow_errors: List[str] = field(default_factory=list)
    
    # Flow-specific metrics
    context_retention_score: float = 0.0
    business_logic_score: float = 0.0
    tool_usage_score: float = 0.0


class ConversationFlow:
    """
    Multi-step conversation flow testing for production agentic systems.
    
    This class enables testing of complex workflows, context retention,
    tool usage patterns, and business logic validation through behavioral observation.
    """
    
    def __init__(
        self,
        flow_id: str,
        description: str = "",
        evaluator_models: Optional[List[str]] = None,
        consensus_threshold: float = 0.7,
        config_mode: str = "fast"
    ):
        """
        Initialize a conversation flow test.
        
        Args:
            flow_id: Unique identifier for the flow
            description: Human-readable flow description
            evaluator_models: LLM models for evaluation
            consensus_threshold: Minimum consensus score to pass
            config_mode: Configuration mode ('fast', 'thorough', 'production')
        """
        self.flow_id = flow_id
        self.description = description
        self.steps: List[FlowStep] = []
        self.conversation_history: List[Dict[str, str]] = []
        
        # Evaluation configuration based on mode
        if config_mode == "fast":
            self.config = EvaluationLoopConfig.fast_mode()
        elif config_mode == "thorough":
            self.config = EvaluationLoopConfig.thorough_mode()
        elif config_mode == "production":
            self.config = EvaluationLoopConfig.production_mode()
        else:
            self.config = EvaluationLoopConfig()
        
        # Override with custom parameters if provided
        if evaluator_models:
            self.config.evaluator_models = evaluator_models
        if consensus_threshold != 0.7:
            self.config.consensus_threshold = consensus_threshold
    
    def step(
        self, 
        user_input: str, 
        criteria: List[str],
        step_id: Optional[str] = None,
        expect_context_retention: bool = False,
        expect_tool_usage: Optional[List[str]] = None,
        expect_business_logic: Optional[List[str]] = None,
        **metadata
    ) -> 'ConversationFlow':
        """
        Add a conversation step to the flow.
        
        Args:
            user_input: What the user says in this step
            criteria: Semantic criteria for evaluating the agent's response
            step_id: Optional step identifier
            expect_context_retention: Whether this step should show context awareness
            expect_tool_usage: Expected tool usage indicators in response
            expect_business_logic: Expected business logic patterns
            **metadata: Additional step metadata
            
        Returns:
            Self for method chaining
            
        Example:
            flow.step(
                "Hello, I'm a new customer",
                criteria=[
                    "Response should acknowledge new customer status",
                    "Response should begin onboarding process"
                ],
                expect_context_retention=False,
                expect_business_logic=["customer_onboarding"]
            )
        """
        step_id = step_id or f"step_{len(self.steps) + 1}"
        
        step = FlowStep(
            step_id=step_id,
            step_type=FlowStepType.USER_INPUT,
            user_input=user_input,
            criteria=criteria,
            expect_context_retention=expect_context_retention,
            expect_tool_usage_indicators=expect_tool_usage or [],
            expect_business_logic=expect_business_logic or [],
            metadata=metadata
        )
        
        self.steps.append(step)
        return self
    
    def conditional_step(
        self,
        condition_check: Callable[[str], bool],
        true_step: FlowStep,
        false_step: Optional[FlowStep] = None
    ) -> 'ConversationFlow':
        """
        Add a conditional step based on previous agent response.
        
        Args:
            condition_check: Function that takes agent response and returns bool
            true_step: Step to execute if condition is true
            false_step: Step to execute if condition is false
            
        Returns:
            Self for method chaining
        """
        conditional_step = FlowStep(
            step_id=f"conditional_{len(self.steps) + 1}",
            step_type=FlowStepType.CONDITIONAL,
            conditional_check=condition_check,
            metadata={"true_step": true_step, "false_step": false_step}
        )
        
        self.steps.append(conditional_step)
        return self
    
    def parallel_steps(self, *steps: FlowStep) -> 'ConversationFlow':
        """
        Add multiple steps that can be executed in parallel.
        
        Args:
            *steps: Multiple FlowStep objects to execute concurrently
            
        Returns:
            Self for method chaining
        """
        parallel_step = FlowStep(
            step_id=f"parallel_{len(self.steps) + 1}",
            step_type=FlowStepType.PARALLEL,
            parallel_steps=list(steps)
        )
        
        self.steps.append(parallel_step)
        return self
    
    def context_check(
        self,
        user_input: str,
        context_criteria: List[str],
        step_id: Optional[str] = None
    ) -> 'ConversationFlow':
        """
        Add a step specifically to test context retention.
        
        Args:
            user_input: Input that requires context from previous steps
            context_criteria: Criteria specifically about context awareness
            step_id: Optional step identifier
            
        Returns:
            Self for method chaining
            
        Example:
            flow.context_check(
                "What was my name again?",
                criteria=[
                    "Response should remember the name from earlier",
                    "Response should show conversation awareness"
                ]
            )
        """
        return self.step(
            user_input=user_input,
            criteria=context_criteria,
            step_id=step_id,
            expect_context_retention=True
        )
    
    def tool_usage_check(
        self,
        user_input: str,
        expected_tools: List[str],
        criteria: List[str],
        step_id: Optional[str] = None
    ) -> 'ConversationFlow':
        """
        Add a step to test tool usage patterns.
        
        Args:
            user_input: Input that should trigger tool usage
            expected_tools: List of expected tool usage indicators
            criteria: Criteria for evaluating tool usage in response
            step_id: Optional step identifier
            
        Returns:
            Self for method chaining
            
        Example:
            flow.tool_usage_check(
                "Book me a flight to NYC",
                expected_tools=["flight_search", "availability_check"],
                criteria=[
                    "Response should indicate searching for flights",
                    "Response should show tool usage without exposing internals"
                ]
            )
        """
        return self.step(
            user_input=user_input,
            criteria=criteria,
            step_id=step_id,
            expect_tool_usage=expected_tools
        )
    
    def business_logic_check(
        self,
        user_input: str,
        business_rules: List[str],
        criteria: List[str],
        step_id: Optional[str] = None
    ) -> 'ConversationFlow':
        """
        Add a step to test business logic compliance.
        
        Args:
            user_input: Input that should trigger business logic
            business_rules: List of business rules that should be applied
            criteria: Criteria for evaluating business logic in response
            step_id: Optional step identifier
            
        Returns:
            Self for method chaining
        """
        return self.step(
            user_input=user_input,
            criteria=criteria,
            step_id=step_id,
            expect_business_logic=business_rules
        )
    
    async def execute(self, agent: AgentUnderTest) -> FlowResult:
        """
        Execute the conversation flow against the agent.
        
        Args:
            agent: The agent instance to test
            
        Returns:
            FlowResult with detailed flow execution results
        """
        start_time = time.time()
        evaluator = EvaluationLoop(self.config)
        
        # Reset agent and conversation history
        agent.reset_conversation()
        self.conversation_history = []
        
        step_results = []
        flow_errors = []
        steps_executed = 0
        
        try:
            for i, step in enumerate(self.steps):
                step_start = time.time()
                step_result = await self._execute_step(agent, step, evaluator)
                step_end = time.time()
                
                if step_result:
                    step_results.append(step_result)
                    steps_executed += 1
                    
                    # Add to conversation history
                    self.conversation_history.append({
                        "step_id": step.step_id,
                        "user_input": step.user_input or "",
                        "agent_response": step_result.agent_response,
                        "passed": step_result.passed
                    })
        
        except Exception as e:
            flow_errors.append(f"Flow execution error: {str(e)}")
        
        # Calculate overall metrics
        overall_score = (
            sum(r.overall_score for r in step_results) / len(step_results)
            if step_results else 0.0
        )
        
        passed = all(r.passed for r in step_results) and not flow_errors
        
        # Calculate flow-specific metrics
        context_score = self._calculate_context_retention_score(step_results)
        business_score = self._calculate_business_logic_score(step_results)
        tool_score = self._calculate_tool_usage_score(step_results)
        
        return FlowResult(
            flow_id=self.flow_id,
            description=self.description,
            steps_executed=steps_executed,
            total_steps=len(self.steps),
            passed=passed,
            overall_score=overall_score,
            execution_time=time.time() - start_time,
            step_results=step_results,
            flow_errors=flow_errors,
            context_retention_score=context_score,
            business_logic_score=business_score,
            tool_usage_score=tool_score
        )
    
    def execute_sync(self, agent: AgentUnderTest) -> FlowResult:
        """
        Synchronous wrapper for execute() method.
        
        Args:
            agent: The agent instance to test
            
        Returns:
            FlowResult with detailed flow execution results
        """
        import asyncio
        return asyncio.run(self.execute(agent))
    
    async def _execute_step(
        self, 
        agent: AgentUnderTest, 
        step: FlowStep, 
        evaluator: EvaluationLoop
    ) -> Optional[SemanticTestResult]:
        """Execute a single flow step"""
        
        if step.step_type == FlowStepType.USER_INPUT:
            return await self._execute_user_input_step(agent, step, evaluator)
        elif step.step_type == FlowStepType.CONDITIONAL:
            return await self._execute_conditional_step(agent, step, evaluator)
        elif step.step_type == FlowStepType.PARALLEL:
            return await self._execute_parallel_steps(agent, step, evaluator)
        else:
            return None
    
    async def _execute_user_input_step(
        self,
        agent: AgentUnderTest,
        step: FlowStep,
        evaluator: EvaluationLoop
    ) -> SemanticTestResult:
        """Execute a user input step"""
        start_time = time.time()
        
        try:
            # Get agent response
            agent_start = time.time()
            agent_response = agent.send_message(step.user_input)
            agent_end = time.time()
            
            # Build enhanced criteria including flow-specific checks
            enhanced_criteria = step.criteria.copy()
            
            # Add context retention criteria if expected
            if step.expect_context_retention and self.conversation_history:
                enhanced_criteria.append(
                    "Response should demonstrate awareness of previous conversation context"
                )
            
            # Add tool usage criteria if expected
            if step.expect_tool_usage_indicators:
                for tool in step.expect_tool_usage_indicators:
                    enhanced_criteria.append(
                        f"Response should indicate usage of {tool} functionality"
                    )
            
            # Add business logic criteria if expected
            if step.expect_business_logic:
                for rule in step.expect_business_logic:
                    enhanced_criteria.append(
                        f"Response should demonstrate {rule} business logic"
                    )
            
            # Convert to SemanticCriterion objects
            semantic_criteria = [
                SemanticCriterion(criterion=criterion)
                for criterion in enhanced_criteria
            ]
            
            # Evaluate with LLMs
            eval_start = time.time()
            consensus_results = await evaluator.evaluate_response(
                step.user_input,
                agent_response,
                semantic_criteria
            )
            eval_end = time.time()
            
            # Calculate results
            passed = all(result.passed for result in consensus_results)
            overall_score = (
                sum(result.consensus_score for result in consensus_results) 
                / len(consensus_results) if consensus_results else 0.0
            )
            
            # Show detailed evaluation when running tests
            if self._is_in_test_environment():
                self._print_step_evaluation(step, agent_response, consensus_results, passed, overall_score)
            
            # Format criterion results
            criterion_results = []
            for consensus_result in consensus_results:
                criterion_results.append({
                    "criterion": consensus_result.criterion,
                    "passed": consensus_result.passed,
                    "consensus_score": consensus_result.consensus_score,
                    "evaluations": [
                        {
                            "evaluator": eval_result.evaluator_model,
                            "decision": eval_result.decision,
                            "confidence": eval_result.confidence,
                            "reasoning": eval_result.reasoning
                        }
                        for eval_result in consensus_result.individual_results
                    ]
                })
            
            return SemanticTestResult(
                test_id=f"{self.flow_id}_{step.step_id}",
                description=f"Flow step: {step.step_id}",
                user_input=step.user_input,
                agent_response=agent_response,
                criteria=enhanced_criteria,
                passed=passed,
                overall_score=overall_score,
                criterion_results=criterion_results,
                execution_time=time.time() - start_time
            )
            
        except Exception as e:
            return SemanticTestResult(
                test_id=f"{self.flow_id}_{step.step_id}",
                description=f"Flow step: {step.step_id}",
                user_input=step.user_input or "",
                agent_response="",
                criteria=step.criteria,
                passed=False,
                overall_score=0.0,
                errors=[f"Step execution error: {str(e)}"],
                execution_time=time.time() - start_time
            )
    
    async def _execute_conditional_step(
        self,
        agent: AgentUnderTest,
        step: FlowStep,
        evaluator: EvaluationLoop
    ) -> Optional[SemanticTestResult]:
        """Execute a conditional step based on previous response"""
        if not self.conversation_history:
            return None
        
        last_response = self.conversation_history[-1]["agent_response"]
        condition_result = step.conditional_check(last_response)
        
        next_step = (
            step.metadata["true_step"] if condition_result 
            else step.metadata["false_step"]
        )
        
        if next_step:
            return await self._execute_step(agent, next_step, evaluator)
        return None
    
    async def _execute_parallel_steps(
        self,
        agent: AgentUnderTest,
        step: FlowStep,
        evaluator: EvaluationLoop
    ) -> Optional[SemanticTestResult]:
        """Execute parallel steps (for now, execute sequentially)"""
        # For simplicity, execute sequentially
        # In production, this could be enhanced for true parallel execution
        results = []
        for parallel_step in step.parallel_steps:
            result = await self._execute_step(agent, parallel_step, evaluator)
            if result:
                results.append(result)
        
        # Return combined result
        if results:
            return results[0]  # For now, return first result
        return None
    
    def _calculate_context_retention_score(self, step_results: List[SemanticTestResult]) -> float:
        """Calculate context retention score across the flow"""
        context_steps = [
            r for r in step_results 
            if any("context" in criterion.lower() for criterion in r.criteria)
        ]
        
        if not context_steps:
            return 1.0
            
        return sum(r.overall_score for r in context_steps) / len(context_steps)
    
    def _calculate_business_logic_score(self, step_results: List[SemanticTestResult]) -> float:
        """Calculate business logic compliance score"""
        business_steps = [
            r for r in step_results 
            if any("business" in criterion.lower() for criterion in r.criteria)
        ]
        
        if not business_steps:
            return 1.0
            
        return sum(r.overall_score for r in business_steps) / len(business_steps)
    
    def _calculate_tool_usage_score(self, step_results: List[SemanticTestResult]) -> float:
        """Calculate tool usage pattern score"""
        tool_steps = [
            r for r in step_results 
            if any("tool" in criterion.lower() or "usage" in criterion.lower() for criterion in r.criteria)
        ]
        
        if not tool_steps:
            return 1.0
            
        return sum(r.overall_score for r in tool_steps) / len(tool_steps)
    
    def _print_step_evaluation(self, step: FlowStep, agent_response: str, consensus_results, passed: bool, overall_score: float):
        """Print detailed evaluation results in a readable format"""
        print(f"\n{'='*80}")
        print(f"🔍 STEP EVALUATION: {step.step_id}")
        print(f"📄 Flow: {self.description}" + (f" | {self.flow_id}" if self.description != self.flow_id else ""))
        print(f"{'='*80}")
        print(f"📝 User Input: '{step.user_input}'")
        print(f"🤖 Agent Response: '{agent_response}'")
        print(f"{'─'*80}")
        
        for i, result in enumerate(consensus_results):
            status_icon = "✅" if result.passed else "❌"
            status_text = "PASS" if result.passed else "FAIL"
            print(f"\n📋 Criterion {i+1}: {status_icon} {status_text} (Score: {result.consensus_score:.2f})")
            print(f"   └── '{result.criterion}'")
            
            if result.individual_results:
                for eval_result in result.individual_results:
                    decision_icon = "✅" if eval_result.decision == "YES" else "❌" if eval_result.decision == "NO" else "⚠️"
                    print(f"   └── {decision_icon} {eval_result.evaluator_model}: {eval_result.decision}")
                    print(f"       💭 {eval_result.reasoning}")
        
        print(f"\n{'─'*80}")
        step_status_icon = "✅" if passed else "❌"
        step_status_text = "PASS" if passed else "FAIL" 
        print(f"🎯 Step Result: {step_status_icon} {step_status_text} (Overall Score: {overall_score:.2f})")
        print(f"{'='*80}\n")
    
    def _is_in_test_environment(self) -> bool:
        """Check if we're running in a pytest environment"""
        import sys
        return 'pytest' in sys.modules or 'PYTEST_CURRENT_TEST' in os.environ


def conversation_flow(
    flow_id: str,
    description: str = "",
    evaluator_models: Optional[List[str]] = None,
    consensus_threshold: float = 0.7,
    config_mode: str = "fast"
) -> ConversationFlow:
    """
    Factory function to create a conversation flow.
    
    Args:
        flow_id: Unique identifier for the flow
        description: Human-readable flow description
        evaluator_models: LLM models for evaluation
        consensus_threshold: Minimum consensus score to pass
        config_mode: Configuration mode ('fast', 'thorough', 'production')
        
    Returns:
        ConversationFlow instance
        
    Examples:
        # Fast testing (default)
        flow = conversation_flow("onboarding", "Test user onboarding process")
        
        # Thorough testing with debugging
        flow = conversation_flow("onboarding", config_mode="thorough")
        
        # Production testing
        flow = conversation_flow("onboarding", config_mode="production")
    """
    return ConversationFlow(
        flow_id=flow_id,
        description=description,
        evaluator_models=evaluator_models,
        consensus_threshold=consensus_threshold,
        config_mode=config_mode
    )