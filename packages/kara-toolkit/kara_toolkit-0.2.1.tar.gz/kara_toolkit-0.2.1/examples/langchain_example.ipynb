{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0fe088",
   "metadata": {},
   "source": [
    "# KARA LangChain Integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3020d",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f919063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain_core.documents.base import Document\n",
    "\n",
    "    from kara.integrations.langchain import KARATextSplitter\n",
    "except ImportError as e:\n",
    "    print(\"LangChain is not installed. This notebook requires LangChain.\")\n",
    "    print(\"Please install it with: pip install kara-toolkit[langchain]\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b79b6",
   "metadata": {},
   "source": [
    "## Document Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a206b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document length: 1315 characters\n"
     ]
    }
   ],
   "source": [
    "# Original document\n",
    "original_doc = (\n",
    "    \"LangChain is an open-source framework built in Python that helps developers create \"\n",
    "    \"applications powered by large language models (LLMs). It allows seamless integration \"\n",
    "    \"between LLMs and external data sources like APIs, files, and databases. With LangChain, \"\n",
    "    \"developers can build dynamic workflows where a language model not only generates text but \"\n",
    "    \"also interacts with tools and environments. This makes it ideal for creating advanced \"\n",
    "    \"chatbots, agents, and AI systems that go beyond static prompting. LangChain provides both \"\n",
    "    \"low-level components for custom logic and high-level abstractions for rapid prototyping, \"\n",
    "    \"making it a versatile toolkit for AI application development.\\n\\n\"\n",
    "    \"Python is the primary language used with LangChain due to its rich ecosystem and \"\n",
    "    \"simplicity. Python's popularity in AI and data science makes it a natural fit for \"\n",
    "    \"building with LangChain. Libraries like pydantic, asyncio, and openai integrate smoothly \"\n",
    "    \"with LangChain, enabling developers to quickly build robust, scalable applications. \"\n",
    "    \"Because LangChain supports modularity, developers can extend it using Python's vast \"\n",
    "    \"collection of libraries. Whether you're building an autonomous agent or a document QA \"\n",
    "    \"tool, Python and LangChain together offer a powerful combination that lowers the barrier \"\n",
    "    \"for building intelligent, interactive systems.\"\n",
    ")\n",
    "\n",
    "print(f\"Original document length: {len(original_doc)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a43c749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document length: 1300 characters\n"
     ]
    }
   ],
   "source": [
    "# Updated document (with additional content)\n",
    "updated_doc = (\n",
    "    \"LangChain is an open-source framework built in Python that helps developers create \"\n",
    "    \"applications powered by large language models (LLMs). It allows seamless integration \"\n",
    "    \"between LLMs and external data sources like APIs, files, and databases. With LangChain, \"\n",
    "    \"developers can build dynamic workflows where a language model not only generates text but \"\n",
    "    \"also interacts with tools and environments. Developers can define step-by-step workflows \"\n",
    "    \"in which an LLM can retrieve data, call APIs, and act based on context. This flexibility \"\n",
    "    \"allows LangChain to support everything from basic assistants to complex, multi-step \"\n",
    "    \"agents capable of reasoning and memory retention.\\n\\n\"\n",
    "    \"Python is the primary language used with LangChain due to its rich ecosystem and \"\n",
    "    \"simplicity. Python's popularity in AI and data science makes it a natural fit for \"\n",
    "    \"building with LangChain. Libraries like pydantic, asyncio, and openai integrate smoothly \"\n",
    "    \"with LangChain, enabling developers to quickly build robust, scalable applications. \"\n",
    "    \"Because LangChain supports modularity, developers can extend it using Python's vast \"\n",
    "    \"collection of libraries. Whether you're building an autonomous agent or a document QA \"\n",
    "    \"tool, Python and LangChain together offer a powerful combination that lowers the barrier \"\n",
    "    \"for building intelligent, interactive systems.\"\n",
    ")\n",
    "\n",
    "print(f\"Updated document length: {len(updated_doc)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2139ce",
   "metadata": {},
   "source": [
    "## Initialize KARA Text Splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KARA splitter with LangChain-compatible interface\n",
    "splitter = KARATextSplitter(\n",
    "    chunk_size=200,\n",
    "    imperfect_chunk_tolerance=10,\n",
    "    separators=[\". \", \"\\n\\n\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0575e",
   "metadata": {},
   "source": [
    "## Step 1: Process Original Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ebdc068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9 chunks:\n",
      "\n",
      "Chunk 1: `LangChain is an open-source framework built in Python that helps...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2: `It allows seamless integration between LLMs and external data sou...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3: `With LangChain, developers can build dynamic workflows where a la...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4: `This makes it ideal for creating advanced chatbots, agents, and A...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5: `LangChain provides both low-level components for custom logic and...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6: `Python is the primary language used with LangChain due to its ric...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 7: `Libraries like pydantic, asyncio, and openai integrate smoothly w...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 8: `Because LangChain supports modularity, developers can extend it u...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 9: `Whether you're building an autonomous agent or a document QA tool...`\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "original_chunks = splitter.split_text(original_doc)\n",
    "\n",
    "print(f\"Created {len(original_chunks)} chunks:\")\n",
    "print()\n",
    "for i, chunk in enumerate(original_chunks, 1):\n",
    "    print(f\"Chunk {i}: `{chunk[:65].strip()}...`\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf7163",
   "metadata": {},
   "source": [
    "## Step 2: Process Updated Document\n",
    "\n",
    "Now let's process the updated document and see how KARA reuses existing chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac54cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 9 chunks\n",
      "\n",
      "Chunk 1 [REUSED]: `LangChain is an open-source framework built in Python t...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 [REUSED]: `It allows seamless integration between LLMs and externa...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 [REUSED]: `With LangChain, developers can build dynamic workflows...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4 [NEW]: `Developers can define step-by-step workflows in which a...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5 [NEW]: `This flexibility allows LangChain to support everything...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6 [REUSED]: `Python is the primary language used with LangChain due...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 7 [REUSED]: `Libraries like pydantic, asyncio, and openai integrate...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 8 [REUSED]: `Because LangChain supports modularity, developers can e...`\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 9 [REUSED]: `Whether you're building an autonomous agent or a docume...`\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "updated_chunks = splitter.split_text(updated_doc)\n",
    "\n",
    "print(f\"Result: {len(updated_chunks)} chunks\")\n",
    "print()\n",
    "for i, chunk in enumerate(updated_chunks, 1):\n",
    "    # Check if this chunk existed before\n",
    "    is_reused = chunk in original_chunks\n",
    "    status = \"REUSED\" if is_reused else \"NEW\"\n",
    "    print(f\"Chunk {i} [{status}]: `{chunk[:55].strip()}...`\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982b7eb",
   "metadata": {},
   "source": [
    "## Step 3: Efficiency Analysis\n",
    "\n",
    "Let's calculate and visualize the efficiency gains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7363d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KARA Efficiency Analysis\n",
      "========================================\n",
      "Total chunks in updated document: 9\n",
      "Chunks reused from original: 7\n",
      "\n",
      "Overall efficiency: 7/9 = 77.8%\n"
     ]
    }
   ],
   "source": [
    "reused_count = sum(1 for chunk in updated_chunks if chunk in original_chunks)\n",
    "total_chunks = len(updated_chunks)\n",
    "efficiency_pct = reused_count / total_chunks\n",
    "new_chunks = total_chunks - reused_count\n",
    "\n",
    "print(\"KARA Efficiency Analysis\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total chunks in updated document: {total_chunks}\")\n",
    "print(f\"Chunks reused from original: {reused_count}\")\n",
    "print()\n",
    "print(f\"Overall efficiency: {reused_count}/{total_chunks} = {efficiency_pct:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcd04d",
   "metadata": {},
   "source": [
    "## Step 4: LangChain Document Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a57600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document metadata: {'source': 'langchain_guide.txt', 'version': '2.0', 'author': 'Documentation Team', 'last_updated': '2024-01-15'}\n",
      "Content length: 1300 chars\n"
     ]
    }
   ],
   "source": [
    "# Create a LangChain Document with metadata\n",
    "doc_with_metadata = Document(\n",
    "    page_content=updated_doc,\n",
    "    metadata={\n",
    "        \"source\": \"langchain_guide.txt\",\n",
    "        \"version\": \"2.0\",\n",
    "        \"author\": \"Documentation Team\",\n",
    "        \"last_updated\": \"2024-01-15\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Document metadata: {doc_with_metadata.metadata}\")\n",
    "print(f\"Content length: {len(doc_with_metadata.page_content)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "feebc6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Chunk 1:\n",
      "Content: 'LangChain is an open-source framework built in Python that helps developers create applications powered by lar...'\n",
      "Metadata: {'source': 'langchain_guide.txt', 'version': '2.0', 'author': 'Documentation Team', 'last_updated': '2024-01-15'}\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Document Chunk 2:\n",
      "Content: 'It allows seamless integration between LLMs and external data sources like APIs, files, and databases....'\n",
      "Metadata: {'source': 'langchain_guide.txt', 'version': '2.0', 'author': 'Documentation Team', 'last_updated': '2024-01-15'}\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Document Chunk 3:\n",
      "Content: 'With LangChain, developers can build dynamic workflows where a language model not only generates text but also...'\n",
      "Metadata: {'source': 'langchain_guide.txt', 'version': '2.0', 'author': 'Documentation Team', 'last_updated': '2024-01-15'}\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Split the document using KARA\n",
    "chunked_docs = splitter.split_documents([doc_with_metadata])\n",
    "\n",
    "for i, doc in enumerate(chunked_docs[:3], 1):\n",
    "    print(f\"Document Chunk {i}:\")\n",
    "    print(f\"Content: '{doc.page_content.strip()[:110]}...'\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 125)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ffe610",
   "metadata": {},
   "source": [
    "## Step 5: Multi-Document Processing\n",
    "\n",
    "Let's demonstrate KARA's capabilities with multiple documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a26bfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Doc 1: framework (high priority, 1315 chars)\n",
      "   Doc 2: python (medium priority, 153 chars)\n",
      "   Doc 3: vectors (high priority, 145 chars)\n"
     ]
    }
   ],
   "source": [
    "# Create multiple documents with different topics\n",
    "docs_multi = [\n",
    "    Document(\n",
    "        page_content=original_doc,\n",
    "        metadata={\"source\": \"doc1.txt\", \"topic\": \"framework\", \"priority\": \"high\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=(\n",
    "            \"Python's rich ecosystem makes it ideal for AI development. Libraries like numpy, \"\n",
    "            \"pandas, and scikit-learn integrate seamlessly with LangChain components.\"\n",
    "        ),\n",
    "        metadata={\"source\": \"doc2.txt\", \"topic\": \"python\", \"priority\": \"medium\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=(\n",
    "            \"Vector databases enable semantic search in RAG applications. They store \"\n",
    "            \"embeddings and allow for efficient similarity-based retrieval of context.\"\n",
    "        ),\n",
    "        metadata={\"source\": \"doc3.txt\", \"topic\": \"vectors\", \"priority\": \"high\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, doc in enumerate(docs_multi, 1):\n",
    "    topic = doc.metadata.get(\"topic\")\n",
    "    priority = doc.metadata.get(\"priority\")\n",
    "    length = len(doc.page_content)\n",
    "    print(f\"   Doc {i}: {topic} ({priority} priority, {length} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5885c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 3 documents into 11 chunks\n",
      "\n",
      "Chunks with their sources:\n",
      "\n",
      "Chunk 1 [doc1.txt/framework/high]:\n",
      "'LangChain is an open-source framework built in Python that h...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 2 [doc1.txt/framework/high]:\n",
      "'It allows seamless integration between LLMs and external dat...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 3 [doc1.txt/framework/high]:\n",
      "'With LangChain, developers can build dynamic workflows where...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 4 [doc1.txt/framework/high]:\n",
      "'This makes it ideal for creating advanced chatbots, agents, ...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 5 [doc1.txt/framework/high]:\n",
      "'LangChain provides both low-level components for custom logi...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 6 [doc1.txt/framework/high]:\n",
      "'Python is the primary language used with LangChain due to it...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 7 [doc1.txt/framework/high]:\n",
      "'Libraries like pydantic, asyncio, and openai integrate smoot...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 8 [doc1.txt/framework/high]:\n",
      "'Because LangChain supports modularity, developers can extend...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 9 [doc1.txt/framework/high]:\n",
      "'Whether you're building an autonomous agent or a document QA...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 10 [doc2.txt/python/medium]:\n",
      "'Python's rich ecosystem makes it ideal for AI development. L...'\n",
      "----------------------------------------------------------------------\n",
      "Chunk 11 [doc3.txt/vectors/high]:\n",
      "'Vector databases enable semantic search in RAG applications....'\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process multiple documents\n",
    "multi_chunked_docs = splitter.split_documents(docs_multi)\n",
    "\n",
    "print(f\"Split {len(docs_multi)} documents into {len(multi_chunked_docs)} chunks\")\n",
    "print(\"\\nChunks with their sources:\")\n",
    "print()\n",
    "\n",
    "for i, doc in enumerate(multi_chunked_docs, 1):\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    topic = doc.metadata.get(\"topic\", \"unknown\")\n",
    "    priority = doc.metadata.get(\"priority\", \"unknown\")\n",
    "    content_preview = doc.page_content.strip()[:60]\n",
    "\n",
    "    print(f\"Chunk {i} [{source}/{topic}/{priority}]:\")\n",
    "    print(f\"'{content_preview}...'\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aeac89",
   "metadata": {},
   "source": [
    "## Step 6: Document Updates with Multi-Document Efficiency\n",
    "\n",
    "Now let's update our document collection and see how KARA handles the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdddcafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating with modified and new documents:\n",
      "   - Document 1: Modified (content added)\n",
      "   - Document 2: Unchanged\n",
      "   - Document 3: Removed\n",
      "   - Document 4: New (RAG explanation)\n"
     ]
    }
   ],
   "source": [
    "# Update documents: modify one, keep others, add new one\n",
    "updated_docs_multi = [\n",
    "    Document(\n",
    "        page_content=updated_doc,  # Modified content\n",
    "        metadata={\"source\": \"doc1.txt\", \"topic\": \"framework\", \"priority\": \"high\", \"version\": \"2.0\"},\n",
    "    ),\n",
    "    docs_multi[1],  # Unchanged Python doc\n",
    "    Document(\n",
    "        page_content=(\n",
    "            \"Retrieval-Augmented Generation (RAG) combines the power of large language \"\n",
    "            \"models with external knowledge retrieval to provide more accurate and \"\n",
    "            \"contextual responses.\"\n",
    "        ),\n",
    "        metadata={\"source\": \"doc4.txt\", \"topic\": \"rag\", \"priority\": \"high\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Updating with modified and new documents:\")\n",
    "print(\"   - Document 1: Modified (content added)\")\n",
    "print(\"   - Document 2: Unchanged\")\n",
    "print(\"   - Document 3: Removed\")\n",
    "print(\"   - Document 4: New (RAG explanation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9804d67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Document Update Results:\n",
      "=============================================\n",
      "Original chunks: 11\n",
      "Updated chunks: 11\n",
      "Chunks reused: 8\n",
      "New chunks: 3\n",
      "\n",
      "Multi-doc efficiency: 8/11 = 72.7%\n"
     ]
    }
   ],
   "source": [
    "# Process updated documents and calculate efficiency\n",
    "original_texts = [doc.page_content for doc in multi_chunked_docs]\n",
    "new_chunked_docs = splitter.split_documents(updated_docs_multi)\n",
    "new_texts = [doc.page_content for doc in new_chunked_docs]\n",
    "\n",
    "# Calculate reuse statistics\n",
    "reused_chunks = sum(1 for text in new_texts if text in original_texts)\n",
    "total_new_chunks = len(new_texts)\n",
    "multi_efficiency = (reused_chunks / total_new_chunks) * 100\n",
    "\n",
    "print(\"Multi-Document Update Results:\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Original chunks: {len(original_texts)}\")\n",
    "print(f\"Updated chunks: {total_new_chunks}\")\n",
    "print(f\"Chunks reused: {reused_chunks}\")\n",
    "print(f\"New chunks: {total_new_chunks - reused_chunks}\")\n",
    "print(\"\")\n",
    "print(f\"Multi-doc efficiency: {reused_chunks}/{total_new_chunks} = {multi_efficiency:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8524242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed breakdown:\n",
      "\n",
      "doc1.txt:\n",
      "   Chunks: 9\n",
      "   Reused: 7 (77.8%)\n",
      "\n",
      "doc2.txt:\n",
      "   Chunks: 1\n",
      "   Reused: 1 (100.0%)\n",
      "\n",
      "doc4.txt:\n",
      "   Chunks: 1\n",
      "   Reused: 0 (0.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show detailed breakdown by document\n",
    "print(\"Detailed breakdown:\")\n",
    "print()\n",
    "\n",
    "# Group chunks by source\n",
    "chunks_by_source = {}\n",
    "for doc in new_chunked_docs:\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    if source not in chunks_by_source:\n",
    "        chunks_by_source[source] = []\n",
    "    chunks_by_source[source].append(doc.page_content)\n",
    "\n",
    "for source, chunks in chunks_by_source.items():\n",
    "    reused_in_source = sum(1 for chunk in chunks if chunk in original_texts)\n",
    "    total_in_source = len(chunks)\n",
    "    source_efficiency = (reused_in_source / total_in_source) * 100 if total_in_source > 0 else 0\n",
    "\n",
    "    print(f\"{source}:\")\n",
    "    print(f\"   Chunks: {total_in_source}\")\n",
    "    print(f\"   Reused: {reused_in_source} ({source_efficiency:.1f}%)\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
