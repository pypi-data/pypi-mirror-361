import os
from unittest.mock import AsyncMock, patch

import pytest

from cogent.base.models.chunk import Chunk
from cogent.base.providers.embedding.litellm_embedding import LiteLLMEmbeddingModel


class TestIntegrationLiteLLMEmbedding:
    """Integration tests for LiteLLMEmbeddingModel with external services."""

    @pytest.fixture
    def mock_config(self):
        """Mock configuration with test models."""
        return {
            "llm": {
                "registered_models": {
                    "test_ollama_embedding": {"model_name": "nomic-embed-text", "api_base": "http://localhost:11434"},
                    "test_openai_embedding": {"model_name": "text-embedding-3-small"},
                    "test_openai_embedding_large": {"model_name": "text-embedding-3-large"},
                }
            },
            "embedding": {"embedding_dimensions": 768},
        }

    @pytest.fixture
    def test_chunks(self):
        """Test chunks for embedding."""
        return [
            Chunk(content="This is a test object about artificial intelligence.", metadata={"source": "test"}),
            Chunk(content="Machine learning is a subset of AI.", metadata={"source": "test"}),
            Chunk(content="Deep learning uses neural networks.", metadata={"source": "test"}),
        ]

    @pytest.fixture
    def test_texts(self):
        """Test texts for embedding."""
        return [
            "This is a test object about artificial intelligence.",
            "Machine learning is a subset of AI.",
            "Deep learning uses neural networks.",
        ]

    @pytest.fixture
    def single_text(self):
        """Single test text for query embedding."""
        return "What is artificial intelligence?"

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_ollama_object_embedding(self, mock_config, test_texts):
        """Test object embedding with Ollama model."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            # Skip if Ollama is not available
            try:
                pass
            except ImportError:
                pytest.skip("Ollama library not available")

            # Check if Ollama service is running
            try:
                import httpx

                async with httpx.AsyncClient() as client:
                    response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
                    if response.status_code != 200:
                        pytest.skip("Ollama service not running")
            except Exception:
                pytest.skip("Ollama service not accessible")

            model = LiteLLMEmbeddingModel("test_ollama_embedding")
            try:
                embeddings = await model.embed_objects(test_texts)

                assert isinstance(embeddings, list)
                assert len(embeddings) == len(test_texts)
                assert all(isinstance(emb, list) for emb in embeddings)
                assert all(isinstance(val, float) for emb in embeddings for val in emb)
                assert all(len(emb) > 0 for emb in embeddings)

                # Check that embeddings are not all zeros
                assert not all(all(val == 0.0 for val in emb) for emb in embeddings)
            except Exception as e:
                if "object dict can't be used in 'await' expression" in str(e):
                    pytest.skip(f"LiteLLM/Ollama compatibility issue: {e}")
                else:
                    raise

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_openai_object_embedding(self, mock_config, test_texts):
        """Test object embedding with OpenAI model."""
        # Skip if OpenAI API key is not available
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OpenAI API key not available")

        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embeddings = await model.embed_objects(test_texts)

            assert isinstance(embeddings, list)
            assert len(embeddings) == len(test_texts)
            assert all(isinstance(emb, list) for emb in embeddings)
            assert all(isinstance(val, float) for emb in embeddings for val in emb)
            assert all(len(emb) > 0 for emb in embeddings)

            # Check that embeddings are not all zeros
            assert not all(all(val == 0.0 for val in emb) for emb in embeddings)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_openai_large_embedding_dimensions(self, mock_config, test_texts):
        """Test OpenAI large model with custom dimensions."""
        # Skip if OpenAI API key is not available
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OpenAI API key not available")

        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = 1536  # Large model dimensions

            model = LiteLLMEmbeddingModel("test_openai_embedding_large")
            embeddings = await model.embed_objects(test_texts)

            assert isinstance(embeddings, list)
            assert len(embeddings) == len(test_texts)
            assert all(isinstance(emb, list) for emb in embeddings)
            assert all(len(emb) > 0 for emb in embeddings)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_ollama_query_embedding(self, mock_config, single_text):
        """Test query embedding with Ollama model."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            # Skip if Ollama is not available
            try:
                pass
            except ImportError:
                pytest.skip("Ollama library not available")

            # Check if Ollama service is running
            try:
                import httpx

                async with httpx.AsyncClient() as client:
                    response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
                    if response.status_code != 200:
                        pytest.skip("Ollama service not running")
            except Exception:
                pytest.skip("Ollama service not accessible")

            model = LiteLLMEmbeddingModel("test_ollama_embedding")
            try:
                embedding = await model.embed_query(single_text)

                assert isinstance(embedding, list)
                assert all(isinstance(val, float) for val in embedding)
                assert len(embedding) > 0
                assert not all(val == 0.0 for val in embedding)
            except Exception as e:
                if "object dict can't be used in 'await' expression" in str(e):
                    pytest.skip(f"LiteLLM/Ollama compatibility issue: {e}")
                else:
                    raise

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_openai_query_embedding(self, mock_config, single_text):
        """Test query embedding with OpenAI model."""
        # Skip if OpenAI API key is not available
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OpenAI API key not available")

        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embedding = await model.embed_query(single_text)

            assert isinstance(embedding, list)
            assert all(isinstance(val, float) for val in embedding)
            assert len(embedding) > 0
            assert not all(val == 0.0 for val in embedding)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_ollama_chunk_embedding_for_ingestion(self, mock_config, test_chunks):
        """Test chunk embedding for ingestion with Ollama model."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 100

            # Skip if Ollama is not available
            try:
                pass
            except ImportError:
                pytest.skip("Ollama library not available")

            # Check if Ollama service is running
            try:
                import httpx

                async with httpx.AsyncClient() as client:
                    response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
                    if response.status_code != 200:
                        pytest.skip("Ollama service not running")
            except Exception:
                pytest.skip("Ollama service not accessible")

            model = LiteLLMEmbeddingModel("test_ollama_embedding")
            try:
                embeddings = await model.embed_for_ingestion(test_chunks)

                assert isinstance(embeddings, list)
                assert len(embeddings) == len(test_chunks)
                assert all(isinstance(emb, list) for emb in embeddings)
                assert all(isinstance(val, float) for emb in embeddings for val in emb)
                assert all(len(emb) > 0 for emb in embeddings)
            except Exception as e:
                if "object dict can't be used in 'await' expression" in str(e):
                    pytest.skip(f"LiteLLM/Ollama compatibility issue: {e}")
                else:
                    raise

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_openai_chunk_embedding_for_ingestion(self, mock_config, test_chunks):
        """Test chunk embedding for ingestion with OpenAI model."""
        # Skip if OpenAI API key is not available
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OpenAI API key not available")

        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 100

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embeddings = await model.embed_for_ingestion(test_chunks)

            assert isinstance(embeddings, list)
            assert len(embeddings) == len(test_chunks)
            assert all(isinstance(emb, list) for emb in embeddings)
            assert all(isinstance(val, float) for emb in embeddings for val in emb)
            assert all(len(emb) > 0 for emb in embeddings)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_ollama_single_chunk_embedding(self, mock_config):
        """Test single chunk embedding with Ollama model."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 100

            # Skip if Ollama is not available
            try:
                pass
            except ImportError:
                pytest.skip("Ollama library not available")

            # Check if Ollama service is running
            try:
                import httpx

                async with httpx.AsyncClient() as client:
                    response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
                    if response.status_code != 200:
                        pytest.skip("Ollama service not running")
            except Exception:
                pytest.skip("Ollama service not accessible")

            model = LiteLLMEmbeddingModel("test_ollama_embedding")
            single_chunk = Chunk(content="This is a single test chunk.", metadata={"source": "test"})
            try:
                embeddings = await model.embed_for_ingestion(single_chunk)

                assert isinstance(embeddings, list)
                assert len(embeddings) == 1
                assert isinstance(embeddings[0], list)
                assert all(isinstance(val, float) for val in embeddings[0])
                assert len(embeddings[0]) > 0
            except Exception as e:
                if "object dict can't be used in 'await' expression" in str(e):
                    pytest.skip(f"LiteLLM/Ollama compatibility issue: {e}")
                else:
                    raise

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_openai_single_chunk_embedding(self, mock_config):
        """Test single chunk embedding with OpenAI model."""
        # Skip if OpenAI API key is not available
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OpenAI API key not available")

        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 100

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            single_chunk = Chunk(content="This is a single test chunk.", metadata={"source": "test"})
            embeddings = await model.embed_for_ingestion(single_chunk)

            assert isinstance(embeddings, list)
            assert len(embeddings) == 1
            assert isinstance(embeddings[0], list)
            assert all(isinstance(val, float) for val in embeddings[0])
            assert len(embeddings[0]) > 0

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_ollama_embed_for_query(self, mock_config, single_text):
        """Test embed_for_query with Ollama model."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            # Skip if Ollama is not available
            try:
                pass
            except ImportError:
                pytest.skip("Ollama library not available")

            # Check if Ollama service is running
            try:
                import httpx

                async with httpx.AsyncClient() as client:
                    response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
                    if response.status_code != 200:
                        pytest.skip("Ollama service not running")
            except Exception:
                pytest.skip("Ollama service not accessible")

            model = LiteLLMEmbeddingModel("test_ollama_embedding")
            try:
                embedding = await model.embed_for_query(single_text)

                assert isinstance(embedding, list)
                assert all(isinstance(val, float) for val in embedding)
                assert len(embedding) > 0
                assert not all(val == 0.0 for val in embedding)
            except Exception as e:
                if "object dict can't be used in 'await' expression" in str(e):
                    pytest.skip(f"LiteLLM/Ollama compatibility issue: {e}")
                else:
                    raise

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_openai_embed_for_query(self, mock_config, single_text):
        """Test embed_for_query with OpenAI model."""
        # Skip if OpenAI API key is not available
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OpenAI API key not available")

        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embedding = await model.embed_for_query(single_text)

            assert isinstance(embedding, list)
            assert all(isinstance(val, float) for val in embedding)
            assert len(embedding) > 0
            assert not all(val == 0.0 for val in embedding)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_ollama_batch_processing(self, mock_config):
        """Test batch processing with Ollama model."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 2  # Small batch size for testing

            # Skip if Ollama is not available
            try:
                pass
            except ImportError:
                pytest.skip("Ollama library not available")

            # Check if Ollama service is running
            try:
                import httpx

                async with httpx.AsyncClient() as client:
                    response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
                    if response.status_code != 200:
                        pytest.skip("Ollama service not running")
            except Exception:
                pytest.skip("Ollama service not accessible")

            model = LiteLLMEmbeddingModel("test_ollama_embedding")
            large_chunk_list = [
                Chunk(content=f"This is test chunk number {i}.", metadata={"source": "test", "index": i})
                for i in range(5)  # More chunks than batch size
            ]
            try:
                embeddings = await model.embed_for_ingestion(large_chunk_list)

                assert isinstance(embeddings, list)
                assert len(embeddings) == len(large_chunk_list)
                assert all(isinstance(emb, list) for emb in embeddings)
                assert all(len(emb) > 0 for emb in embeddings)
            except Exception as e:
                if "object dict can't be used in 'await' expression" in str(e):
                    pytest.skip(f"LiteLLM/Ollama compatibility issue: {e}")
                else:
                    raise

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_openai_batch_processing(self, mock_config):
        """Test batch processing with OpenAI model."""
        # Skip if OpenAI API key is not available
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OpenAI API key not available")

        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 2  # Small batch size for testing

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            large_chunk_list = [
                Chunk(content=f"This is test chunk number {i}.", metadata={"source": "test", "index": i})
                for i in range(5)  # More chunks than batch size
            ]
            embeddings = await model.embed_for_ingestion(large_chunk_list)

            assert isinstance(embeddings, list)
            assert len(embeddings) == len(large_chunk_list)
            assert all(isinstance(emb, list) for emb in embeddings)
            assert all(len(emb) > 0 for emb in embeddings)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_error_handling_invalid_model(self):
        """Test error handling for invalid model."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = {}

            with pytest.raises(ValueError, match="Model 'invalid_model' not found"):
                LiteLLMEmbeddingModel("invalid_model")

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_error_handling_empty_texts(self, mock_config):
        """Test handling of empty text list."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embeddings = await model.embed_objects([])

            assert isinstance(embeddings, list)
            assert len(embeddings) == 0

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_error_handling_empty_chunks(self, mock_config):
        """Test handling of empty chunk list."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 100

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embeddings = await model.embed_for_ingestion([])

            assert isinstance(embeddings, list)
            assert len(embeddings) == 0


class TestLiteLLMEmbeddingUnit:
    """Unit tests for LiteLLMEmbeddingModel with mocked dependencies."""

    @pytest.fixture
    def mock_config(self):
        """Mock configuration with test models."""
        return {
            "llm": {
                "registered_models": {
                    "test_ollama_embedding": {"model_name": "nomic-embed-text", "api_base": "http://localhost:11434"},
                    "test_openai_embedding": {"model_name": "text-embedding-3-small"},
                    "test_openai_embedding_large": {"model_name": "text-embedding-3-large"},
                }
            },
            "embedding": {"embedding_dimensions": 768},
        }

    @pytest.fixture
    def mock_litellm_response(self):
        """Mock LiteLLM embedding response generator for batch tests."""

        def _make_response(num_embeddings=3, dim=765):
            class MockResponse:
                def __init__(self, num_embeddings, dim):
                    self.data = [{"embedding": [float(i % 10) for i in range(dim)]} for _ in range(num_embeddings)]

            return MockResponse(num_embeddings, dim)

        return _make_response

    @pytest.fixture
    def mock_litellm_response_object(self):
        """Mock LiteLLM embedding response as object with data attribute."""

        class MockResponse:
            def __init__(self):
                self.data = [
                    {"embedding": [0.1, 0.2, 0.3, 0.4, 0.5] * 153},  # 765 dimensions
                    {"embedding": [0.2, 0.3, 0.4, 0.5, 0.6] * 153},  # 765 dimensions
                ]

        return MockResponse()

    @pytest.fixture
    def mock_litellm_response_direct(self):
        """Mock LiteLLM embedding response as direct list."""
        return [
            [0.1, 0.2, 0.3, 0.4, 0.5] * 153,  # 765 dimensions
            [0.2, 0.3, 0.4, 0.5, 0.6] * 153,  # 765 dimensions
        ]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_model_initialization(self, mock_config):
        """Test model initialization with valid config."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            model = LiteLLMEmbeddingModel("test_openai_embedding")

            assert model.model_key == "test_openai_embedding"
            assert model.model_config == mock_config["llm"]["registered_models"]["test_openai_embedding"]
            assert model.dimensions == 768

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_model_initialization_invalid_model(self, mock_config):
        """Test model initialization with invalid model key."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]

            with pytest.raises(ValueError, match="Model 'invalid_model' not found"):
                LiteLLMEmbeddingModel("invalid_model")

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_litellm_embedding(self, mock_config, mock_litellm_response):
        """Test embedding with mocked LiteLLM."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            texts = ["Test text 1", "Test text 2", "Test text 3"]
            mock_litellm.return_value = mock_litellm_response(num_embeddings=len(texts), dim=765)

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embeddings = await model.embed_objects(texts)

            assert len(embeddings) == 3
            assert all(len(emb) == 765 for emb in embeddings)
            assert all(isinstance(val, float) for emb in embeddings for val in emb)

            # Verify LiteLLM was called correctly
            mock_litellm.assert_called_once()
            call_args = mock_litellm.call_args
            assert call_args[1]["input"] == texts
            assert call_args[1]["model"] == "text-embedding-3-small"

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_ollama_embedding(self, mock_config, mock_litellm_response):
        """Test Ollama embedding with mocked Ollama client."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch("cogent.base.providers.embedding.litellm_embedding.ollama.AsyncClient") as mock_ollama_client,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            texts = ["Test text 1", "Test text 2"]

            # Mock Ollama client and its embeddings method
            mock_client_instance = AsyncMock()
            mock_ollama_client.return_value = mock_client_instance

            # Mock the embeddings response
            mock_embeddings = [[0.1, 0.2, 0.3] * 256] * len(texts)  # 768 dimensions
            mock_client_instance.embeddings.side_effect = [{"embedding": embedding} for embedding in mock_embeddings]

            model = LiteLLMEmbeddingModel("test_ollama_embedding")
            embeddings = await model.embed_objects(texts)

            assert len(embeddings) == 2
            assert all(len(emb) == 768 for emb in embeddings)

            # Verify Ollama client was called correctly
            mock_ollama_client.assert_called_once_with(host="http://localhost:11434")
            assert mock_client_instance.embeddings.call_count == 2
            mock_client_instance.embeddings.assert_any_call(model="nomic-embed-text", prompt="Test text 1")
            mock_client_instance.embeddings.assert_any_call(model="nomic-embed-text", prompt="Test text 2")

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_openai_large_embedding(self, mock_config, mock_litellm_response):
        """Test OpenAI large model with dimensions parameter."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = 1536
            texts = ["Test text"]
            mock_litellm.return_value = mock_litellm_response(num_embeddings=len(texts), dim=765)

            model = LiteLLMEmbeddingModel("test_openai_embedding_large")
            await model.embed_objects(texts)

            # Verify dimensions parameter was passed
            mock_litellm.assert_called_once()
            call_args = mock_litellm.call_args
            assert call_args[1]["model"] == "text-embedding-3-large"
            assert call_args[1]["dimensions"] == 2000  # PGVECTOR_MAX_DIMENSIONS

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_query_embedding(self, mock_config, mock_litellm_response):
        """Test query embedding with mocked LiteLLM."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            query_text = "What is artificial intelligence?"
            mock_litellm.return_value = mock_litellm_response(num_embeddings=1, dim=765)

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embedding = await model.embed_query(query_text)

            assert isinstance(embedding, list)
            assert len(embedding) == 765
            assert all(isinstance(val, float) for val in embedding)

            # Verify LiteLLM was called with single text
            mock_litellm.assert_called_once()
            call_args = mock_litellm.call_args
            assert call_args[1]["input"] == [query_text]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_chunk_embedding_for_ingestion(self, mock_config, mock_litellm_response):
        """Test chunk embedding for ingestion with mocked LiteLLM."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 100
            chunks = [
                Chunk(content="Test chunk 1", metadata={"source": "test"}),
                Chunk(content="Test chunk 2", metadata={"source": "test"}),
                Chunk(content="Test chunk 3", metadata={"source": "test"}),
            ]
            mock_litellm.return_value = mock_litellm_response(num_embeddings=len(chunks), dim=765)

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embeddings = await model.embed_for_ingestion(chunks)

            assert len(embeddings) == 3
            assert all(len(emb) == 765 for emb in embeddings)

            # Verify LiteLLM was called with chunk contents
            mock_litellm.assert_called_once()
            call_args = mock_litellm.call_args
            assert call_args[1]["input"] == ["Test chunk 1", "Test chunk 2", "Test chunk 3"]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_single_chunk_embedding(self, mock_config, mock_litellm_response):
        """Test single chunk embedding with mocked LiteLLM."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 100
            single_chunk = Chunk(content="Single test chunk", metadata={"source": "test"})
            mock_litellm.return_value = mock_litellm_response(num_embeddings=1, dim=765)

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embeddings = await model.embed_for_ingestion(single_chunk)

            assert len(embeddings) == 1
            assert len(embeddings[0]) == 765

            # Verify LiteLLM was called with single chunk content
            mock_litellm.assert_called_once()
            call_args = mock_litellm.call_args
            assert call_args[1]["input"] == ["Single test chunk"]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_batch_processing(self, mock_config, mock_litellm_response):
        """Test batch processing with mocked LiteLLM."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 2  # Small batch size

            # Patch the mock to return the correct number of embeddings for each batch
            def side_effect(input, **kwargs):
                return mock_litellm_response(num_embeddings=len(input), dim=765)

            mock_litellm.side_effect = side_effect

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            chunks = [
                Chunk(content=f"Test chunk {i}", metadata={"source": "test", "index": i})
                for i in range(5)  # More chunks than batch size
            ]
            embeddings = await model.embed_for_ingestion(chunks)

            assert len(embeddings) == 5
            assert all(len(emb) == 765 for emb in embeddings)

            # Verify LiteLLM was called multiple times for batching
            assert mock_litellm.call_count == 3  # 5 chunks / 2 batch size = 3 calls

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_embed_for_query(self, mock_config, mock_litellm_response):
        """Test embed_for_query with mocked LiteLLM."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            query_text = "What is machine learning?"
            mock_litellm.return_value = mock_litellm_response(num_embeddings=1, dim=765)

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embedding = await model.embed_for_query(query_text)

            assert isinstance(embedding, list)
            assert len(embedding) == 765
            assert all(isinstance(val, float) for val in embedding)

            # Verify embed_for_query calls embed_query
            mock_litellm.assert_called_once()
            call_args = mock_litellm.call_args
            assert call_args[1]["input"] == [query_text]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_error_handling(self, mock_config):
        """Test error handling with mocked LiteLLM."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch("cogent.base.providers.embedding.litellm_embedding.litellm.aembedding") as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_litellm.side_effect = Exception("API Error")

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            texts = ["Test text"]

            with pytest.raises(Exception, match="API Error"):
                await model.embed_objects(texts)

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_query_error_fallback(self, mock_config):
        """Test query embedding error handling."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_litellm.side_effect = Exception("API Error")

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            query_text = "Test query"

            # Should raise exception on error (no fallback implemented)
            with pytest.raises(Exception, match="API Error"):
                await model.embed_query(query_text)

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_empty_texts(self, mock_config):
        """Test handling of empty text list."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embeddings = await model.embed_objects([])

            assert isinstance(embeddings, list)
            assert len(embeddings) == 0

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_empty_chunks(self, mock_config):
        """Test handling of empty chunk list."""
        with patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config:
            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]
            mock_get_config.return_value.llm.embedding_batch_size = 100

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            embeddings = await model.embed_for_ingestion([])

            assert isinstance(embeddings, list)
            assert len(embeddings) == 0

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_dimension_validation_warning(self, mock_config, mock_litellm_response):
        """Test dimension validation warning."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
            patch("cogent.base.providers.embedding.litellm_embedding.logger") as mock_logger,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = 1000  # Different from response
            texts = ["Test text"]
            mock_litellm.return_value = mock_litellm_response(num_embeddings=len(texts), dim=765)

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            await model.embed_objects(texts)

            # Should log warning about dimension mismatch
            mock_logger.warning.assert_called_once()
            warning_msg = mock_logger.warning.call_args[0][0]
            assert "dimension mismatch" in warning_msg
            assert "got 765, expected 1000" in warning_msg

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_mocked_response_formats(self, mock_config):
        """Test handling of different LiteLLM response formats."""
        with (
            patch("cogent.base.providers.embedding.litellm_embedding.get_cogent_config") as mock_get_config,
            patch(
                "cogent.base.providers.embedding.litellm_embedding.litellm.aembedding", new_callable=AsyncMock
            ) as mock_litellm,
        ):

            mock_get_config.return_value.llm.registered_models = mock_config["llm"]["registered_models"]
            mock_get_config.return_value.llm.embedding_dimensions = mock_config["embedding"]["embedding_dimensions"]

            model = LiteLLMEmbeddingModel("test_openai_embedding")
            texts = ["Test text 1", "Test text 2"]

            # Test object response format (what production code expects)
            class MockResponse:
                def __init__(self):
                    self.data = [
                        {"embedding": [0.1, 0.2, 0.3] * 256},  # 768 dimensions
                        {"embedding": [0.2, 0.3, 0.4] * 256},  # 768 dimensions
                    ]

            mock_litellm.return_value = MockResponse()
            embeddings = await model.embed_objects(texts)
            assert len(embeddings) == 2
            assert all(len(emb) == 768 for emb in embeddings)
