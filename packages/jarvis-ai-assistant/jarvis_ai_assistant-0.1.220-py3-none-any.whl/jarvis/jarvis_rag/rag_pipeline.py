import os
from typing import List, Literal, Optional, cast

from langchain.docstore.document import Document

from .embedding_manager import EmbeddingManager
from .llm_interface import JarvisPlatform_LLM, LLMInterface, ToolAgent_LLM
from .query_rewriter import QueryRewriter
from .reranker import Reranker
from .retriever import ChromaRetriever
from jarvis.jarvis_utils.config import (
    get_rag_embedding_mode,
    get_rag_vector_db_path,
    get_rag_embedding_cache_path,
    get_rag_embedding_models,
)


class JarvisRAGPipeline:
    """
    The main orchestrator for the RAG pipeline.

    This class integrates the embedding manager, retriever, and LLM to provide
    a complete pipeline for adding documents and querying them.
    """

    def __init__(
        self,
        llm: Optional[LLMInterface] = None,
        embedding_mode: Optional[Literal["performance", "accuracy"]] = None,
        db_path: Optional[str] = None,
        collection_name: str = "jarvis_rag_collection",
    ):
        """
        Initializes the RAG pipeline.

        Args:
            llm: An instance of a class implementing LLMInterface.
                 If None, defaults to the ToolAgent_LLM.
            embedding_mode: The mode for the local embedding model. If None, uses config value.
            db_path: Path to the persistent vector database. If None, uses config value.
            collection_name: Name of the collection in the vector database.
        """
        # Determine the embedding model to isolate data paths
        _embedding_mode = embedding_mode or get_rag_embedding_mode()
        embedding_models = get_rag_embedding_models()
        model_name = embedding_models[_embedding_mode]["model_name"]
        sanitized_model_name = model_name.replace("/", "_").replace("\\", "_")

        # If a specific db_path is given, use it. Otherwise, create a model-specific path.
        _final_db_path = (
            str(db_path)
            if db_path
            else os.path.join(get_rag_vector_db_path(), sanitized_model_name)
        )
        # Always create a model-specific cache path.
        _final_cache_path = os.path.join(
            get_rag_embedding_cache_path(), sanitized_model_name
        )

        self.embedding_manager = EmbeddingManager(
            mode=cast(Literal["performance", "accuracy"], _embedding_mode),
            cache_dir=_final_cache_path,
        )
        self.retriever = ChromaRetriever(
            embedding_manager=self.embedding_manager,
            db_path=_final_db_path,
            collection_name=collection_name,
        )
        # Default to the ToolAgent_LLM unless a specific LLM is provided
        self.llm = llm if llm is not None else ToolAgent_LLM()
        self.reranker = Reranker()
        # Use a standard LLM for the query rewriting task, not the agent
        self.query_rewriter = QueryRewriter(JarvisPlatform_LLM())

        print("✅ JarvisRAGPipeline 初始化成功。")

    def add_documents(self, documents: List[Document]):
        """
        Adds documents to the vector knowledge base.

        Args:
            documents: A list of LangChain Document objects to add.
        """
        self.retriever.add_documents(documents)

    def _create_prompt(
        self, query: str, context_docs: List[Document], source_files: List[str]
    ) -> str:
        """Creates the final prompt for the LLM or Agent."""
        context = "\n\n".join([doc.page_content for doc in context_docs])
        sources_text = "\n".join([f"- {source}" for source in source_files])

        prompt_template = f"""
        你是一个专家助手。请根据用户的问题，结合下面提供的参考信息来回答。

        **重要**: 提供的上下文和文件列表**仅供参考**，可能不完整或已过时。在回答前，你应该**优先使用工具（如 read_code）来获取最新、最准确的信息**。

        参考文件列表:
        ---
        {sources_text}
        ---

        参考上下文:
        ---
        {context}
        ---

        问题: {query}

        回答:
        """
        return prompt_template.strip()

    def query(self, query_text: str, n_results: int = 5) -> str:
        """
        Performs a query against the knowledge base using a multi-query
        retrieval and reranking pipeline.

        Args:
            query_text: The user's original question.
            n_results: The number of final relevant chunks to retrieve.

        Returns:
            The answer generated by the LLM.
        """
        # 1. Rewrite the original query into multiple queries
        rewritten_queries = self.query_rewriter.rewrite(query_text)

        # 2. Retrieve initial candidates for each rewritten query
        all_candidate_docs = []
        for q in rewritten_queries:
            print(f"🔍 正在为查询变体 '{q}' 进行混合检索...")
            candidates = self.retriever.retrieve(q, n_results=n_results * 2)
            all_candidate_docs.extend(candidates)

        # De-duplicate the candidate documents
        unique_docs_dict = {doc.page_content: doc for doc in all_candidate_docs}
        unique_candidate_docs = list(unique_docs_dict.values())

        if not unique_candidate_docs:
            return "我在提供的文档中找不到任何相关信息来回答您的问题。"

        # 3. Rerank the unified candidate pool against the *original* query
        print(
            f"🔍 正在对 {len(unique_candidate_docs)} 个候选文档进行重排（基于原始问题）..."
        )
        retrieved_docs = self.reranker.rerank(
            query_text, unique_candidate_docs, top_n=n_results
        )

        if not retrieved_docs:
            return "我在提供的文档中找不到任何相关信息来回答您的问题。"

        # Print the sources of the final retrieved documents
        sources = sorted(
            list(
                {
                    doc.metadata["source"]
                    for doc in retrieved_docs
                    if "source" in doc.metadata
                }
            )
        )
        if sources:
            print(f"📚 根据以下文档回答:")
            for source in sources:
                print(f"  - {source}")

        # 4. Create the final prompt and generate the answer
        # We use the original query_text for the final prompt to the LLM
        prompt = self._create_prompt(query_text, retrieved_docs, sources)

        print("🤖 正在从LLM生成答案...")
        answer = self.llm.generate(prompt)

        return answer
