{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct u-µP under SGD\n",
    "\n",
    "It has been suggested that our current implementation of u-µP under SGD is incorrect,\n",
    "insofar as it doesn't scale in the same way as µP with hidden size, even accounting\n",
    "for abc-symmetry (see the u-µP paper Eq. 2).\n",
    "\n",
    "To test this we will take the SGD implementation from the original mup repo, and\n",
    "iteratively change it towards our current implementation, seeing where it breaks down.\n",
    "We will conclude by running our fixed u-µP SGD alongside a the mup SGD to show that the\n",
    "two are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using SGD from mup repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mup\n",
    "\n",
    "import mup\n",
    "import torch\n",
    "from torch import nn\n",
    "import unit_scaling as uu\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 3\n",
    "d_inp = 5\n",
    "d_hid = 7\n",
    "d_out = 11\n",
    "steps = 3\n",
    "\n",
    "xs = torch.randn(steps, b, d_inp)\n",
    "# Set up some arbitrary function we wish to learn\n",
    "ys = torch.tanh(xs @ torch.randn(d_inp, d_out))\n",
    "ys /= ys.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard torch nn.Linear init is proportional to 1/sqrt(fan_in), but with some\n",
    "# convoluted constant. For simplicity these two classes just give 1/sqrt(fan_in) init.\n",
    "class Linear(nn.Linear):\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.normal_(self.weight, std=self.weight.shape[1] ** -0.5)\n",
    "\n",
    "\n",
    "class MuReadout(mup.MuReadout):\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.normal_(self.weight, std=self.weight.shape[1] ** -0.5)\n",
    "\n",
    "\n",
    "# Basic 3-layer MLP, no biases\n",
    "class ModelA(nn.Sequential):\n",
    "    def __init__(self, d_inp: int, d_hid: int, d_out: int):\n",
    "        super().__init__(\n",
    "            Linear(d_inp, d_hid, bias=False),\n",
    "            Linear(d_hid, d_hid, bias=False),\n",
    "            MuReadout(d_hid, d_out, bias=False),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, opt):\n",
    "    for x, y in zip(xs, ys):\n",
    "        y_pred = model(x)\n",
    "        loss = ((y - y_pred) ** 2).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        model.zero_grad()\n",
    "        print(f\"loss={loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1472)\n",
    "model_a = ModelA(d_inp, d_hid, d_out)\n",
    "# mup requires this \"base shapes\" step. This sets it up such that there is no base shape\n",
    "# (i.e. it equals 1), and only d_hid determines the model-width\n",
    "mup.set_base_shapes(model_a, ModelA(d_inp, 1, d_out))\n",
    "opt_a = mup.optim.MuSGD(model_a.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.1976\n",
      "loss=1.1297\n",
      "loss=0.9949\n"
     ]
    }
   ],
   "source": [
    "training_loop(model_a, opt_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ our test for subsequent SGD implementations being correct is that they generate\n",
    "these three loss values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permute under abc-symmetry in the same way as u-µP\n",
    "\n",
    "This doesn't give our full u-µP implementation (see next section for that), but is our\n",
    "starting point. Here we'll create the same model, but using the unit scaling library\n",
    "(`uu.`) which unit-inits the weights and adds unit-scaled multipliers. We adjust the\n",
    "lrs accordingly under abc-symmetry.\n",
    "\n",
    "To make this concrete, here is the original table of SGD scaling factors for mup, as\n",
    "implemented in their library (table 8 in the Tensor Programs V paper):\n",
    "\n",
    "| | input | hidden | output |\n",
    "|-|-|-|-|\n",
    "| mult (A) | 1 | 1 | 1/d_hid |\n",
    "| init std (B) | 1/sqrt(d_in) | 1/sqrt(d_hid) | 1 |\n",
    "| SGD lr (C) | d_hid | 1 | d_hid |\n",
    "\n",
    "abc-symmetry says that our model's dynamics are invariant to changes of the form:\n",
    "\n",
    "```A ← Aθ, B ← B/θ, C ← C/θ^2```\n",
    "\n",
    "Setting `θ` to give `B=1` everywhere gives our unit-scaled table:\n",
    "\n",
    "| | input | hidden | output |\n",
    "|-|-|-|-|\n",
    "| mult (A) | 1/sqrt(d_in) | 1/sqrt(d_hid) | 1/d_hid |\n",
    "| init std (B) | 1 | 1 | 1 |\n",
    "| SGD lr (C) | d_hid * d_in | d_hid | d_hid |\n",
    "\n",
    "Our unit-scaled layers implement A and B - this gives our second model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelB(nn.Sequential):\n",
    "    def __init__(self, d_inp: int, d_hid: int, d_out: int):\n",
    "        super().__init__(\n",
    "            uu.Linear(d_inp, d_hid),\n",
    "            uu.Linear(d_hid, d_hid),\n",
    "            uu.LinearReadout(d_hid, d_out, constraint=\"to_output_scale\"),\n",
    "        )\n",
    "# The \"to_output_scale\" is necessary here, for reasons explained in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For C, our SGD LRs, the bottom row of the above table suggests we wish to scale by\n",
    "\n",
    "```(d_hid * d_inp**0.5, d_hid**0.5, 1)```.\n",
    "\n",
    "However our unit-scaling library doesn't apply the A multipliers when computing grad_ws\n",
    "(this ensures unit-scale), meaning we actually need to use the top row multiplied by the\n",
    "bottom row for our SGD LRs. This gives:\n",
    "\n",
    "```(d_hid * d_inp**0.5, d_hid**0.5, 1)```,\n",
    "\n",
    "which we use below.\n",
    "\n",
    "Our unit-scaled layers also contain a special multiplier for the grad_w calculation,\n",
    "designed to maintain unit scale. This multiplier equals 1/sqrt(batch_size). To get back\n",
    "to equivalence with the original SGD we therefore also multiply the base lr by \n",
    "sqrt(batch_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_param_groups(model, base_lr, lr_mods):\n",
    "    parameter_groups = []\n",
    "    for params, lr_mod in zip(model.parameters(), lr_mods):\n",
    "        parameter_groups.append({\"params\": [params], \"lr\": base_lr * lr_mod})\n",
    "    return parameter_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1472)\n",
    "\n",
    "model_b = ModelB(d_inp, d_hid, d_out)\n",
    "base_lr = 1e-1 * b**0.5\n",
    "opt_b = torch.optim.SGD(\n",
    "    gen_param_groups(model_b, base_lr, lr_mods=(d_hid * d_inp**0.5, d_hid**0.5, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.1976\n",
      "loss=1.1297\n",
      "loss=0.9949\n"
     ]
    }
   ],
   "source": [
    "training_loop(model_b, opt_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss here is exactly the same as in the previous case, meaning our implementation\n",
    "is still correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce unit-scaled readout scaling\n",
    "\n",
    "The final step to get a unit-scaled model here is to introduce the following\n",
    "trick in the backward pass:\n",
    "\n",
    "Our above model has to use 1/d_hid as our output mult.\n",
    "This differs from the 1/sqrt(d) mult we usually use to get unit-scaling at init.\n",
    "This is fine in the forward pass as the readout is the last layer, but in the backward\n",
    "pass it's more of a problem as this mis-scaling propagates.\n",
    "To fix this, we simply hack the gradient of `uu.LinearReadout` to use the ideal unit\n",
    "scaling factor in the backward pass, which in this case is 1/sqrt(d_out), keeping\n",
    "1/d_hid in the forward.\n",
    "\n",
    "This is the default implementation of `uu.LinearReadout`, and is why we had to add\n",
    "`constraint=\"to_output_scale\"` in our previous `ModelB`. This change gives us `ModelC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelC(nn.Sequential):\n",
    "    def __init__(self, d_inp: int, d_hid: int, d_out: int):\n",
    "        super().__init__(\n",
    "            uu.Linear(d_inp, d_hid),\n",
    "            uu.Linear(d_hid, d_hid),\n",
    "            uu.LinearReadout(d_hid, d_out),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same optimizer as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1472)\n",
    "\n",
    "model_c = ModelC(d_inp, d_hid, d_out)\n",
    "base_lr = 1e-1 * b**0.5\n",
    "opt_c = torch.optim.SGD(\n",
    "    gen_param_groups(model_c, base_lr, lr_mods=(d_inp**0.5 * d_hid, d_hid**0.5, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.1976\n",
      "loss=1.1471\n",
      "loss=0.9887\n"
     ]
    }
   ],
   "source": [
    "training_loop(model_c, opt_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that the loss values are different — something has changed here and our SGD\n",
    "implementation is now broken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using above + fix - show it's the same\n",
    "\n",
    "The problem is that our backward pass re-scaling must be compensated for in our LRs.\n",
    "We have gone from 1/d_hid scaling in our readout grad to 1/sqrt(d_out). Thus\n",
    "to compensate we must multiply all our learning rates by sqrt(d_out)/d_hid.\n",
    "\n",
    "Recall that previously we had:\n",
    "\n",
    "```(d_hid * d_inp**0.5, d_hid**0.5, 1)```,\n",
    "\n",
    "which now changes to:\n",
    "\n",
    "```\n",
    "(d_hid * d_inp**0.5 * sqrt(d_out)/d_hid, d_hid**0.5 * sqrt(d_out)/d_hid, 1)\n",
    "= (d_inp**0.5 * d_out**0.5, d_hid**-0.5 * d_out**0.5, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1472)\n",
    "\n",
    "model_c2 = ModelC(d_inp, d_hid, d_out)\n",
    "base_lr = 1e-1 * b**0.5\n",
    "opt_c2 = torch.optim.SGD(\n",
    "    gen_param_groups(\n",
    "        model_c2,\n",
    "        base_lr,\n",
    "        lr_mods=(d_inp**0.5 * d_out**0.5, d_hid**-0.5 * d_out**0.5, 1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.1976\n",
      "loss=1.1297\n",
      "loss=0.9949\n"
     ]
    }
   ],
   "source": [
    "training_loop(model_c2, opt_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equal to our original mup SGD loss, meaning that we've fixed the problem! We\n",
    "now have a unit-scaled model with identical dynamics to the mup model under SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show corrected library implementation\n",
    "\n",
    "All that remains is to fix our library and show that it indeed matches the original mup\n",
    "SGD implementation.\n",
    "\n",
    "Below is our library SGD implementation, but with one modification added to handle the\n",
    "case (which we make the default) that the readout layer has a `None` constraint - i.e.\n",
    "the gradient is re-scaled, and we must correct for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scale_func_sgd(readout_constraint: Optional[str]):\n",
    "    \"\"\"Calculate the LR scaling factor for :class:`torch.optim.SGD`.\"\"\"\n",
    "\n",
    "    def lr_scale_func_sgd_inner(param):\n",
    "        scale = uu.optim.lr_scale_for_depth(param)\n",
    "        if param.mup_type in (\"bias\", \"norm\"):\n",
    "            return scale * param.shape[0]\n",
    "        if param.mup_type == \"weight\":\n",
    "            if readout_constraint is None:  # <<< NEW MODIFICATION\n",
    "                return scale * uu.optim._get_fan_in(param) ** -0.5\n",
    "            elif readout_constraint == \"to_output_scale\":  # <<< existing case\n",
    "                return scale * uu.optim._get_fan_in(param) ** 0.5\n",
    "            else:\n",
    "                assert False, f\"Unhandled readout constraint: {readout_constraint}\"\n",
    "        if param.mup_type == \"output\":\n",
    "            return scale\n",
    "        assert False, f\"Unexpected mup_type {param.mup_type}\"\n",
    "\n",
    "    return lr_scale_func_sgd_inner\n",
    "\n",
    "\n",
    "class SGD(torch.optim.SGD):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr,\n",
    "        *args,\n",
    "        weight_decay: float = 0,\n",
    "        independent_weight_decay: bool = True,\n",
    "        allow_non_unit_scaling_params: bool = False,\n",
    "        readout_constraint: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        params = uu.optim.scaled_parameters(\n",
    "            params,\n",
    "            lr_scale_func_sgd(readout_constraint),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            independent_weight_decay=independent_weight_decay,\n",
    "            allow_non_unit_scaling_params=allow_non_unit_scaling_params,\n",
    "        )\n",
    "        # No need to forward {lr, weight_decay}, as each group has these specified\n",
    "        super().__init__(params, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1472)\n",
    "\n",
    "model_c3 = ModelC(d_inp, d_hid, d_out)\n",
    "base_lr = 1e-1\n",
    "opt_c3 = SGD(model_c3.parameters(), base_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.1976\n",
      "loss=1.1236\n",
      "loss=1.0068\n"
     ]
    }
   ],
   "source": [
    "training_loop(model_c3, opt_c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly different from our previous loss value. However the difference is only\n",
    "in a couple of constant factors that are introduced by the two schemes — crucially _not_\n",
    "factors that depend on model-width.\n",
    "\n",
    "Specifically, these factors are the 1/sqrt(d_in) scaling factor in the very first mup\n",
    "table presented, and the 1/sqrt(d_out) factor the we introduce in u-µP's readout grad\n",
    "scaling.\n",
    "\n",
    "Below we show how to correct for these constants under SGD such that the two schemes\n",
    "are exactly the same. This is just our original `ModelA` with some non-width-dependent\n",
    "LR tweaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1472)\n",
    "\n",
    "model_a2 = ModelA(d_inp, d_hid, d_out)\n",
    "mup.set_base_shapes(model_a2, ModelA(d_inp, 1, d_out))\n",
    "base_lr = 1e-1 * b**-0.5\n",
    "opt_a2 = mup.optim.MuSGD(\n",
    "    gen_param_groups(\n",
    "        model_a2, base_lr, lr_mods=(d_inp**-1.0 * d_out**-0.5, d_out**-0.5, 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.1976\n",
      "loss=1.1236\n",
      "loss=1.0068\n"
     ]
    }
   ],
   "source": [
    "training_loop(model_a2, opt_a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, we get exactly the same loss, confirming that this fixed u-µP\n",
    "SGD implementation scales in the same way as the original mup SGD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
