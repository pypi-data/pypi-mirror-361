{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyForge CLI End-to-End Testing - Local Environment\n\nThis notebook tests PyForge CLI functionality in a local environment using the pre-built wheel from the dist/ directory.\n\n## Key Differences from Serverless Notebook\n- **Installation Source**: Local dist/ directory instead of Unity Catalog Volume\n- **Sample Datasets**: Downloads to local filesystem instead of volumes\n- **No Databricks Widgets**: Uses variables directly (can be converted to parameters for papermill)\n- **No PySpark**: Tests only pandas-based conversions\n\n## Test Configuration\n- **Environment**: Local Python virtual environment\n- **Installation Source**: dist/pyforge_cli-*.whl (local build)\n- **Sample Data**: Real sample datasets from v1.0.5 release\n- **Output Format**: Parquet and other supported formats\n\n## Prerequisites\n1. PyForge CLI wheel built in dist/ directory via `python -m build`\n2. Python 3.8+ installed locally\n3. Write permissions for creating test directories\n\n## How to Use This Notebook\n1. Ensure you have built the wheel: `python -m build --wheel`\n2. Run all cells in sequence\n3. Review the test results and summary report\n\n## Key Features of This Notebook\n1. **Consistent Structure**: Mirrors the serverless notebook for easy maintenance\n2. **Comprehensive Testing**: Tests all supported file formats\n3. **Directory Creation**: Ensures output directories exist before conversion\n4. **PDF Handling**: Skips PDF conversions due to known issues\n5. **Detailed Observations**: Logs test results for each conversion\n6. **Error Handling**: Graceful handling of known issues",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration Parameters (equivalent to Databricks widgets)\n# =============================================================================\n# CONFIGURATION SECTION\n# =============================================================================\n\nimport os\nimport sys\nfrom pathlib import Path\n\n# Navigate to project root (3 levels up from notebook location)\nproject_root = Path(os.getcwd()).parent.parent.parent\nos.chdir(project_root)\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Configuration parameters (these would be widgets in Databricks)\nSAMPLE_DATASETS_PATH = \"sample-datasets\"\nPYFORGE_VERSION = \"1.0.8.dev4\"  # Will be detected from wheel\nFORCE_CONVERSION = True\nTEST_SMALLEST_FILES_ONLY = True\nSKIP_PDF_FILES = True  # Due to known issues\n\n# Derived paths\nVENV_PATH = \"test_env\"\nCONVERTED_OUTPUT_PATH = \"test_output\"\n\n# Find latest wheel in dist/\nimport glob\nwheel_files = glob.glob(\"dist/pyforge_cli-*.whl\")\nif wheel_files:\n    PYFORGE_WHEEL_PATH = sorted(wheel_files)[-1]  # Get latest version\n    # Extract version from wheel filename\n    import re\n    version_match = re.search(r'pyforge_cli-(.+?)-py', PYFORGE_WHEEL_PATH)\n    if version_match:\n        PYFORGE_VERSION = version_match.group(1)\nelse:\n    PYFORGE_WHEEL_PATH = None\n\nprint(f\"üîß Configuration:\")\nprint(f\"   PyForge Version: {PYFORGE_VERSION}\")\nprint(f\"   PyForge Wheel Path: {PYFORGE_WHEEL_PATH}\")\nprint(f\"   Sample Datasets Path: {SAMPLE_DATASETS_PATH}\")\nprint(f\"   Output Path: {CONVERTED_OUTPUT_PATH}\")\nprint(f\"   Force Conversion: {FORCE_CONVERSION}\")\nprint(f\"   Test Smallest Files Only: {TEST_SMALLEST_FILES_ONLY}\")\nprint(f\"   Skip PDF Files: {SKIP_PDF_FILES}\")\n\nif not PYFORGE_WHEEL_PATH:\n    print(\"‚ùå ERROR: No wheel found in dist/. Please build first: python -m build --wheel\")\n    sys.exit(1)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "%%sh\n# Environment Setup - Create Virtual Environment\n# =============================================================================\n# ENVIRONMENT SETUP SECTION\n# =============================================================================\n\necho \"üîç Setting up test environment...\"\n\n# Remove existing test environment for clean start\nif [ -d \"test_env\" ]; then\n    echo \"Removing existing test environment...\"\n    rm -rf test_env\nfi\n\n# Create fresh virtual environment\npython3 -m venv test_env\necho \"‚úÖ Created fresh virtual environment: test_env/\"\n\n# Verify environment creation\nif [ -f \"test_env/bin/activate\" ]; then\n    echo \"‚úÖ Virtual environment created successfully\"\nelse\n    echo \"‚ùå Failed to create virtual environment\"\n    exit 1\nfi"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "%%sh\n# Install PyForge CLI from Local Wheel\n# =============================================================================\n# INSTALLATION FROM LOCAL WHEEL\n# =============================================================================\n\n# Find the latest wheel file\nWHEEL_FILE=$(ls dist/pyforge_cli-*.whl | sort -V | tail -1)\n\necho \"üì¶ Installing PyForge CLI from local wheel...\"\necho \"   Installing from: $WHEEL_FILE\"\n\n# Activate virtual environment and install\nsource test_env/bin/activate\n\n# Upgrade pip first\npip install --upgrade pip --quiet\n\n# Install PyForge from wheel\npip install \"$WHEEL_FILE\"\n\necho \"‚úÖ PyForge CLI installed successfully!\"\n\n# Verify installation\necho \"\"\necho \"üîç Verifying installation...\"\npyforge --version\n\n# List installed packages\necho \"\"\necho \"üìã Key dependencies installed:\"\npip list | grep -E \"(pandas|pyarrow|openpyxl|PyMuPDF|chardet|requests|dbfread|jaydebeapi)\""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "%%sh\n# Setup Sample Datasets\n# =============================================================================\n# SAMPLE DATASETS SETUP\n# =============================================================================\n\nsource test_env/bin/activate\n\necho \"üì• Setting up sample datasets...\"\n\n# Remove existing sample datasets for clean start\nif [ -d \"sample-datasets\" ]; then\n    echo \"Removing existing sample-datasets directory...\"\n    rm -rf sample-datasets\nfi\n\n# Install sample datasets using PyForge CLI\necho \"üì¶ Installing sample datasets using PyForge CLI...\"\npyforge install sample-datasets --force\n\n# Check if installation was successful\nif [ -d \"sample-datasets\" ]; then\n    echo \"‚úÖ Sample datasets installed successfully!\"\n    \n    # Count files by type\n    echo \"\"\n    echo \"üìä Sample datasets summary:\"\n    echo \"   CSV files: $(find sample-datasets -name '*.csv' 2>/dev/null | wc -l)\"\n    echo \"   XML files: $(find sample-datasets -name '*.xml' 2>/dev/null | wc -l)\"\n    echo \"   Excel files: $(find sample-datasets -name '*.xlsx' -o -name '*.xls' 2>/dev/null | wc -l)\"\n    echo \"   PDF files: $(find sample-datasets -name '*.pdf' 2>/dev/null | wc -l)\"\n    echo \"   Access files: $(find sample-datasets -name '*.mdb' -o -name '*.accdb' 2>/dev/null | wc -l)\"\n    echo \"   DBF files: $(find sample-datasets -name '*.dbf' 2>/dev/null | wc -l)\"\nelse\n    echo \"‚ö†Ô∏è  Sample datasets installation may have failed\"\nfi"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Discover and Display Downloaded Files\n# =============================================================================\n# FILE DISCOVERY AND DETAILED DISPLAY\n# =============================================================================\n\nimport os\nimport subprocess\nimport pandas as pd\nfrom datetime import datetime\nimport json\n\ndef discover_and_display_files():\n    \"\"\"Discover all downloaded files and display them with size information.\"\"\"\n    print(\"üîç Discovering all downloaded files in sample datasets...\")\n    \n    all_files = []\n    files_by_type = {}\n    supported_extensions = {\n        '.csv': 'CSV',\n        '.xlsx': 'Excel', \n        '.xls': 'Excel',\n        '.xml': 'XML',\n        '.pdf': 'PDF',\n        '.dbf': 'DBF',\n        '.mdb': 'MDB',\n        '.accdb': 'ACCDB'\n    }\n    \n    if os.path.exists(SAMPLE_DATASETS_PATH):\n        for root, dirs, files in os.walk(SAMPLE_DATASETS_PATH):\n            # Skip already converted files\n            if 'converted' in root or 'parquet' in root:\n                continue\n                \n            for file in files:\n                file_path = os.path.join(root, file)\n                file_ext = os.path.splitext(file)[1].lower()\n                \n                if file_ext in supported_extensions:\n                    # Get relative path for better display\n                    rel_path = os.path.relpath(file_path, SAMPLE_DATASETS_PATH)\n                    folder_category = rel_path.split(os.sep)[0] if os.sep in rel_path else 'root'\n                    \n                    file_info = {\n                        'file_name': file,\n                        'file_type': supported_extensions[file_ext],\n                        'extension': file_ext,\n                        'category': folder_category,\n                        'file_path': file_path,\n                        'relative_path': rel_path,\n                        'size_bytes': os.path.getsize(file_path),\n                        'size_mb': round(os.path.getsize(file_path) / (1024*1024), 3),\n                        'size_readable': format_file_size(os.path.getsize(file_path))\n                    }\n                    \n                    all_files.append(file_info)\n                    \n                    # Group by file type\n                    if file_info['file_type'] not in files_by_type:\n                        files_by_type[file_info['file_type']] = []\n                    files_by_type[file_info['file_type']].append(file_info)\n        \n        # Sort files by size within each type\n        for file_type in files_by_type:\n            files_by_type[file_type].sort(key=lambda x: x['size_bytes'])\n            \n    return all_files, files_by_type\n\ndef format_file_size(size_bytes):\n    \"\"\"Format file size in human-readable format.\"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if size_bytes < 1024.0:\n            return f\"{size_bytes:.2f} {unit}\"\n        size_bytes /= 1024.0\n    return f\"{size_bytes:.2f} TB\"\n\n# Discover files\nall_files, files_by_type = discover_and_display_files()\n\n# Display summary statistics\nprint(f\"\\nüìä Downloaded Files Summary:\")\nprint(f\"   Total files found: {len(all_files)}\")\nprint(f\"   Total size: {format_file_size(sum(f['size_bytes'] for f in all_files))}\")\nprint(f\"   File types: {', '.join(sorted(files_by_type.keys()))}\")\n\n# Display files by type\nprint(\"\\nüìã Files by Type (sorted by size):\")\nfor file_type, files in sorted(files_by_type.items()):\n    print(f\"\\n{file_type} Files ({len(files)} files):\")\n    for i, file_info in enumerate(files[:3]):  # Show first 3 files of each type\n        print(f\"   {i+1}. {file_info['file_name']} - {file_info['size_readable']} - {file_info['relative_path']}\")\n    if len(files) > 3:\n        print(f\"   ... and {len(files) - 3} more {file_type} files\")\n\n# Create DataFrame for display\nif all_files:\n    df_all_files = pd.DataFrame(all_files)\n    \n    # Summary by file type\n    print(\"\\nüìä Detailed Summary by File Type:\")\n    summary_by_type = df_all_files.groupby('file_type').agg({\n        'file_name': 'count',\n        'size_mb': ['sum', 'mean', 'min', 'max']\n    }).round(3)\n    summary_by_type.columns = ['file_count', 'total_size_mb', 'avg_size_mb', 'min_size_mb', 'max_size_mb']\n    print(summary_by_type.to_string())\n    \n    # Show smallest file of each type\n    print(\"\\nüéØ Smallest File of Each Type (for testing):\")\n    smallest_files = []\n    for file_type in files_by_type:\n        if files_by_type[file_type]:\n            smallest = files_by_type[file_type][0]  # Already sorted by size\n            smallest_files.append(smallest)\n            print(f\"   {smallest['file_type']}: {smallest['file_name']} ({smallest['size_readable']})\")\n    \nelse:\n    print(\"\\n‚ö†Ô∏è  No files found in the sample datasets directory.\")\n    print(\"   Please check if the sample datasets were downloaded successfully.\")\n\n# Store the catalog for later use\nfiles_catalog = all_files\nprint(f\"\\n‚úÖ File discovery completed. Found {len(files_catalog)} files ready for testing.\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Select Files for Testing\n# =============================================================================\n# FILE SELECTION FOR TESTING\n# =============================================================================\n\ndef select_files_for_testing(all_files, files_by_type, test_smallest_only=True):\n    \"\"\"Select files for testing based on configuration.\"\"\"\n    selected_files = []\n    \n    if test_smallest_only:\n        print(\"üéØ Selecting SMALLEST file of each type for testing...\")\n        \n        # Get smallest file of each type\n        for file_type in sorted(files_by_type.keys()):\n            if files_by_type[file_type]:\n                smallest_file = files_by_type[file_type][0]  # Already sorted by size\n                selected_files.append(smallest_file)\n                print(f\"   {file_type}: {smallest_file['file_name']} ({smallest_file['size_readable']})\")\n    else:\n        print(\"üìã Selecting ALL files for testing...\")\n        selected_files = all_files\n        print(f\"   Total files selected: {len(selected_files)}\")\n    \n    return selected_files\n\n# Select files based on configuration\nfiles_for_testing = select_files_for_testing(all_files, files_by_type, TEST_SMALLEST_FILES_ONLY)\n\n# Display selected files\nprint(f\"\\nüìä Files Selected for Testing: {len(files_for_testing)}\")\nif files_for_testing:\n    df_selected = pd.DataFrame(files_for_testing)\n    print(\"\\nSelected files:\")\n    print(df_selected[['file_type', 'file_name', 'size_readable', 'category', 'file_path']].to_string(index=False))\n    \n    # Calculate total size and estimated time\n    total_size_mb = sum(f['size_mb'] for f in files_for_testing)\n    estimated_time = len(files_for_testing) * 5  # Assume 5 seconds per file average\n    \n    print(f\"\\nüìà Test Estimation:\")\n    print(f\"   Files to process: {len(files_for_testing)}\")\n    print(f\"   Total data size: {format_file_size(total_size_mb * 1024 * 1024)}\")\n    print(f\"   Estimated time: ~{estimated_time} seconds\")\nelse:\n    print(\"‚ö†Ô∏è  No files selected for testing!\")\n\n# Update files_catalog with selected files\nfiles_catalog = files_for_testing\nprint(f\"\\n‚úÖ File selection completed. {len(files_catalog)} files ready for conversion testing.\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive Conversion Testing\n# =============================================================================\n# BULK CONVERSION TESTING\n# =============================================================================\n\nimport time\n\ndef run_conversion_test(file_info):\n    \"\"\"Run conversion test for a single file.\"\"\"\n    file_path = file_info['file_path']\n    file_type = file_info['file_type']\n    file_name = file_info['file_name']\n    file_ext = file_info['extension']\n    \n    # Create output path\n    output_name = file_name.split('.')[0]\n    output_dir = os.path.join(CONVERTED_OUTPUT_PATH, file_info['category'])\n    output_path = os.path.join(output_dir, f\"{output_name}.parquet\")\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Skip PDF files if configured\n    if SKIP_PDF_FILES and file_ext == '.pdf':\n        print(f\"   ‚ö†Ô∏è  Skipping PDF file - known conversion issues\")\n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': 'SKIPPED',\n            'duration_seconds': 0,\n            'error_message': 'PDF conversion temporarily disabled due to known issues',\n            'output_path': None,\n            'size_mb': file_info.get('size_mb', 0),\n            'converter_used': 'N/A',\n            'observation': {\n                'file': file_name,\n                'type': file_type,\n                'status': 'SKIPPED',\n                'reason': 'PDF conversion issues'\n            }\n        }\n    \n    # Build conversion command\n    cmd = [\n        f'{VENV_PATH}/bin/pyforge', 'convert', file_path, output_path, \n        '--format', 'parquet'\n    ]\n    \n    if FORCE_CONVERSION:\n        cmd.append('--force')\n    \n    print(f\"\\nüîÑ Converting {file_name} ({file_type})...\")\n    print(f\"   File size: {file_info.get('size_readable', 'Unknown')}\")\n    print(f\"   Output dir: {output_dir}\")\n    print(f\"   Command: {' '.join(cmd)}\")\n    \n    # Log test observation\n    observation = {\n        'file': file_name,\n        'type': file_type,\n        'size': file_info.get('size_readable', 'Unknown'),\n        'start_time': datetime.now().strftime('%H:%M:%S')\n    }\n    \n    try:\n        start_time = time.time()\n        \n        # Set timeout based on file size\n        file_size_mb = file_info.get('size_mb', 0)\n        if file_size_mb > 100:\n            timeout = 300  # 5 minutes for large files\n        elif file_size_mb > 10:\n            timeout = 120  # 2 minutes for medium files\n        else:\n            timeout = 60  # 1 minute for small files\n        \n        print(f\"   Timeout: {timeout}s\")\n        \n        # Run conversion\n        result = subprocess.run(\n            cmd, \n            capture_output=True, \n            text=True, \n            timeout=timeout\n        )\n        \n        end_time = time.time()\n        duration = round(end_time - start_time, 2)\n        \n        if result.returncode == 0:\n            status = 'SUCCESS'\n            error_message = None\n            converter_used = 'Standard'  # Local doesn't have PySpark\n            print(f\"   ‚úÖ Success ({duration}s)\")\n            \n            # Verify output file exists\n            if os.path.exists(output_path):\n                print(f\"   ‚úÖ Output file verified: {output_path}\")\n                observation['output_verified'] = True\n            else:\n                print(f\"   ‚ö†Ô∏è  Output file not found\")\n                observation['output_verified'] = False\n                \n            observation['status'] = 'SUCCESS'\n            observation['duration'] = f\"{duration}s\"\n            observation['converter'] = converter_used\n        else:\n            status = 'FAILED'\n            error_message = result.stderr.strip() if result.stderr else result.stdout.strip()\n            converter_used = 'Unknown'\n            print(f\"   ‚ùå Failed ({duration}s)\")\n            print(f\"   Error: {error_message[:200]}...\")\n            \n            observation['status'] = 'FAILED'\n            observation['duration'] = f\"{duration}s\"\n            observation['error'] = error_message[:200]\n        \n        # Print detailed observation\n        print(f\"\\nüìù Test Observation:\")\n        for key, value in observation.items():\n            print(f\"   {key}: {value}\")\n        \n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': status,\n            'duration_seconds': duration,\n            'error_message': error_message,\n            'output_path': output_path if status == 'SUCCESS' else None,\n            'size_mb': file_size_mb,\n            'command': ' '.join(cmd),\n            'converter_used': converter_used,\n            'observation': observation\n        }\n        \n    except subprocess.TimeoutExpired:\n        observation['status'] = 'TIMEOUT'\n        observation['duration'] = f\"{timeout}s\"\n        print(f\"   ‚è∞ Timeout after {timeout}s\")\n        \n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': 'TIMEOUT',\n            'duration_seconds': timeout,\n            'error_message': f'Conversion timed out after {timeout} seconds',\n            'output_path': None,\n            'size_mb': file_size_mb,\n            'command': ' '.join(cmd),\n            'converter_used': 'Unknown',\n            'observation': observation\n        }\n    except Exception as e:\n        observation['status'] = 'ERROR'\n        observation['error'] = str(e)\n        print(f\"   üö´ Error: {str(e)}\")\n        \n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': 'ERROR',\n            'duration_seconds': 0,\n            'error_message': str(e),\n            'output_path': None,\n            'size_mb': file_size_mb,\n            'command': ' '.join(cmd),\n            'converter_used': 'Unknown',\n            'observation': observation\n        }\n\ndef run_bulk_tests():\n    \"\"\"Run conversion tests for selected files.\"\"\"\n    print(f\"\\nüöÄ Starting conversion tests...\")\n    print(f\"üìÅ Output directory: {CONVERTED_OUTPUT_PATH}\")\n    print(f\"üìä Test mode: {'Smallest files only' if TEST_SMALLEST_FILES_ONLY else 'All files'}\")\n    print(f\"üîß Force conversion: {FORCE_CONVERSION}\")\n    \n    # Ensure base output directory exists\n    os.makedirs(CONVERTED_OUTPUT_PATH, exist_ok=True)\n    \n    test_results = []\n    test_observations = []\n    total_start_time = time.time()\n    \n    for i, file_info in enumerate(files_catalog, 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"üìù Test {i}/{len(files_catalog)}\")\n        result = run_conversion_test(file_info)\n        test_results.append(result)\n        test_observations.append(result['observation'])\n    \n    total_end_time = time.time()\n    total_duration = round(total_end_time - total_start_time, 2)\n    \n    # Print test observations summary\n    print(f\"\\n{'='*60}\")\n    print(\"üìä TEST OBSERVATIONS SUMMARY:\")\n    print(f\"{'='*60}\")\n    for obs in test_observations:\n        print(f\"\\n{obs['file']} ({obs['type']}, {obs.get('size', 'Unknown')}):\")\n        print(f\"   Status: {obs['status']}\")\n        if 'duration' in obs:\n            print(f\"   Duration: {obs.get('duration', 'N/A')}\")\n        if 'converter' in obs:\n            print(f\"   Converter: {obs['converter']}\")\n        if 'reason' in obs:\n            print(f\"   Reason: {obs['reason']}\")\n        if 'error' in obs:\n            print(f\"   Error: {obs['error'][:100]}...\")\n    \n    return test_results, total_duration\n\n# Run the bulk conversion tests\nprint(\"üéØ Executing conversion tests...\")\ntest_results, total_test_duration = run_bulk_tests()\n\nprint(f\"\\nüèÅ Conversion testing completed in {total_test_duration} seconds!\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Generate Summary Report\n# =============================================================================\n# SUMMARY REPORT GENERATION\n# =============================================================================\n\ndef generate_summary_report(test_results, total_duration):\n    \"\"\"Generate comprehensive summary report of conversion tests.\"\"\"\n    \n    df_results = pd.DataFrame(test_results)\n    \n    # Overall statistics\n    total_files = len(test_results)\n    successful = len(df_results[df_results['status'] == 'SUCCESS']) if len(df_results) > 0 else 0\n    failed = len(df_results[df_results['status'] == 'FAILED']) if len(df_results) > 0 else 0\n    skipped = len(df_results[df_results['status'] == 'SKIPPED']) if len(df_results) > 0 else 0\n    timeout = len(df_results[df_results['status'] == 'TIMEOUT']) if len(df_results) > 0 else 0\n    errors = len(df_results[df_results['status'] == 'ERROR']) if len(df_results) > 0 else 0\n    \n    # Calculate success rate excluding skipped files\n    files_attempted = total_files - skipped\n    success_rate = round((successful / files_attempted) * 100, 1) if files_attempted > 0 else 0\n    \n    # Performance statistics\n    successful_tests = df_results[df_results['status'] == 'SUCCESS'] if len(df_results) > 0 else pd.DataFrame()\n    avg_duration = round(successful_tests['duration_seconds'].mean(), 2) if len(successful_tests) > 0 else 0\n    total_conversion_time = round(df_results['duration_seconds'].sum(), 2) if len(df_results) > 0 else 0\n    total_size_processed = round(successful_tests['size_mb'].sum(), 2) if len(successful_tests) > 0 else 0\n    \n    # Summary dictionary\n    summary = {\n        'test_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'environment': 'Local Python Environment',\n        'pyforge_version': PYFORGE_VERSION,\n        'total_files_tested': total_files,\n        'files_attempted': files_attempted,\n        'successful_conversions': successful,\n        'failed_conversions': failed,\n        'skipped_files': skipped,\n        'timeout_files': timeout,\n        'error_files': errors,\n        'success_rate_percent': success_rate,\n        'total_test_duration_seconds': total_duration,\n        'total_conversion_time_seconds': total_conversion_time,\n        'average_conversion_time_seconds': avg_duration,\n        'total_data_processed_mb': total_size_processed,\n        'wheel_path': PYFORGE_WHEEL_PATH,\n        'sample_datasets_path': SAMPLE_DATASETS_PATH,\n        'output_directory': CONVERTED_OUTPUT_PATH\n    }\n    \n    return summary, df_results\n\n# Generate summary report\nsummary_report, df_detailed_results = generate_summary_report(test_results, total_test_duration)\n\n# Display summary report\nprint(\"=\" * 80)\nprint(\"üéØ PYFORGE CLI LOCAL TESTING SUMMARY\")\nprint(\"=\" * 80)\n\nprint(f\"üìÖ Test Timestamp: {summary_report['test_timestamp']}\")\nprint(f\"üè¢ Environment: {summary_report['environment']}\")\nprint(f\"üîß PyForge Version: {summary_report['pyforge_version']}\")\nprint(f\"üì¶ Wheel Path: {summary_report['wheel_path']}\")\n\nprint(\"\\nüìä OVERALL RESULTS:\")\nprint(f\"   Total Files: {summary_report['total_files_tested']}\")\nprint(f\"   Files Attempted: {summary_report['files_attempted']}\")\nprint(f\"   ‚úÖ Successful: {summary_report['successful_conversions']}\")\nprint(f\"   ‚ùå Failed: {summary_report['failed_conversions']}\")\nprint(f\"   ‚è≠Ô∏è  Skipped: {summary_report['skipped_files']}\")\nprint(f\"   ‚è∞ Timeout: {summary_report['timeout_files']}\")\nprint(f\"   üö´ Errors: {summary_report['error_files']}\")\nprint(f\"   üéØ Success Rate: {summary_report['success_rate_percent']}% (of attempted files)\")\n\nprint(\"\\n‚è±Ô∏è  PERFORMANCE METRICS:\")\nprint(f\"   Total Test Duration: {summary_report['total_test_duration_seconds']}s\")\nprint(f\"   Total Conversion Time: {summary_report['total_conversion_time_seconds']}s\")\nprint(f\"   Average Conversion Time: {summary_report['average_conversion_time_seconds']}s\")\nprint(f\"   Total Data Processed: {summary_report['total_data_processed_mb']} MB\")\n\nprint(\"\\nüìã RESULTS BY FILE TYPE:\")\nif len(df_detailed_results) > 0:\n    type_summary = df_detailed_results.groupby('file_type')['status'].value_counts().unstack(fill_value=0)\n    print(type_summary.to_string())\n    \n    print(\"\\nüìä DETAILED RESULTS:\")\n    print(df_detailed_results[['file_name', 'file_type', 'status', 'duration_seconds', 'size_mb', 'converter_used']].to_string(index=False))\n    \n    # Show failed conversions details\n    failed_tests = df_detailed_results[df_detailed_results['status'].isin(['FAILED', 'ERROR', 'TIMEOUT'])]\n    if len(failed_tests) > 0:\n        print(f\"\\n‚ùå FAILED CONVERSIONS DETAILS ({len(failed_tests)} failures):\")\n        for _, test in failed_tests.iterrows():\n            print(f\"\\n{test['file_name']} ({test['file_type']}):\")\n            print(f\"   Status: {test['status']}\")\n            if test['error_message']:\n                print(f\"   Error: {test['error_message'][:200]}...\")\n    \n    # Show skipped files\n    skipped_tests = df_detailed_results[df_detailed_results['status'] == 'SKIPPED']\n    if len(skipped_tests) > 0:\n        print(f\"\\n‚è≠Ô∏è  SKIPPED FILES ({len(skipped_tests)} files):\")\n        for _, test in skipped_tests.iterrows():\n            print(f\"   {test['file_name']}: {test['error_message']}\")\nelse:\n    print(\"   No test results to display\")\n\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Validate Converted Files\n# =============================================================================\n# CONVERTED FILE VALIDATION\n# =============================================================================\n\ndef validate_converted_files():\n    \"\"\"Validate converted Parquet files using pandas.\"\"\"\n    print(\"üîç Validating converted Parquet files...\")\n    \n    successful_conversions = df_detailed_results[df_detailed_results['status'] == 'SUCCESS']\n    validation_results = []\n    \n    if len(successful_conversions) == 0:\n        print(\"‚ö†Ô∏è  No successful conversions to validate.\")\n        return\n    \n    for _, result in successful_conversions.iterrows():\n        output_path = result['output_path']\n        file_name = result['file_name']\n        file_type = result['file_type']\n        \n        # Skip PDF validations as they're known to have issues\n        if file_type == 'PDF':\n            print(f\"  ‚ö†Ô∏è  Skipping validation for PDF file: {file_name}\")\n            validation_results.append({\n                'file_name': file_name,\n                'file_type': file_type,\n                'status': 'SKIPPED',\n                'rows': 0,\n                'columns': 0,\n                'error': 'PDF validation skipped due to known issues'\n            })\n            continue\n        \n        try:\n            # Try to read the parquet file with pandas\n            df = pd.read_parquet(output_path)\n            row_count = len(df)\n            col_count = len(df.columns)\n            \n            validation_results.append({\n                'file_name': file_name,\n                'file_type': file_type,\n                'status': 'VALID',\n                'rows': row_count,\n                'columns': col_count,\n                'error': None\n            })\n            \n            print(f\"  ‚úÖ {file_name}: {row_count} rows, {col_count} columns\")\n            \n            # Show a sample of data for small files\n            if row_count <= 10 and row_count > 0:\n                print(f\"     Columns: {list(df.columns)}\")\n                print(f\"     Sample data:\")\n                print(df.head(3).to_string())\n            \n        except Exception as e:\n            error_msg = str(e)\n            status = 'INVALID'\n                \n            validation_results.append({\n                'file_name': file_name,\n                'file_type': file_type,\n                'status': status,\n                'rows': 0,\n                'columns': 0,\n                'error': error_msg[:200] if len(error_msg) > 200 else error_msg\n            })\n            print(f\"  ‚ùå {file_name}: Validation failed - {error_msg[:100]}...\")\n    \n    if validation_results:\n        print(f\"\\nüìä Validation Summary:\")\n        df_validation = pd.DataFrame(validation_results)\n        print(df_validation.to_string(index=False))\n        \n        valid_count = len(df_validation[df_validation['status'] == 'VALID'])\n        skipped_count = len(df_validation[df_validation['status'] == 'SKIPPED'])\n        total_count = len(df_validation)\n        \n        print(f\"\\n‚úÖ Validation Results:\")\n        print(f\"   Valid files: {valid_count}/{total_count}\")\n        print(f\"   Skipped: {skipped_count}\")\n        \n        if valid_count == (total_count - skipped_count):\n            print(\"\\nüéâ ALL CONVERTED FILES (EXCEPT SKIPPED) ARE VALID PARQUET FILES!\")\n            print(\"‚úÖ PyForge CLI is working correctly in local environment\")\n            \n        # Show breakdown by file type\n        print(\"\\nüìä Validation by File Type:\")\n        type_summary = df_validation.groupby('file_type')['status'].value_counts().unstack(fill_value=0)\n        print(type_summary.to_string())\n\n# Run validation\nvalidate_converted_files()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Final Test Summary and Recommendations\n# =============================================================================\n# FINAL SUMMARY\n# =============================================================================\n\nprint(\"üéâ PYFORGE CLI LOCAL TESTING COMPLETED!\")\nprint(\"=\" * 70)\n\nprint(f\"üìä FINAL STATISTICS:\")\nprint(f\"   Environment: Local Python {sys.version.split()[0]}\")\nprint(f\"   PyForge Version: {summary_report['pyforge_version']}\")\nprint(f\"   Installation Source: Local wheel from dist/\")\nprint(f\"   Files Processed: {summary_report['total_files_tested']}\")\nprint(f\"   Success Rate: {summary_report['success_rate_percent']}%\")\nprint(f\"   Total Time: {summary_report['total_test_duration_seconds']}s\")\nprint(f\"   Data Processed: {summary_report['total_data_processed_mb']} MB\")\n\nprint(f\"\\nüìÅ LOCAL PATHS:\")\nprint(f\"   Source Data: {summary_report['sample_datasets_path']}\")\nprint(f\"   Converted Files: {summary_report['output_directory']}\")\nprint(f\"   Wheel Location: {summary_report['wheel_path']}\")\n\nprint(f\"\\nüöÄ FEATURES TESTED:\")\nprint(f\"   ‚úÖ Local Installation from Wheel\")\nprint(f\"   ‚úÖ Sample Dataset Installation\")\nprint(f\"   ‚úÖ File Discovery and Selection\")\nprint(f\"   ‚úÖ Directory Creation Before Conversion\")\nprint(f\"   ‚úÖ Multiple Format Support\")\nprint(f\"   ‚úÖ Error Handling and Timeout Management\")\n\nprint(f\"\\nüí° RECOMMENDATIONS:\")\nif summary_report['success_rate_percent'] >= 90:\n    print(\"   ‚úÖ Excellent performance! PyForge CLI works well locally.\")\n    print(\"   üöÄ Ready for production use.\")\nelif summary_report['success_rate_percent'] >= 75:\n    print(\"   ‚ö†Ô∏è  Good performance with some issues. Review failed conversions.\")\n    print(\"   üîç Consider fixing format-specific issues.\")\nelse:\n    print(\"   ‚ùå Performance needs attention. Check failed conversions and error messages.\")\n    print(\"   üõ†Ô∏è  Debug required before production deployment.\")\n\nprint(f\"\\nüìã KNOWN ISSUES:\")\nprint(f\"   ‚ö†Ô∏è  PDF conversions may produce invalid Parquet files\")\nprint(f\"   ‚ö†Ô∏è  Large files may require longer timeouts\")\nprint(f\"   ‚ö†Ô∏è  Some Excel files with spaces in names may fail\")\n\nprint(f\"\\nüéØ NEXT STEPS:\")\nprint(f\"   1. Review any failed conversions in detail\")\nprint(f\"   2. Test with larger datasets if needed\")\nprint(f\"   3. Deploy to Databricks for serverless testing\")\nprint(f\"   4. Address known PDF conversion issues\")\n\nprint(\"\\nüéâ Local testing completed successfully!\")\nprint(\"‚úÖ PyForge CLI is ready for use!\")"
  },
  {
   "cell_type": "code",
   "source": "%%sh\n# Optional: Cleanup Test Environment\n# =============================================================================\n# CLEANUP (OPTIONAL)\n# =============================================================================\n\n# Uncomment the following lines to clean up after testing:\n\n# echo \"üßπ Cleaning up test environment...\"\n\n# # Remove test output\n# # rm -rf test_output/\n\n# # Remove sample datasets\n# # rm -rf sample-datasets/\n\n# # Remove virtual environment\n# # rm -rf test_env/\n\n# echo \"‚úÖ Cleanup completed!\"\necho \"üí° To clean up, uncomment the cleanup commands above and run this cell\"",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Step 18: Validate converted files and show data samples\n",
    "cd ../../../  # Navigate to project root\n",
    "source test_env/bin/activate\n",
    "\n",
    "echo '=== CONVERSION VALIDATION ==='\n",
    "echo 'Generated files:'\n",
    "ls -la test_output/ || echo \"No test_output directory found\"\n",
    "\n",
    "echo ''\n",
    "echo '=== DATA VERIFICATION ==='\n",
    "if [ -d test_output ]; then\n",
    "    python3 -c \"\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_dir = 'test_output'\n",
    "success_count = 0\n",
    "total_files = 0\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith('.parquet'):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(output_dir, file)\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                print(f'‚úÖ {file}: {len(df)} rows, {len(df.columns)} columns')\n",
    "                if len(df) > 0:\n",
    "                    print(f'   Sample data: {list(df.columns[:3])}')\n",
    "                success_count += 1\n",
    "            except Exception as e:\n",
    "                print(f'‚ùå {file}: Failed to read - {str(e)}')\n",
    "\n",
    "    print(f'\\nüìä SUMMARY: {success_count}/{total_files} files successfully converted and readable')\n",
    "    if success_count == total_files and total_files > 0:\n",
    "        print('üéâ ALL CONVERSIONS SUCCESSFUL!')\n",
    "    elif success_count > 0:\n",
    "        print('‚ö†Ô∏è PARTIAL SUCCESS - some conversions worked')\n",
    "    else:\n",
    "        print('‚ùå NO SUCCESSFUL CONVERSIONS')\n",
    "else:\n",
    "    print('‚ùå No test_output directory found')\n",
    "\"\n",
    "else\n",
    "    echo \"‚ùå No test_output directory found\"\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}