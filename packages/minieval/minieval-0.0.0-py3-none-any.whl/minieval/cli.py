from rich.pretty import pprint

from minieval.backends.mock import MockLLM
from minieval.datatypes import Response
from minieval.formatters import CoT
from minieval.metrics import passAtK
from minieval.tasks.minerva import Math500


def main():
    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-4B")
    apply_chat_template = tokenizer.apply_chat_template

    task = Math500()
    metric = passAtK(ks=[1, 2, 4])
    formatter = CoT(template_func=apply_chat_template)
    llm = MockLLM()

    instances = task.requests
    requests = formatter.build_requests(instances)

    pprint(requests[0])

    sampling_params = {}
    generations: list[str] = llm.generate(requests, sampling_params=sampling_params)

    outputs: list[str] = task.extract_outputs(generations)

    responses = []
    for inst, req, out in zip(instances, requests, outputs):
        responses += [Response(input=inst, output=out)]

    metric.grade_responses(responses)

    dataset_scores = metric.compute_metric()

    print("Score:")
    print(dataset_scores)


if __name__ == "__main__":
    main()
