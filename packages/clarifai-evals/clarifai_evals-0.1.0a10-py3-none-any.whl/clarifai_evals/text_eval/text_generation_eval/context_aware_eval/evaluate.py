from typing import Dict, List, Tuple

from uptrain import EvalLLM, Evals

from clarifai_evals.constant import DEFAULT_TEXT_GEN_JUDGE_LLM_URL, DataTypes, VisualizationTypes
from clarifai_evals.text_eval.text_generation_eval.base_text_gen_eval import \
    BaseTextGenerationEvaluator


class TextGenContextRelevanceEvaluator(BaseTextGenerationEvaluator):
  """Grades how well the response generated by the LLM aligns with the provided ground truth using recall"""

  def __init__(
      self,
      clarifai_pat: str,
      clarifai_model_url: str = DEFAULT_TEXT_GEN_JUDGE_LLM_URL,
  ):
    """Initializes the evaluator object"""
    super().__init__(
        clarifai_pat=clarifai_pat,
        description=
        "Measures if the retrieved context has enough information to answer the question being asked",
        has_explanation=False,
        aggregation_type="mean",
        graph_type=VisualizationTypes.undefined,
        corrective_action=
        "1. Apply dataset/metadata filters to retrieve chunks from preferred/relevant space \n\
          2. Post process retrieved context (summarize) to remove unwanted data like headers etc \n\
          3. Try changing retrieval configurations (no of hits, hit rate) \n\
          4. Try increasing chunk overlap while uploading and try preprocessing steps",
        examples=None,
        library="uptrain",
        unit=DataTypes.float_type,
        clarifai_model_url=clarifai_model_url,
    )

  def evaluate(
      self,
      query_batch: List[str],
      predictions_batch: List[str] = None,
      ground_truth_batch: List[str] = None,
      context_batch: List[str] = None,
  ) -> Tuple[List, float, List]:
    """
        This method evaluates the context relevance of the retrieved context with the query provided
        Args:
            query_batch (List[str]): A list of queries
            context_batch (List[str]): A list of contexts
        Returns:
            List: Per sample score
            float: The average score
            List: The explanations for each sample
        """
    data = self.batch_to_uptrain_inputs(query_batch, context_batch=context_batch)
    eval_llm = EvalLLM(self.settings)
    results = eval_llm.evaluate(data=data, checks=[Evals.CONTEXT_RELEVANCE])
    metric_scores = []
    explanations = []
    for result in results:
      metric_scores.append(
          round(result["score_context_relevance"], 4) if result["score_context_relevance"] else 0)
      expln = result["explanation_context_relevance"]
      expln = expln["Reasoning"] if "Reasoning" in expln else expln
      explanations.append(expln)
    return metric_scores, round(sum(metric_scores) / len(metric_scores), 4), explanations

  def get_graph_data(self, ground_truth_batch: List[List], predictions_dict: Dict) -> None:
    return None


class TextGenContextUtilizationEvaluator(BaseTextGenerationEvaluator):
  """Measures if the generated response has sufficiently used the retrieved context to answer the question being asked."""

  def __init__(
      self,
      clarifai_pat: str,
      clarifai_model_url: str = DEFAULT_TEXT_GEN_JUDGE_LLM_URL,
  ):
    super().__init__(
        clarifai_pat=clarifai_pat,
        description=
        "Measures if the generated response has sufficiently used the retrieved context to answer the question being asked.",
        has_explanation=False,
        aggregation_type="mean",
        graph_type=VisualizationTypes.undefined,
        corrective_action=
        "1. Prompt the llm to break the context into smaller parts and ask the model to address each part using context \n\
          2. Explicitly state why the context is relevant and how it should be used in the response. \n\
          3. Ask the model to directly reference parts of the context in its answer \n",
        examples=None,
        library="uptrain",
        unit=DataTypes.float_type,
        clarifai_model_url=clarifai_model_url,
    )

  def evaluate(
      self,
      query_batch: List[str],
      predictions_batch: List[str],
      ground_truth_batch: List[str] = None,
      context_batch: List[str] = None,
  ) -> Tuple[List, float, List]:
    """
        This method evaluates the context utilization of the predicted text with the context provided
        Args:
            query_batch (List[str]): A list of queries
            predictions_batch (List[str]): A list of predictions
            context_batch (List[str]): A list of contexts
        Returns:
            List: Per sample score
            float: The average score
            List: The explanations for each sample
        """
    data = self.batch_to_uptrain_inputs(
        query_batch, predictions_batch, context_batch=context_batch)
    eval_llm = EvalLLM(self.settings)
    results = eval_llm.evaluate(data=data, checks=[Evals.RESPONSE_COMPLETENESS_WRT_CONTEXT])
    metric_scores = []
    explanations = []
    for result in results:
      metric_scores.append(
          round(result["score_response_completeness_wrt_context"], 4)
          if result["score_response_completeness_wrt_context"] else 0)
      expln = result["explanation_response_completeness_wrt_context"]
      expln = (expln["Reasoning"] if isinstance(expln, dict) and "Reasoning" in expln else expln)
      explanations.append(expln)
    return metric_scores, round(sum(metric_scores) / len(metric_scores), 4), explanations

  def get_graph_data(self, ground_truth_batch: List[List], predictions_dict: Dict) -> Dict:
    return None


class TextGenFactualAccuracyEvaluator(BaseTextGenerationEvaluator):
  """Measures the degree to which a claim made in the response is true according to the context provided."""

  def __init__(
      self,
      clarifai_pat: str,
      clarifai_model_url: str = DEFAULT_TEXT_GEN_JUDGE_LLM_URL,
  ):
    super().__init__(
        clarifai_pat=clarifai_pat,
        description=
        "Measures the degree to which a claim made in the response is true according to the context provided.",
        has_explanation=False,
        aggregation_type="mean",
        graph_type=VisualizationTypes.undefined,
        corrective_action="1. Ask the model to quote or refer directly to the context. \n\
          2. Ask the model to verify its answers with the context and provide evidence",
        examples=None,
        library="uptrain",
        unit=DataTypes.float_type,
        clarifai_model_url=clarifai_model_url,
    )

  def evaluate(
      self,
      query_batch: List[str],
      predictions_batch: List[str],
      ground_truth_batch: List[str] = None,
      context_batch: List[str] = None,
  ) -> Tuple[List, float, List]:
    """
        This method evaluates the factual accuracy of the predicted text with the context provided
        Args:
            query_batch (List[str]): A list of queries
            predictions_batch (List[str]): A list of predictions
            context_batch (List[str]): A list of contexts
        Returns:
            List: Per sample score
            float: The average score
            List: The explanations for each sample
        """
    data = self.batch_to_uptrain_inputs(
        query_batch, predictions_batch, context_batch=context_batch)
    eval_llm = EvalLLM(self.settings)
    results = eval_llm.evaluate(data=data, checks=[Evals.FACTUAL_ACCURACY])
    metric_scores = []
    explanations = []
    for result in results:
      metric_scores.append(
          round(result["score_factual_accuracy"], 4) if result["score_factual_accuracy"] else 0)
      expln = result["explanation_factual_accuracy"]
      expln = (expln["Reasoning"] if isinstance(expln, dict) and "Reasoning" in expln else expln)
      explanations.append(expln)
    return metric_scores, round(sum(metric_scores) / len(metric_scores), 4), explanations

  def get_graph_data(self, ground_truth_batch: List[List], predictions_dict: Dict) -> None:
    return None


class TextGenResponseConsistencyEvaluator(BaseTextGenerationEvaluator):
  """Grades how consistent the response is with the question asked as well as with the context provided"""

  def __init__(
      self,
      clarifai_pat: str,
      clarifai_model_url: str = DEFAULT_TEXT_GEN_JUDGE_LLM_URL,
  ):
    super().__init__(
        clarifai_pat=clarifai_pat,
        description=
        "Grades how consistent the response is with the question asked as well as with the context provided",
        has_explanation=False,
        aggregation_type="mean",
        graph_type=VisualizationTypes.undefined,
        corrective_action=None,
        examples=None,
        library="uptrain",
        unit=DataTypes.float_type,
        clarifai_model_url=clarifai_model_url,
    )

  def evaluate(
      self,
      query_batch: List[str],
      predictions_batch: List[str],
      ground_truth_batch: List[str] = None,
      context_batch: List[str] = None,
  ) -> Tuple[List, float, List]:
    """
        This method evaluates the consistency of the predicted text with the context provided
        Args:
            query_batch (List[str]): A list of queries
            predictions_batch (List[str]): A list of predictions
            context_batch (List[str]): A list of contexts
        Returns:
            List: Per sample score
            float: The average score
            List: The explanations for each sample
        """
    data = self.batch_to_uptrain_inputs(
        query_batch, predictions_batch, context_batch=context_batch)
    eval_llm = EvalLLM(self.settings)
    results = eval_llm.evaluate(data=data, checks=[Evals.RESPONSE_CONSISTENCY])
    metric_scores = []
    explanations = []
    for result in results:
      metric_scores.append(
          round(result["score_response_consistency"], 4)
          if result["score_response_consistency"] else 0)
      expln = result["explanation_response_consistency"]
      expln = (expln["Reasoning"] if isinstance(expln, dict) and "Reasoning" in expln else expln)
      explanations.append(expln)
    return metric_scores, round(sum(metric_scores) / len(metric_scores), 4), explanations

  def get_graph_data(self, ground_truth_batch: List[List], predictions_dict: Dict) -> None:
    return None
