# project_odysseus/knowledge_system.py
import os
import pickle
import re
import requests
from bs4 import BeautifulSoup
import spacy
import networkx as nx
from ddgs import DDGS

# 1. ê²€ìƒ‰ ë° ìŠ¤í¬ë ˆì´í•‘ ëª¨ë“ˆ
def search_all_engines(query: str, num_results: int = 1000) -> list[str]:
    """ì—¬ëŸ¬ ê²€ìƒ‰ ì—”ì§„ì„ í†µí•´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ì–´ì™€ URL ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print(f"ğŸ” '{query}'ì— ëŒ€í•œ ë‹¤ì¤‘ ì—”ì§„ ê²€ìƒ‰ ì¤‘...")
    urls = set()
    try:
        # DuckDuckGo API ì‚¬ìš©
        with DDGS() as ddgs:
            for r in ddgs.text(query, max_results=num_results):
                urls.add(r['href'])
        
        # (ì—¬ê¸°ì— Google, Bing ë“± ë‹¤ë¥¸ ê²€ìƒ‰ ì—”ì§„ API í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€ ê°€ëŠ¥)

    except Exception as e:
        print(f"Error during search: {e}")
        
    print(f"    -> {len(urls)}ê°œì˜ ê³ ìœ  URL ë°œê²¬.")
    return list(urls)

def scrape_url_content(url: str) -> str:
    """ë‹¨ì¼ URLì˜ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ìŠ¤í¬ë ˆì´í•‘í•©ë‹ˆë‹¤."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë°©í•´ ìš”ì†Œ(ë„¤ë¹„ê²Œì´ì…˜ ë°”, í‘¸í„° ë“±) ì œê±°
        for element in soup(['nav', 'footer', 'header', 'script', 'style']):
            element.decompose()
            
        text = soup.get_text(separator=' ', strip=True)
        return re.sub(r'\s+', ' ', text) # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¶•ì†Œ
    except requests.RequestException as e:
        print(f"    â—ï¸ URL ìŠ¤í¬ë ˆì´í•‘ ì‹¤íŒ¨: {url} ({e})")
        return ""

# 2. ì§€ì‹ ì¶”ì¶œ ë° ê·¸ë˜í”„ ì²˜ë¦¬ ëª¨ë“ˆ
class KnowledgeSystem:
    def __init__(self, graph_file_path="knowledge_graph.pkl"):
        self.graph_file_path = graph_file_path
        self.nlp = spacy.load("en_core_web_sm") # ì˜ì–´ ëª¨ë¸ ë¡œë“œ
        self.graph = self._load_graph()

    def _load_graph(self) -> nx.DiGraph:
        """ë¡œì»¬ íŒŒì¼ì—ì„œ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        if os.path.exists(self.graph_file_path):
            print(f"ğŸ’¾ ê¸°ì¡´ ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ ì¤‘: '{self.graph_file_path}'")
            with open(self.graph_file_path, 'rb') as f:
                return pickle.load(f)
        else:
            print("âœ¨ ìƒˆë¡œìš´ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±.")
            return nx.DiGraph()

    def _save_graph(self):
        """ì§€ì‹ ê·¸ë˜í”„ë¥¼ ë¡œì»¬ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        print(f"ğŸ’¾ ì§€ì‹ ê·¸ë˜í”„ ì €ì¥ ì¤‘: '{self.graph_file_path}'")
        with open(self.graph_file_path, 'wb') as f:
            pickle.dump(self.graph, f)

    def extract_knowledge_from_text(self, text: str, source_url: str) -> nx.DiGraph:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì—¬ ì„ì‹œ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        doc = self.nlp(text)
        temp_graph = nx.DiGraph()
        
        # (PERSON) --[VERB]--> (ORG) ê°™ì€ ë‹¨ìˆœ ê´€ê³„ ì¶”ì¶œ
        for token in doc:
            if token.pos_ == "VERB":
                subject, obj = None, None
                for child in token.children:
                    if child.dep_ == "nsubj" and child.ent_type_: # ì£¼ì–´
                        subject = child.text
                    if child.dep_ == "dobj" and child.ent_type_: # ëª©ì ì–´
                        obj = child.text
                
                if subject and obj:
                    temp_graph.add_node(subject, type=doc.ents[0].label_ if doc.ents else 'UNKNOWN')
                    temp_graph.add_node(obj, type=doc.ents[0].label_ if doc.ents else 'UNKNOWN')
                    temp_graph.add_edge(subject, obj, label=token.lemma_, source=source_url)
        return temp_graph

    def merge_and_resolve(self, new_graph: nx.DiGraph):
        """ì‹ ê·œ ê·¸ë˜í”„ë¥¼ ê¸°ì¡´ ê·¸ë˜í”„ì™€ ë³‘í•©í•˜ê³  ì¶©ëŒì„ í•´ê²°í•©ë‹ˆë‹¤."""
        conflicts = []
        for u, v, data in new_graph.edges(data=True):
            # u(ì£¼ì–´)ì™€ data['label'](ê´€ê³„)ì´ ë™ì¼í•œë° v(ëª©ì ì–´)ê°€ ë‹¤ë¥¸ ê²½ìš° ì¶©ëŒë¡œ ê°„ì£¼
            if self.graph.has_node(u):
                for _, existing_v, existing_data in self.graph.out_edges(u, data=True):
                    if existing_data['label'] == data['label'] and existing_v != v:
                        conflict = {
                            "node": u,
                            "relation": data['label'],
                            "old_value": existing_v,
                            "new_value": v,
                            "new_source": data['source']
                        }
                        conflicts.append(conflict)
                        print(f"ğŸ’¥ ì¶©ëŒ ë°œê²¬: {u} -> {data['label']} -> {existing_v} vs {v}")
        
        # ì¶©ëŒ í•´ê²° ë¡œì§
        for conflict in conflicts:
            self._resolve_conflict(conflict)
            
        # ì¶©ëŒ í•´ê²° í›„ ìµœì¢… ë³‘í•©
        self.graph.add_nodes_from(new_graph.nodes(data=True))
        self.graph.add_edges_from(new_graph.edges(data=True))

    def _resolve_conflict(self, conflict: dict):
        """íŠ¹ì • ì¶©ëŒì— ëŒ€í•´ ì¬ê²€ìƒ‰ì„ í†µí•´ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        node, rel = conflict['node'], conflict['relation']
        val1, val2 = conflict['old_value'], conflict['new_value']
        
        # ë§¤ìš° êµ¬ì²´ì ì¸ ì¿¼ë¦¬ ìƒì„±
        resolution_query = f'"{node}" {rel} "{val1}" or "{val2}"'
        print(f"âš”ï¸ ì¶©ëŒ í•´ê²° ì‹œì‘: '{resolution_query}'")
        
        urls = search_all_engines(resolution_query, num_results=3)
        resolution_text = " ".join([scrape_url_content(url) for url in urls])
        
        # ì¬ê²€ìƒ‰í•œ í…ìŠ¤íŠ¸ì—ì„œ ì–´ëŠ ìª½ì´ ë” ë§ì´ ì–¸ê¸‰ë˜ëŠ”ì§€ í™•ì¸
        count1 = resolution_text.lower().count(val1.lower())
        count2 = resolution_text.lower().count(val2.lower())

        if count1 > count2:
            print(f"    -> í•´ê²°: ê¸°ì¡´ ì •ë³´ '{val1}' ìœ ì§€.")
            # ê¸°ì¡´ ì •ë³´ ìœ ì§€, ì•„ë¬´ê²ƒë„ ì•ˆ í•¨
        elif count2 > count1:
            print(f"    -> í•´ê²°: ìƒˆë¡œìš´ ì •ë³´ '{val2}'ë¡œ ì—…ë°ì´íŠ¸.")
            # ê¸°ì¡´ì˜ ì˜ëª»ëœ ì—£ì§€ ì œê±°
            if self.graph.has_edge(node, val1):
                self.graph.remove_edge(node, val1)
            # ìƒˆë¡œìš´ ì—£ì§€ ì¶”ê°€ (ë³‘í•© ì‹œ ìë™ìœ¼ë¡œ ë¨)
        else:
            print("    -> í•´ê²° ì‹¤íŒ¨: ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ í˜„ ìƒíƒœ ìœ ì§€.")

    def answer_from_graph(self, query: str) -> str:
        """ê·¸ë˜í”„ì—ì„œ ì¿¼ë¦¬ ì£¼ì œì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤."""
        # ì¿¼ë¦¬ì—ì„œ í•µì‹¬ ê°œì²´ ì°¾ê¸°
        doc = self.nlp(query)
        subjects = [ent.text for ent in doc.ents]
        if not subjects:
            subjects = [token.text for token in doc if token.pos_ in ("NOUN", "PROPN")]
        
        if not subjects:
            return "ì§ˆë¬¸ì—ì„œ í•µì‹¬ ì£¼ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        main_subject = subjects[0]
        if not self.graph.has_node(main_subject):
            return f"'{main_subject}'ì— ëŒ€í•œ ì§€ì‹ì´ ì•„ì§ ì—†ìŠµë‹ˆë‹¤."

        print(f"ğŸ“– ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ë‹µë³€ ìƒì„± (ì£¼ì œ: {main_subject})")
        info = [f"'{main_subject}'ì— ëŒ€í•œ ì¢…í•© ì •ë³´:"]
        for u, v, data in self.graph.out_edges(main_subject, data=True):
            info.append(f" - {u} --[{data['label']}]--> {v}")
        for u, v, data in self.graph.in_edges(main_subject, data=True):
            info.append(f" - {u} --[{data['label']}]--> {v}")
            
        return "\n".join(info)

    def run_knowledge_cycle(self, query: str):
        """ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ì „ì²´ ì§€ì‹ ì²˜ë¦¬ ì‚¬ì´í´ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        # 1. ê²€ìƒ‰ ë° ìŠ¤í¬ë ˆì´í•‘
        urls = search_all_engines(query)
        scraped_contents = [scrape_url_content(url) for url in urls]
        
        # 2. ì‹ ê·œ ì§€ì‹ ì¶”ì¶œ
        print("\nğŸŒ€ ì‹ ê·œ ì§€ì‹ ì¶”ì¶œ ë° ë³‘í•© ì‹œì‘...")
        combined_new_graph = nx.DiGraph()
        for i, text in enumerate(scraped_contents):
            if text:
                source_url = urls[i]
                temp_graph = self.extract_knowledge_from_text(text, source_url)
                combined_new_graph.add_nodes_from(temp_graph.nodes(data=True))
                combined_new_graph.add_edges_from(temp_graph.edges(data=True))
        
        # 3. ë³‘í•© ë° ì¶©ëŒ í•´ê²°
        self.merge_and_resolve(combined_new_graph)
        
        # 4. ì €ì¥
        self._save_graph()
        
        # 5. ë‹µë³€
        print("\n" + "="*50)
        answer = self.answer_from_graph(query)
        print(answer)
        print("="*50 + "\n")


# 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§
if __name__ == "__main__":
    ks = KnowledgeSystem()
    
    # ì˜ˆì‹œ: 'Apple Inc'ì— ëŒ€í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ì§€ì‹ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•/ì—…ë°ì´íŠ¸
    # ì²˜ìŒ ì‹¤í–‰í•˜ë©´ ê·¸ë˜í”„ê°€ ìƒì„±ë˜ê³ , ë‘ ë²ˆì§¸ ì‹¤í–‰í•˜ë©´ ê¸°ì¡´ ê·¸ë˜í”„ì™€ ë³‘í•©/ì¶©ëŒ í•´ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤.
    try:
        ks.run_knowledge_cycle("Apple Inc. history founders")
    except Exception as e:
        print(f"An error occurred during the cycle: {e}")
