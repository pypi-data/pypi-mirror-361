from dataclasses import dataclass
from typing import Optional
from rich.pretty import pprint
from rich.console import Console


console = Console()

import sys
from omegaconf import OmegaConf

from minieval.backends import Backend, init_backend
from minieval.datatypes import LMOutput, Response, Task
from minieval.formatters import CoT, base_model_chat_template
from minieval.metrics import passAtK
from minieval.tasks.minerva import Math500
from minieval.tasks.gpqa import GPQA


@dataclass
class ModelConfig:
    name: str
    revision: Optional[str] = None


@dataclass
class TaskConfig:
    alias: str
    limit: Optional[int] = None


@dataclass
class Config:
    backend: Backend
    model: ModelConfig
    tasks: list[TaskConfig]
    

class EvalRunner():
    def __init__(self, config: Config):
        self.config = config

        self.llm = init_backend(
            backend_type=self.config.backend, 
            model_name=self.config.model.name
        )

        if self.config.backend == Backend.litellm:
            self.template_func = None
        else:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.config.model.name)
            if hasattr(tokenizer, 'apply_chat_template'):
                self.template_func = tokenizer.apply_chat_template
            else:
                self.template_func = base_model_chat_template

    
    def run(self, task: Task):
        console.rule(f"[bold red]{task.__class__.__name__}")

        metric = passAtK(ks=[1, 2, 4])
        formatter = CoT(template_func=self.template_func)

        instances = task.requests

        messages = formatter.build_messages(instances)

        if self.config.backend == Backend.litellm:
            requests = messages
        else:
            requests = formatter.build_requests(messages)

        sampling_params = {}

        generations: list[LMOutput] = self.llm.generate(requests, sampling_params=sampling_params)

        generations: list[LMOutput] = task.extract_outputs(generations)

        responses = []
        for inst, msg, req, gen in zip(instances, messages, requests, generations):
            responses += [Response(input=inst, messages=msg, request=req, output=gen)]

        pprint(responses[0])

        instance_scores = metric.grade_responses(responses)

        dataset_scores = metric.compute_metric()

        pprint(dataset_scores)

    
    def evaluate(self):
        for task_config in self.config.tasks:
            task = Math500()

            self.run(task)


def apply_overrides(config):
    base = OmegaConf.structured(config)
    
    # Get CLI args up to '--' if present, otherwise all args
    args = sys.argv[1:sys.argv.index("--")] if "--" in sys.argv else sys.argv[1:]
    cli_args = [arg.lstrip("-") for arg in args]
    
    # Merge overrides
    overrides = OmegaConf.from_cli(cli_args)
    merged = OmegaConf.merge(base, overrides)
    return OmegaConf.to_object(merged)


def main():
    config = Config(
        backend='mock',
        model = ModelConfig(
            name="Qwen/Qwen3-0.6B"
        ),
        tasks = [
            TaskConfig(
                alias="minerva_500:cot",
                limit=5
            ),
            TaskConfig(
                alias="gpqa:mc",
                limit=5
            )
        ]
    )

    config = apply_overrides(config)

    pprint(config, expand_all=True)

    runner = EvalRunner(config)

    runner.evaluate()


if __name__ == "__main__":
    main()
