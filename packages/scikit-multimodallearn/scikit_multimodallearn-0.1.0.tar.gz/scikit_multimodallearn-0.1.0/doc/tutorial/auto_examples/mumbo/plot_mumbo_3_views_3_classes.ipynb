{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n================================\nMumbo 3 views, 3 classes example\n================================\n\nIn this toy example, we generate data from three classes, split between three\ntwo-dimensional views.\n\nFor each view, the data are generated so that the points for two classes are\nwell seperated, while the points for the third class are not seperated with\nthe two other classes. That means that, taken separately, none of the single\nviews allows for a good classification of the data.\n\nNevertheless, the MuMBo algorithm take adavantage of the complementarity of\nthe views to rightly classify the points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom multimodal.boosting.mumbo import MumboClassifier\nfrom matplotlib import pyplot as plt\n\n\ndef generate_data(n_samples, lim):\n    \"\"\"Generate random data in a rectangle\"\"\"\n    lim = np.array(lim)\n    n_features = lim.shape[0]\n    data = np.random.random((n_samples, n_features))\n    data = (lim[:, 1]-lim[:, 0]) * data + lim[:, 0]\n    return data\n\n\nseed = 12\nnp.random.seed(seed)\n\nn_samples = 300\n\nview_0 = np.concatenate((generate_data(n_samples, [[0., 1.], [0., 1.]]),\n                         generate_data(n_samples, [[1., 2.], [0., 1.]]),\n                         generate_data(n_samples, [[0., 2.], [0., 1.]])))\n\nview_1 = np.concatenate((generate_data(n_samples, [[1., 2.], [0., 1.]]),\n                         generate_data(n_samples, [[0., 2.], [0., 1.]]),\n                         generate_data(n_samples, [[0., 1.], [0., 1.]])))\n\nview_2 = np.concatenate((generate_data(n_samples, [[0., 2.], [0., 1.]]),\n                         generate_data(n_samples, [[0., 1.], [0., 1.]]),\n                         generate_data(n_samples, [[1., 2.], [0., 1.]])))\n\nX = np.concatenate((view_0, view_1, view_2), axis=1)\n\ny = np.zeros(3*n_samples, dtype=np.int64)\ny[n_samples:2*n_samples] = 1\ny[2*n_samples:] = 2\n\nviews_ind = np.array([0, 2, 4, 6])\n\nn_estimators = 4\nclf = MumboClassifier(n_estimators=n_estimators)\nclf.fit(X, y, views_ind)\n\nprint('\\nAfter 4 iterations, the MuMBo classifier reaches exact '\n      'classification for the\\nlearning samples:')\nfor ind, score in enumerate(clf.staged_score(X, y)):\n    print('  - iteration {}, score: {}'.format(ind + 1, score))\n\nprint('\\nThe resulting MuMBo classifier uses four sub-classifiers that are '\n      'wheighted\\nusing the following weights:\\n'\n      '  estimator weights: {}'.format(clf.estimator_weights_))\n\nprint('\\nThe first sub-classifier uses the data of view 0 to compute '\n      'its classification\\nresults, the second and third sub-classifiers use '\n      'the data of view 1, while the\\nfourth one uses the data of '\n      'view 2:\\n'\n      '  best views: {}'. format(clf.best_views_))\n\nprint('\\nThe first figure displays the data, splitting the representation '\n      'between the\\nthree views.')\n\nstyles = ('.b', '.r', '.g')\nfig = plt.figure(figsize=(12., 11.))\nfig.suptitle('Representation of the data', size=16)\nfor ind_view in range(3):\n    ax = plt.subplot(3, 1, ind_view + 1)\n    ax.set_title('View {}'.format(ind_view))\n    ind_feature = ind_view * 2\n    for ind_class in range(3):\n        ind_samples = (y == ind_class)\n        ax.plot(X[ind_samples, ind_feature],\n                X[ind_samples, ind_feature + 1],\n                styles[ind_class],\n                label='Class {}'.format(ind_class))\n    ax.legend(loc='upper left', framealpha=0.9)\n\nprint('\\nThe second figure displays the classification results for the '\n      'sub-classifiers\\non the learning sample data.\\n')\n\nfig = plt.figure(figsize=(14., 11.))\nfig.suptitle('Classification results on the learning data for the '\n             'sub-classifiers', size=16)\nfor ind_estimator in range(n_estimators):\n    best_view = clf.best_views_[ind_estimator]\n    y_pred = clf.estimators_[ind_estimator].predict(\n        X[:, 2*best_view:2*best_view+2])\n    background_color = (1.0, 1.0, 0.9)\n    for ind_view in range(3):\n        ax = plt.subplot(3, 4, ind_estimator + 4*ind_view + 1)\n        if ind_view == best_view:\n            ax.set_facecolor(background_color)\n        ax.set_title(\n            'Sub-classifier {} - View {}'.format(ind_estimator, ind_view))\n        ind_feature = ind_view * 2\n        for ind_class in range(3):\n            ind_samples = (y_pred == ind_class)\n            ax.plot(X[ind_samples, ind_feature],\n                    X[ind_samples, ind_feature + 1],\n                    styles[ind_class],\n                    label='Class {}'.format(ind_class))\n        ax.legend(title='Predicted class:', loc='upper left', framealpha=0.9)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}