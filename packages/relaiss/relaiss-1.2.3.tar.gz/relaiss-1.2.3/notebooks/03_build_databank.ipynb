{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d12a07",
   "metadata": {},
   "source": [
    "# Building a New Dataset Bank for reLAISS\n",
    "### Authors: Evan Reynolds and Alex Gagliano\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to build a new dataset bank for reLAISS and use different feature combinations for nearest neighbor searches. The dataset bank is the foundation of reLAISS, containing all the features of transients that are used for similarity searches and anomaly detection.\n",
    "\n",
    "Building your own dataset bank allows you to incorporate new data, apply custom preprocessing steps, and tailor the feature set to your specific research needs.\n",
    "\n",
    "## Topics Covered\n",
    "1. Adding extinction corrections (A_V)\n",
    "2. Joining new lightcurve features\n",
    "3. Handling missing values\n",
    "4. Building the final dataset bank\n",
    "5. Using different feature combinations for nearest neighbor search:\n",
    "   - Lightcurve-only features\n",
    "   - Host-only features\n",
    "   - Custom feature subsets\n",
    "   - Feature weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1293e6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and create the required directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a68e81ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing reference bank...\n",
      "Loading preprocessed features from cache...\n",
      "Caching preprocessed reference bank...\n",
      "Building search index...\n",
      "Loading previously saved ANNOY index...\n",
      "Done!\n",
      "\n",
      "Loaded index with 25515 items\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from relaiss import constants\n",
    "import relaiss as rl\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('./figures', exist_ok=True)\n",
    "os.makedirs('./sfddata-master', exist_ok=True)\n",
    "\n",
    "# Define default feature sets from constants\n",
    "default_lc_features = constants.lc_features_const.copy()\n",
    "default_host_features = constants.host_features_const.copy()\n",
    "\n",
    "# Initialize client\n",
    "client = rl.ReLAISS()\n",
    "client.load_reference(\n",
    "    path_to_sfd_folder='./sfddata-master'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445b28b",
   "metadata": {},
   "source": [
    "## 1. Adding Extinction Corrections (A_V)\n",
    "\n",
    "The first step in building a dataset bank is to add extinction corrections to account for interstellar dust. The Schlegel, Finkbeiner & Davis (SFD) dust maps are used to estimate the amount of extinction.\n",
    "\n",
    "```python\n",
    "# Example code for adding extinction corrections\n",
    "from sfdmap2 import sfdmap\n",
    "\n",
    "df = pd.read_csv(\"../data/large_df_bank.csv\")\n",
    "m = sfdmap.SFDMap('../data/sfddata-master')\n",
    "RV = 3.1  # Standard value for Milky Way\n",
    "ebv = m.ebv(df['ra'].values, df['dec'].values)\n",
    "df['A_V'] = RV * ebv\n",
    "df.to_csv(\"../data/large_df_bank_wAV.csv\", index=False)\n",
    "```\n",
    "\n",
    "This adds the A_V (extinction in V-band) column to your dataset, which will be used later in the feature processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b533e",
   "metadata": {},
   "source": [
    "## 2. Joining New Lightcurve Features\n",
    "\n",
    "If you have additional features in a separate dataset, you can merge them with your existing bank:\n",
    "\n",
    "```python\n",
    "# Example code for joining features\n",
    "df_large = pd.read_csv(\"../data/large_df_bank_wAV.csv\")\n",
    "df_small = pd.read_csv(\"../data/small_df_bank_re_laiss.csv\")\n",
    "\n",
    "key = 'ztf_object_id'\n",
    "extra_features = [col for col in df_large.columns if col not in df_small.columns]\n",
    "\n",
    "merged_df = df_small.merge(df_large[[key] + extra_features], on=key, how='left')\n",
    "\n",
    "lc_feature_names = constants.lc_features_const.copy()\n",
    "host_feature_names = constants.host_features_const.copy()\n",
    "\n",
    "small_final_df = merged_df.replace([np.inf, -np.inf, -999], np.nan).dropna(subset=lc_feature_names + host_feature_names)\n",
    "\n",
    "small_final_df.to_csv(\"../data/small_hydrated_df_bank_re_laiss.csv\", index=False)\n",
    "```\n",
    "\n",
    "This merges additional features from a larger dataset into your working dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68126464",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values\n",
    "\n",
    "Missing values in the dataset can cause problems during analysis. reLAISS uses KNN imputation to fill in missing values:\n",
    "\n",
    "```python\n",
    "# Example code for handling missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "raw_host_feature_names = constants.raw_host_features_const.copy()\n",
    "raw_dataset_bank = pd.read_csv('../data/large_df_bank_wAV.csv')\n",
    "\n",
    "X = raw_dataset_bank[lc_feature_names + raw_host_feature_names]\n",
    "feat_imputer = KNNImputer(weights='distance').fit(X)\n",
    "imputed_filt_arr = feat_imputer.transform(X)\n",
    "\n",
    "imputed_df = pd.DataFrame(imputed_filt_arr, columns=lc_feature_names + raw_host_feature_names)\n",
    "imputed_df.index = raw_dataset_bank.index\n",
    "raw_dataset_bank[lc_feature_names + raw_host_feature_names] = imputed_df\n",
    "\n",
    "imputed_df_bank = raw_dataset_bank\n",
    "```\n",
    "\n",
    "KNN imputation works by finding the k-nearest neighbors in feature space for samples with missing values and using their values to fill in the gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b678ace",
   "metadata": {},
   "source": [
    "## 4. Building the Final Dataset Bank\n",
    "\n",
    "With all the preprocessing done, we can now build the final dataset bank using the `build_dataset_bank` function from reLAISS:\n",
    "\n",
    "```python\n",
    "# Example code for building the final dataset bank\n",
    "from relaiss.features import build_dataset_bank\n",
    "\n",
    "dataset_bank = build_dataset_bank(\n",
    "    raw_df_bank=imputed_df_bank,\n",
    "    av_in_raw_df_bank=True,\n",
    "    path_to_sfd_folder=\"../data/sfddata-master\",\n",
    "    building_entire_df_bank=True\n",
    ")\n",
    "\n",
    "# Clean and save final dataset\n",
    "final_dataset_bank = dataset_bank.replace(\n",
    "    [np.inf, -np.inf, -999], np.nan\n",
    ").dropna(subset=lc_feature_names + host_feature_names)\n",
    "\n",
    "final_dataset_bank.to_csv('../data/large_final_df_bank_new_lc_feats.csv', index=False)\n",
    "```\n",
    "\n",
    "This function applies additional processing to prepare the features for reLAISS, including normalization and other transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0970c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Building your own dataset bank and customizing feature combinations provides powerful flexibility for tailoring reLAISS to your specific research questions. By selecting different feature combinations and adjusting feature weights, you can explore various aspects of transient similarity and discover new insights about the transient population.\n",
    "\n",
    "The process involves several steps:\n",
    "1. Preprocessing your data with extinction corrections\n",
    "2. Merging additional features if needed\n",
    "3. Handling missing values through imputation\n",
    "4. Building the final dataset bank\n",
    "5. Customizing feature sets for different analysis goals\n",
    "\n",
    "These capabilities make reLAISS a versatile tool across a wide range of research applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
