"""
LLM-based Conversation Summarizer for Q Memory MCP Server
LLMì— ìœ„íƒí•˜ì—¬ ëŒ€í™” ë‚´ìš©ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ìš”ì•½
"""

import json
import os
import requests
from typing import List, Dict, Any, Optional


class LLMSummarizer:
    """LLM ê¸°ë°˜ ëŒ€í™” ìš”ì•½ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # OpenAI API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.api_url = "https://api.openai.com/v1/chat/completions"
        self.model = "gpt-3.5-turbo"
        self.max_retries = 2
        self.timeout = 30
    
    def _call_openai_api(self, prompt: str) -> Optional[str]:
        """OpenAI API í˜¸ì¶œ"""
        if not self.api_key:
            return None
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": [
                {
                    "role": "system", 
                    "content": "You are a helpful assistant that summarizes conversations accurately and concisely in Korean."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_tokens": 2000,
            "temperature": 0.3
        }
        
        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    self.api_url, 
                    headers=headers, 
                    json=data, 
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result['choices'][0]['message']['content'].strip()
                else:
                    print(f"OpenAI API error: {response.status_code} - {response.text}")
                    
            except Exception as e:
                print(f"OpenAI API call failed (attempt {attempt + 1}): {e}")
                if attempt == self.max_retries - 1:
                    return None
        
        return None
    
    def _format_conversation_for_llm(self, conv: Dict[str, Any]) -> str:
        """ëŒ€í™”ë¥¼ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ í¬ë§·íŒ…"""
        user_msg = conv.get('user_message', '')
        ai_msg = conv.get('ai_response', '')
        timestamp = conv.get('timestamp', '')[:16].replace('T', ' ')
        
        return f"[{timestamp}] User: {user_msg}\nAssistant: {ai_msg}"
    
    def _create_batch_summary_prompt(self, conversations: List[Dict], summary_level: str) -> str:
        """ë°°ì¹˜ ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        conv_texts = []
        for i, conv in enumerate(conversations, 1):
            conv_text = self._format_conversation_for_llm(conv)
            conv_texts.append(f"=== ëŒ€í™” {i} ===\n{conv_text}")
        
        all_conversations = "\n\n".join(conv_texts)
        
        if summary_level == "brief":
            instruction = """ê° ëŒ€í™”ë¥¼ 1-2ì¤„ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”. 
ì£¼ìš” ì£¼ì œì™€ ê²°ë¡ ë§Œ í¬í•¨í•˜ì„¸ìš”. í˜•ì‹: "1. ì£¼ì œ: ê²°ë¡ "
ì˜ˆì‹œ: "1. Python ë¦¬ìŠ¤íŠ¸ ì§ˆë¬¸: ê°€ë³€ê°ì²´ íŠ¹ì„±ê³¼ ì£¼ìš” ë©”ì„œë“œ ì„¤ëª…í•¨"
"""
        elif summary_level == "medium":
            instruction = """ê° ëŒ€í™”ë¥¼ 3-4ì¤„ë¡œ ìƒì„¸íˆ ìš”ì•½í•´ì£¼ì„¸ìš”.
ì£¼ìš” ë‚´ìš©, ì˜ˆì‹œ, ê²°ë¡ ì„ í¬í•¨í•˜ì„¸ìš”. í˜•ì‹: "1. ì£¼ì œ: ìƒì„¸ë‚´ìš©"
ì˜ˆì‹œ: "1. í´ë˜ìŠ¤ ìƒì† ì„¤ëª…: ë¶€ëª¨í´ë˜ìŠ¤ ì†ì„± ìƒì† ë°©ë²•, super() ì‚¬ìš©ë²•, ë‹¤ì¤‘ìƒì† ì£¼ì˜ì‚¬í•­ ë“±ì„ ì½”ë“œ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…í•¨"
"""
        else:
            return ""
        
        prompt = f"""ë‹¤ìŒ {len(conversations)}ê°œì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:

{instruction}

ëŒ€í™” ë‚´ìš©:
{all_conversations}

ìš”ì•½ ê²°ê³¼ (ë²ˆí˜¸ìˆœìœ¼ë¡œ):"""
        
        return prompt
    
    def _parse_batch_summary_response(self, response: str, original_count: int) -> List[str]:
        """ë°°ì¹˜ ìš”ì•½ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ê°œë³„ ìš”ì•½ìœ¼ë¡œ ë¶„ë¦¬"""
        if not response:
            return ["ìš”ì•½ ì‹¤íŒ¨"] * original_count
        
        lines = response.strip().split('\n')
        summaries = []
        
        for line in lines:
            line = line.strip()
            if line and (line.startswith(tuple('123456789')) or line.startswith('â€¢') or line.startswith('-')):
                # ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ í¬ì¸íŠ¸ ì œê±°
                clean_line = line
                for prefix in ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', 'â€¢', '-']:
                    if clean_line.startswith(prefix):
                        clean_line = clean_line[len(prefix):].strip()
                        break
                
                if clean_line:
                    summaries.append(clean_line)
        
        # ì›ë³¸ ê°œìˆ˜ì™€ ë§ì¶”ê¸°
        while len(summaries) < original_count:
            summaries.append("ìš”ì•½ ìƒì„±ë¨")
        
        return summaries[:original_count]
    
    def summarize_conversations_batch(self, conversations: List[Dict], summary_level: str) -> List[str]:
        """ëŒ€í™” ëª©ë¡ì„ ë°°ì¹˜ë¡œ ìš”ì•½"""
        if not conversations:
            return []
        
        # ë°°ì¹˜ í¬ê¸° ì œí•œ (í† í° ì œí•œ ê³ ë ¤)
        batch_size = 15 if summary_level == "brief" else 10
        all_summaries = []
        
        for i in range(0, len(conversations), batch_size):
            batch = conversations[i:i + batch_size]
            
            # LLM API í˜¸ì¶œ
            prompt = self._create_batch_summary_prompt(batch, summary_level)
            response = self._call_openai_api(prompt)
            
            if response:
                batch_summaries = self._parse_batch_summary_response(response, len(batch))
            else:
                # API ì‹¤íŒ¨ ì‹œ í´ë°±: ì›ë³¸ AI ì‘ë‹µì„ ì§§ê²Œ ìë¥´ê¸°
                batch_summaries = []
                for conv in batch:
                    ai_msg = conv.get('ai_response', '')
                    if summary_level == "brief":
                        fallback = ai_msg[:100] + "..." if len(ai_msg) > 100 else ai_msg
                    else:
                        fallback = ai_msg[:200] + "..." if len(ai_msg) > 200 else ai_msg
                    batch_summaries.append(fallback)
            
            all_summaries.extend(batch_summaries)
            
            # ì§„í–‰ìƒí™© í‘œì‹œ
            if len(conversations) > 20:
                progress = min(100, int((i + batch_size) / len(conversations) * 100))
                print(f"ğŸ“Š Summarizing conversations... {progress}%")
        
        return all_summaries
    
    def format_conversations_with_llm_summary(self, conversations: List[Dict[str, Any]]) -> str:
        """LLM ìš”ì•½ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ëª©ë¡ì„ í¬ë§·íŒ…"""
        if not conversations:
            return "â„¹ï¸ **No previous conversations found.**"
        
        total_count = len(conversations)
        
        print(f"ğŸ§  Processing {total_count} conversations with LLM summarization...")
        
        if total_count < 100:
            # 100ê°œ ë¯¸ë§Œ: ì§§ì€ ëŒ€í™”ëŠ” ê·¸ëŒ€ë¡œ, ê¸´ ëŒ€í™”ë§Œ ìš”ì•½
            formatted_text = f"ğŸ§  **All Conversations ({total_count} total):**\n\n"
            
            long_conversations = []
            long_indices = []
            
            # ê¸´ ëŒ€í™” ì‹ë³„
            for i, conv in enumerate(conversations):
                ai_msg = conv.get('ai_response', '')
                if len(ai_msg) > 300:  # 300ì ì´ìƒì¸ ê²½ìš°ë§Œ ìš”ì•½
                    long_conversations.append(conv)
                    long_indices.append(i)
            
            # ê¸´ ëŒ€í™”ë“¤ì„ ë°°ì¹˜ë¡œ ìš”ì•½
            if long_conversations:
                print(f"ğŸ“ Summarizing {len(long_conversations)} long conversations...")
                summaries = self.summarize_conversations_batch(long_conversations, "brief")
                summary_dict = dict(zip(long_indices, summaries))
            else:
                summary_dict = {}
            
            # ì „ì²´ ëŒ€í™” í¬ë§·íŒ…
            for i, conv in enumerate(conversations, 1):
                user_msg = conv.get('user_message', '')
                ai_msg = conv.get('ai_response', '')
                timestamp = conv.get('timestamp', '')[:16].replace('T', ' ')
                
                # ê¸´ ëŒ€í™”ëŠ” ìš”ì•½ ì‚¬ìš©, ì§§ì€ ëŒ€í™”ëŠ” ê·¸ëŒ€ë¡œ
                if i-1 in summary_dict:
                    ai_display = summary_dict[i-1]
                else:
                    ai_display = ai_msg
                
                formatted_text += f"**{i}. ({timestamp})**\n"
                formatted_text += f"**User**: {user_msg}\n"
                formatted_text += f"**Assistant**: {ai_display}\n\n"
            
            return formatted_text
        
        else:
            # 100ê°œ ì´ìƒ: í¼ì„¼íŠ¸ ê¸°ë°˜ ì²˜ë¦¬
            recent_5_percent = max(5, int(total_count * 0.05))
            middle_15_percent = max(10, int(total_count * 0.15))
            old_remainder = total_count - recent_5_percent - middle_15_percent
            
            formatted_text = f"ğŸ§  **Complete Conversation History ({total_count} total conversations):**\n\n"
            
            # ì˜¤ë˜ëœ ëŒ€í™”ë“¤ (80% - Brief Summary)
            if old_remainder > 0:
                print(f"ğŸ“š Summarizing {old_remainder} older conversations...")
                old_conversations = conversations[:old_remainder]
                old_summaries = self.summarize_conversations_batch(old_conversations, "brief")
                
                formatted_text += f"ğŸ“š **Earlier Context ({old_remainder} conversations - AI summarized):**\n"
                for i, (conv, summary) in enumerate(zip(old_conversations, old_summaries), 1):
                    user_msg = conv.get('user_message', '')
                    timestamp = conv.get('timestamp', '')[:16].replace('T', ' ')
                    
                    formatted_text += f"{i}. ({timestamp}) User: {user_msg}\n"
                    formatted_text += f"   AI: {summary}\n\n"
                
                formatted_text += "\n"
            
            # ì¤‘ê°„ ëŒ€í™”ë“¤ (15% - Medium Summary)  
            if middle_15_percent > 0:
                print(f"ğŸ’¬ Summarizing {middle_15_percent} recent conversations...")
                middle_start = old_remainder
                middle_end = old_remainder + middle_15_percent
                middle_conversations = conversations[middle_start:middle_end]
                middle_summaries = self.summarize_conversations_batch(middle_conversations, "medium")
                
                formatted_text += f"ğŸ’¬ **Recent Context ({middle_15_percent} conversations - AI detailed summaries):**\n"
                
                for i, (conv, summary) in enumerate(zip(middle_conversations, middle_summaries), middle_start + 1):
                    user_msg = conv.get('user_message', '')
                    timestamp = conv.get('timestamp', '')[:16].replace('T', ' ')
                    
                    formatted_text += f"**{i}. ({timestamp})**\n"
                    formatted_text += f"**User**: {user_msg}\n"
                    formatted_text += f"**Assistant**: {summary}\n\n"
                
                formatted_text += "\n"
            
            # ìµœê·¼ ëŒ€í™”ë“¤ (5% - Full Detail)
            recent_start = total_count - recent_5_percent
            recent_conversations = conversations[recent_start:]
            
            formatted_text += f"ğŸ”¥ **Latest Conversations ({recent_5_percent} conversations - full detail):**\n"
            
            for i, conv in enumerate(recent_conversations, recent_start + 1):
                user_msg = conv.get('user_message', '')
                ai_msg = conv.get('ai_response', '')
                timestamp = conv.get('timestamp', '')[:16].replace('T', ' ')
                
                formatted_text += f"**{i}. ({timestamp})**\n"
                formatted_text += f"**User**: {user_msg}\n"
                formatted_text += f"**Assistant**: {ai_msg}\n\n"
            
            formatted_text += "âœ… **I now remember all our previous conversations with AI-powered summaries and can continue where we left off.**\n"
            formatted_text += "ğŸ’¡ **You can reference any of the above topics, and I'll understand the context perfectly.**"
            
            return formatted_text


# ì „ì—­ LLM ìš”ì•½ê¸° ì¸ìŠ¤í„´ìŠ¤
_llm_summarizer = None


def get_llm_summarizer() -> LLMSummarizer:
    """LLM ìš”ì•½ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _llm_summarizer
    if _llm_summarizer is None:
        _llm_summarizer = LLMSummarizer()
    return _llm_summarizer
