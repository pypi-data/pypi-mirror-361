"""
Embeddingæ¨¡å—ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ç±»ä¼¼LlamaIndexçš„æœ¬åœ°åµŒå…¥å­˜å‚¨æ¨¡å—
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from coderepoindex.embeddings import (
    create_simple_rag_system,
    quick_index_and_search,
    SimpleTextSplitter,
    Node,
    Document,
    setup_logging
)
from coderepoindex.models import create_openai_providers


def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨æ–¹æ³•"""
    print("\n=== åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ ===")
    
    # åˆ›å»ºåµŒå…¥æä¾›å•†ï¼ˆæ³¨æ„ï¼šéœ€è¦é…ç½®æ­£ç¡®çš„APIå¯†é’¥ï¼‰
    try:
        provider = create_openai_providers(
            api_key="sk-test-key",  # è¯·æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
            base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
            embedding_model="text-embedding-v3"
        )
        embedding_provider = provider.embedding_provider
        print(f"âœ… åµŒå…¥æä¾›å•†åˆ›å»ºæˆåŠŸ: {embedding_provider.get_model_name()}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºåµŒå…¥æä¾›å•†å¤±è´¥: {e}")
        print("æ³¨æ„ï¼šè¯·é…ç½®æ­£ç¡®çš„APIå¯†é’¥")
        return
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    indexer, retriever = create_simple_rag_system(
        embedding_provider=embedding_provider,
        persist_dir="./demo_index",
        chunk_size=500,
        chunk_overlap=100
    )
    
    # å‡†å¤‡ç¤ºä¾‹æ–‡æ¡£
    documents = [
        {
            "text": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚è¿™åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚",
            "metadata": {"source": "AI_introduction", "topic": "artificial_intelligence"}
        },
        {
            "text": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºå¼€å‘èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–çš„ç®—æ³•ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚",
            "metadata": {"source": "ML_basics", "topic": "machine_learning"}
        },
        {
            "text": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å…·æœ‰å¤šä¸ªå±‚çš„ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘å¤„ç†ä¿¡æ¯çš„æ–¹å¼ã€‚å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚",
            "metadata": {"source": "DL_overview", "topic": "deep_learning"}
        },
        {
            "text": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
            "metadata": {"source": "NLP_intro", "topic": "natural_language_processing"}
        }
    ]
    
    # æ„å»ºç´¢å¼•
    print("ğŸ“– æ„å»ºç´¢å¼•...")
    indexer.build_index(documents)
    
    # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
    stats = indexer.get_statistics()
    print(f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡: {stats['documents']['total_nodes']} ä¸ªèŠ‚ç‚¹, {stats['vectors']['total_vectors']} ä¸ªå‘é‡")
    
    # è¿›è¡Œæ£€ç´¢
    query = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
    print(f"\nğŸ” æŸ¥è¯¢: {query}")
    
    results = retriever.retrieve(query, top_k=3)
    
    print("ğŸ“‹ æ£€ç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"\n{i}. ç›¸ä¼¼åº¦: {result['score']:.4f}")
        print(f"   æ¥æº: {result['metadata'].get('source', 'Unknown')}")
        print(f"   ä¸»é¢˜: {result['metadata'].get('topic', 'Unknown')}")
        print(f"   æ–‡æœ¬: {result['text'][:100]}...")


def demo_text_splitters():
    """æ¼”ç¤ºä¸åŒçš„æ–‡æœ¬åˆ†å—å™¨"""
    print("\n=== æ–‡æœ¬åˆ†å—å™¨ç¤ºä¾‹ ===")
    
    long_text = """
    äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚åœ¨è¿™ä¸ªæ—¶æœŸï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢å¦‚ä½•è®©æœºå™¨æ¨¡æ‹Ÿäººç±»çš„æ€ç»´è¿‡ç¨‹ã€‚

    æ—©æœŸçš„äººå·¥æ™ºèƒ½ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç¬¦å·æ¨ç†å’Œä¸“å®¶ç³»ç»Ÿä¸Šã€‚è¿™äº›ç³»ç»Ÿä½¿ç”¨é¢„å®šä¹‰çš„è§„åˆ™æ¥è§£å†³ç‰¹å®šé¢†åŸŸçš„é—®é¢˜ã€‚

    éšç€è®¡ç®—èƒ½åŠ›çš„æå‡å’Œå¤§æ•°æ®çš„å‡ºç°ï¼Œæœºå™¨å­¦ä¹ æ–¹æ³•å¼€å§‹å…´èµ·ã€‚ç‰¹åˆ«æ˜¯ç¥ç»ç½‘ç»œçš„å‘å±•ï¼Œä¸ºäººå·¥æ™ºèƒ½å¸¦æ¥äº†æ–°çš„çªç ´ã€‚

    ä»Šå¤©ï¼Œæ·±åº¦å­¦ä¹ å·²ç»æˆä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸»æµæ–¹æ³•ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚
    """
    
    # ç®€å•æ–‡æœ¬åˆ†å—å™¨
    simple_splitter = SimpleTextSplitter(chunk_size=200, chunk_overlap=50)
    simple_nodes = simple_splitter.split_text(long_text, {"source": "AI_history"})
    
    print(f"ğŸ“„ ç®€å•åˆ†å—å™¨ç”Ÿæˆ {len(simple_nodes)} ä¸ªèŠ‚ç‚¹:")
    for i, node in enumerate(simple_nodes):
        print(f"  èŠ‚ç‚¹{i+1}: {len(node.text)} å­—ç¬¦")
        print(f"    å†…å®¹é¢„è§ˆ: {node.text[:80].strip()}...")
    
    # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
    doc = Document.from_text(long_text, metadata={"title": "AIå‘å±•å†ç¨‹", "type": "article"})
    print(f"\nğŸ“œ æ–‡æ¡£å¯¹è±¡: ID={doc.get_doc_id()[:8]}..., é•¿åº¦={len(doc)} å­—ç¬¦")


def demo_node_operations():
    """æ¼”ç¤ºèŠ‚ç‚¹æ“ä½œ"""
    print("\n=== èŠ‚ç‚¹æ“ä½œç¤ºä¾‹ ===")
    
    # åˆ›å»ºèŠ‚ç‚¹
    node = Node.from_text(
        "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬èŠ‚ç‚¹ï¼Œç”¨äºæ¼”ç¤ºNodeç±»çš„åŠŸèƒ½ã€‚",
        metadata={"type": "example", "category": "demo"}
    )
    
    print(f"ğŸ“ åˆ›å»ºèŠ‚ç‚¹: {node}")
    print(f"   ID: {node.node_id}")
    print(f"   æ–‡æœ¬é•¿åº¦: {len(node)}")
    
    # æ·»åŠ å…ƒæ•°æ®
    node.add_metadata("priority", "high")
    node.add_metadata("tags", ["demo", "example", "test"])
    
    print(f"ğŸ“‹ æ›´æ–°åçš„å…ƒæ•°æ®: {node.metadata}")
    
    # æ·»åŠ å…³ç³»
    node.add_relationship("parent", "parent-node-id")
    node.add_relationship("next", "next-node-id")
    
    print(f"ğŸ”— èŠ‚ç‚¹å…³ç³»: {node.relationships}")
    
    # è½¬æ¢ä¸ºå­—å…¸
    node_dict = node.to_dict()
    print(f"ğŸ“„ èŠ‚ç‚¹å­—å…¸æ ¼å¼: {list(node_dict.keys())}")
    
    # ä»å­—å…¸æ¢å¤
    restored_node = Node.from_dict(node_dict)
    print(f"ğŸ”„ æ¢å¤çš„èŠ‚ç‚¹: {restored_node.node_id == node.node_id}")


def demo_quick_search():
    """æ¼”ç¤ºå¿«é€Ÿç´¢å¼•å’Œæœç´¢"""
    print("\n=== å¿«é€Ÿç´¢å¼•å’Œæœç´¢ç¤ºä¾‹ ===")
    
    # åˆ›å»ºåµŒå…¥æä¾›å•†ï¼ˆè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿçš„æ–¹å¼ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®çš„APIå¯†é’¥ï¼‰
    try:
        provider = create_openai_providers(
            api_key="sk-test-key",
            base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
            embedding_model="text-embedding-v3"
        )
        embedding_provider = provider.embedding_provider
    except Exception as e:
        print(f"âŒ æ— æ³•åˆ›å»ºåµŒå…¥æä¾›å•†: {e}")
        print("è·³è¿‡å¿«é€Ÿæœç´¢ç¤ºä¾‹ï¼ˆéœ€è¦çœŸå®çš„APIå¯†é’¥ï¼‰")
        return
    
    # å‡†å¤‡æ–‡æ¡£
    tech_documents = [
        {"text": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´å’Œå¯è¯»æ€§è€Œé—»åã€‚", "metadata": {"topic": "python"}},
        {"text": "JavaScriptæ˜¯ä¸€ç§åŠ¨æ€ç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦ç”¨äºWebå¼€å‘ã€‚", "metadata": {"topic": "javascript"}},
        {"text": "Reactæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºç”¨æˆ·ç•Œé¢çš„JavaScriptåº“ã€‚", "metadata": {"topic": "react"}},
        {"text": "Dockeræ˜¯ä¸€ä¸ªå®¹å™¨åŒ–å¹³å°ï¼Œç”¨äºåº”ç”¨ç¨‹åºçš„æ‰“åŒ…å’Œéƒ¨ç½²ã€‚", "metadata": {"topic": "docker"}},
    ]
    
    # å¿«é€Ÿæœç´¢
    query = "ç¼–ç¨‹è¯­è¨€"
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    
    results = quick_index_and_search(
        documents=tech_documents,
        query=query,
        embedding_provider=embedding_provider,
        top_k=2
    )
    
    print("ğŸ“‹ å¿«é€Ÿæœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"{i}. ç›¸ä¼¼åº¦: {result['score']:.4f}")
        print(f"   ä¸»é¢˜: {result['metadata'].get('topic', 'Unknown')}")
        print(f"   å†…å®¹: {result['text']}")


def demo_persistence():
    """æ¼”ç¤ºæŒä¹…åŒ–åŠŸèƒ½"""
    print("\n=== æŒä¹…åŒ–åŠŸèƒ½ç¤ºä¾‹ ===")
    
    from coderepoindex.embeddings import create_indexer, create_retriever
    
    # åˆ›å»ºå¸¦æŒä¹…åŒ–çš„ç´¢å¼•æ„å»ºå™¨
    persist_dir = "./persistence_demo"
    
    try:
        provider = create_openai_providers(
            api_key="sk-test-key",
            base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
            embedding_model="text-embedding-v3"
        )
        embedding_provider = provider.embedding_provider
    except Exception as e:
        print(f"âŒ æ— æ³•åˆ›å»ºåµŒå…¥æä¾›å•†: {e}")
        print("è·³è¿‡æŒä¹…åŒ–ç¤ºä¾‹ï¼ˆéœ€è¦çœŸå®çš„APIå¯†é’¥ï¼‰")
        return
    
    indexer = create_indexer(
        embedding_provider=embedding_provider,
        persist_dir=persist_dir
    )
    
    # æ·»åŠ ä¸€äº›æ–‡æ¡£
    sample_docs = [
        {"text": "æŒä¹…åŒ–æ˜¯æ•°æ®å­˜å‚¨çš„é‡è¦ç‰¹æ€§ã€‚", "metadata": {"type": "definition"}},
        {"text": "ç´¢å¼•å¯ä»¥åŠ é€Ÿæ•°æ®æ£€ç´¢è¿‡ç¨‹ã€‚", "metadata": {"type": "concept"}},
    ]
    
    print("ğŸ’¾ æ„å»ºå¹¶æŒä¹…åŒ–ç´¢å¼•...")
    indexer.build_index(sample_docs)
    
    print(f"ğŸ“ ç´¢å¼•å·²ä¿å­˜åˆ°: {persist_dir}")
    print(f"ğŸ“Š æ–‡æ¡£å­˜å‚¨æ–‡ä»¶: {os.path.exists(os.path.join(persist_dir, 'document_store.json'))}")
    print(f"ğŸ“Š å‘é‡å­˜å‚¨æ–‡ä»¶: {os.path.exists(os.path.join(persist_dir, 'vector_store.json'))}")
    
    # åˆ›å»ºæ–°çš„æ£€ç´¢å™¨æ¥åŠ è½½æŒä¹…åŒ–çš„ç´¢å¼•
    new_retriever = create_retriever(
        embedding_provider=embedding_provider,
        persist_dir=persist_dir
    )
    
    print("ğŸ”„ ä»æŒä¹…åŒ–æ–‡ä»¶åŠ è½½ç´¢å¼•...")
    stats = new_retriever.get_statistics()
    print(f"ğŸ“ˆ åŠ è½½çš„ç»Ÿè®¡ä¿¡æ¯: {stats['documents']['total_nodes']} ä¸ªèŠ‚ç‚¹")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Embeddingæ¨¡å—å®Œæ•´ç¤ºä¾‹")
    print("=" * 50)
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    setup_logging("INFO")
    
    # è¿è¡Œå„ç§æ¼”ç¤º
    demo_node_operations()
    demo_text_splitters()
    
    # éœ€è¦çœŸå®APIå¯†é’¥çš„æ¼”ç¤º
    print("\n" + "=" * 50)
    print("âš ï¸  ä»¥ä¸‹æ¼”ç¤ºéœ€è¦é…ç½®çœŸå®çš„APIå¯†é’¥æ‰èƒ½è¿è¡Œ")
    print("è¯·åœ¨ä»£ç ä¸­æ›¿æ¢ 'sk-test-key' ä¸ºæ‚¨çš„å®é™…APIå¯†é’¥")
    print("=" * 50)
    
    demo_basic_usage()
    demo_quick_search()
    demo_persistence()
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“š ä½¿ç”¨æç¤º:")
    print("1. é…ç½®æ­£ç¡®çš„APIå¯†é’¥å’Œbase_url")
    print("2. æ ¹æ®éœ€è¦é€‰æ‹©åˆé€‚çš„chunk_sizeå’Œchunk_overlap")
    print("3. ä½¿ç”¨æŒä¹…åŒ–åŠŸèƒ½ä¿å­˜ç´¢å¼•ä»¥ä¾¿åç»­ä½¿ç”¨")
    print("4. åˆ©ç”¨å…ƒæ•°æ®è¿‡æ»¤åŠŸèƒ½è¿›è¡Œç²¾ç¡®æ£€ç´¢")


if __name__ == "__main__":
    main() 