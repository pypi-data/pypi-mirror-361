# Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.
# Licensed under the MIT License.

import onnx
import onnx.helper
import onnx.numpy_helper
from onnx import TensorProto, helper
import os
from pathlib import Path
import zlib
import fnmatch


###############################################################################
# Silu
###############################################################################
def gen_silu(overlay, m, k):
    if overlay == "2x4x4":
        mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
        graph = mlir_template.format(m=m, k=k)

        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "silu" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
"""
        opargs = opargs_template.format(m=m, k=k)

        ###########################################################################
        top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_silu.h"
class FlexMLGraph : public adf::graph
{
public:
    SiLUGraph SiluLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
        top_cpp = top_cpp_template
    else:
        size_w = n
        size_h = m

        input_tensor = helper.make_tensor_value_info(
            "input", TensorProto.FLOAT, [1, size_h, size_w]
        )
        output_tensor = helper.make_tensor_value_info(
            "output", TensorProto.FLOAT, [1, size_h, size_w]
        )

        sigmoid_node = helper.make_node(
            "Sigmoid", inputs=["input"], outputs=["sigmoid_output"]
        )

        mul_node = helper.make_node(
            "Mul", inputs=["input", "sigmoid_output"], outputs=["output"]
        )

        graph = helper.make_graph(
            nodes=[sigmoid_node, mul_node],
            name="SiLU-Graph",
            inputs=[input_tensor],
            outputs=[output_tensor],
        )

        opargs = "Placeholder"
        top_cpp = "Placeholder"

    return graph, opargs, top_cpp


def gen_mul_onnx(m, n):
    size_w = n
    size_h = m

    input_1 = helper.make_tensor_value_info(
        "input_1", TensorProto.FLOAT, [1, 1, size_h, size_w]
    )
    input_2 = helper.make_tensor_value_info(
        "input_2", TensorProto.FLOAT, [1, 1, size_h, size_w]
    )

    output = helper.make_tensor_value_info(
        "output", TensorProto.FLOAT, [1, 1, size_h, size_w]
    )

    Mul_node = helper.make_node(
        "Mul", ["input_1", "input_2"], ["output"], name="Mul_node"
    )

    graph = helper.make_graph(
        nodes=[Mul_node], name="MulModel", inputs=[input_1, input_2], outputs=[output]
    )
    return graph


###############################################################################
# Mul/OgaMulBf16TGTiling
###############################################################################
def gen_mul(overlay, m, k):
    if overlay == "2x4x4":
        mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
        graph = mlir_template.format(m=m, k=k)

        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "mul" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
"""
        opargs = opargs_template.format(m=m, k=k)

        ###########################################################################
        top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_mulbf16.h"
class FlexMLGraph : public adf::graph
{
public:
    MulGraph bmmLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
        top_cpp = top_cpp_template
    else:
        graph = "Placeholder"
        opargs = "Placeholder"
        top_cpp = "Placeholder"

    return graph, opargs, top_cpp


def gen_add_onnx(m, n):
    size_h = m
    size_w = n
    # Create input tensors
    input_1 = helper.make_tensor_value_info(
        "input_1", TensorProto.FLOAT, [1, 1, size_h, size_w]
    )
    input_2 = helper.make_tensor_value_info(
        "input_2", TensorProto.FLOAT, [1, 1, size_h, size_w]
    )

    # Create output tensor (same shape as input)
    output = helper.make_tensor_value_info(
        "output", TensorProto.FLOAT, [1, 1, size_h, n]
    )

    # Create Add node
    add_node = helper.make_node(
        "Add", ["input_1", "input_2"], ["output"], name="add_node"
    )

    # Create graph
    graph = helper.make_graph(
        nodes=[add_node], name="AddModel", inputs=[input_1, input_2], outputs=[output]
    )

    return graph


###############################################################################
# add/OgaAddBf16TGTiling
###############################################################################
def gen_add(overlay, m, k):
    if overlay == "2x4x4":
        mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
        graph = mlir_template.format(m=m, k=k)

        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "add" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
"""
        opargs = opargs_template.format(m=m, k=k)

        ###########################################################################
        top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_add.h"
class FlexMLGraph : public adf::graph
{
public:
    Add1dGraph AddLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
        top_cpp = top_cpp_template
    else:
        graph = "Placeholder"
        opargs = "Placeholder"
        top_cpp = "Placeholder"

    return graph, opargs, top_cpp


def gen_rmsnorm_onnx(m, n):
    size_w = n
    size_m = m
    input_data = helper.make_tensor_value_info(
        "input", TensorProto.FLOAT, [1, 1, size_m, size_w]
    )

    output = helper.make_tensor_value_info(
        "out0", TensorProto.FLOAT, [1, 1, size_m, size_w]
    )

    scale = helper.make_node(
        "Constant",
        inputs=[],
        outputs=["scale"],
        name="scale",
        value=helper.make_tensor("scale", TensorProto.FLOAT, [size_w], [2.0] * size_w),
    )

    eps = helper.make_node(
        "Constant",
        inputs=[],
        outputs=["eps"],
        name="eps",
        value=helper.make_tensor("eps", TensorProto.FLOAT, [], [1.23]),
    )

    dd_node = helper.make_node(
        "Mul", inputs=["input", "input"], outputs=["dd"], name="Mul2"
    )

    var_node = helper.make_node(
        "ReduceMean",
        inputs=["dd"],
        outputs=["var"],
        name="ReduceMean3",
        axes=[-1],
        keepdims=1,
    )

    var_eps_node = helper.make_node(
        "Add", inputs=["eps", "var"], outputs=["varEps"], name="Add4"
    )

    std_dev_node = helper.make_node(
        "Sqrt", inputs=["varEps"], outputs=["stdDev"], name="Sqrt5"
    )

    norm_node = helper.make_node(
        "Div", inputs=["input", "stdDev"], outputs=["norm"], name="Div6"
    )

    out_node = helper.make_node(
        "Mul", inputs=["scale", "norm"], outputs=["out0"], name="Mul7"
    )

    graph = helper.make_graph(
        nodes=[
            scale,
            eps,
            dd_node,
            var_node,
            var_eps_node,
            std_dev_node,
            norm_node,
            out_node,
        ],
        name="RMSNormGraphUpdated",
        inputs=[input_data],
        outputs=[output],
    )

    return graph


###############################################################################
# rmsnorm/OgaRMSNormBf16TGTiling
###############################################################################
def gen_rmsnorm(overlay, m, k):
    if overlay == "2x4x4":
        mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
        graph = mlir_template.format(m=m, k=k)

        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "RMSNorm" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
"""
        opargs = opargs_template.format(m=m, k=k)

        ###########################################################################
        top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_rmsnorm.h"
class FlexMLGraph : public adf::graph
{
public:
    RMSNormGraph RMSNormLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
        top_cpp = top_cpp_template
    else:
        graph = "Placeholder"
        opargs = "Placeholder"
        top_cpp = "Placeholder"

    return graph, opargs, top_cpp


###############################################################################
#


def gen_bmm1(overlay, b0, b1, m, n, k):
    mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
    graph = mlir_template.format(m=m, k=k, n=n)
    if overlay == "4x4":
        opargs_template = """Placeholder
"""
    else:
        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "bmm1" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.B0.size = {b0} : ui32
config.generator.operator_args.iteration_domains.B1.size = {b1} : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
config.generator.operator_args.iteration_domains.N.size = {n} : ui32
"""
    opargs = opargs_template.format(b0=b0, b1=b1, m=m, k=k, n=n)

    ###########################################################################
    top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_bmmbf16.h"
class FlexMLGraph : public adf::graph
{
public:
    BMMGraph bmmLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
    top_cpp = top_cpp_template

    return graph, opargs, top_cpp


def gen_bmm2(overlay, b0, b1, m, n, k):
    mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
    graph = mlir_template.format(m=m, k=k, n=n)
    if overlay == "4x4":
        opargs_template = """Placeholder
"""
    else:
        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "bmm2" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.B0.size = {b0} : ui32
config.generator.operator_args.iteration_domains.B1.size = {b1} : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
config.generator.operator_args.iteration_domains.N.size = {n} : ui32
"""
    opargs = opargs_template.format(b0=b0, b1=b1, m=m, k=k, n=n)

    ###########################################################################
    top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"
#include "overlay/single_layer_2x4x4_overlay_bmmbf16_transpose.h"
class FlexMLGraph : public adf::graph
{
public:
    BMMTransposeGraph bmmLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
    top_cpp = top_cpp_template

    return graph, opargs, top_cpp


def gen_matmul(b, m, n, k):
    # BMM
    batch_size = b
    m_size = m
    k_size = k
    n_size = n
    input_shape_1 = [batch_size, m_size, k_size]
    input_shape_2 = [batch_size, k_size, n_size]
    output_shape = [batch_size, m_size, n_size]

    input_tensor_1 = helper.make_tensor_value_info(
        "input_1", TensorProto.FLOAT, input_shape_1
    )
    input_tensor_2 = helper.make_tensor_value_info(
        "input_2", TensorProto.FLOAT, input_shape_2
    )
    output_tensor = helper.make_tensor_value_info(
        "output", TensorProto.FLOAT, output_shape
    )

    matmul_node = helper.make_node(
        "MatMul",
        inputs=["input_1", "input_2"],
        outputs=["output"],
    )

    graph = helper.make_graph(
        [matmul_node], "matmul_graph", [input_tensor_1, input_tensor_2], [output_tensor]
    )
    return graph


###############################################################################
#
###############################################################################
def gen_masked_softmax_onnx(m, n, h):
    size_h = m
    size_w = n
    size_head = h

    # Create input tensors
    input_data = helper.make_tensor_value_info(
        "input_data", TensorProto.FLOAT, [1, size_head, size_h, size_w]
    )  # [batch_size, seq_length, dim]
    mask = helper.make_tensor_value_info(
        "mask", TensorProto.FLOAT, [1, size_head, size_h, size_w]
    )  # Same shape as input_data

    # Create output tensor (same shape as input)
    output_softmax = helper.make_tensor_value_info(
        "output_softmax", TensorProto.FLOAT, [1, size_head, size_h, size_w]
    )

    # Create Add node
    add_node = helper.make_node(
        "Add", inputs=["input_data", "mask"], outputs=["masked_data"], name="AddNode"
    )

    # Create node
    softmax_node = helper.make_node(
        "Softmax",
        inputs=["masked_data"],
        outputs=["output_softmax"],
        axis=-1,
        name="SoftmaxNode",
    )

    # Create graph
    graph = helper.make_graph(
        nodes=[add_node, softmax_node],
        name="MaskedSoftmaxGraph3D",
        inputs=[input_data, mask],
        outputs=[output_softmax],
    )

    return graph


###############################################################################
# masked_softmax/OgaSoftmaxBf16TGTiling
###############################################################################
def gen_masked_softmax(overlay, b, m, k, hs):
    if overlay == "2x4x4":
        mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
        graph = mlir_template.format(m=m, k=k)

        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "Softmax" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.B.size = {b} : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
config.generator.operator_args.iteration_domains.HEADSIZE.size = {head_size} : ui32
"""
        opargs = opargs_template.format(b=b, m=m, k=k, head_size=hs)

        # ----------------------------------------------------------------------
        top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_softmax.h"
class FlexMLGraph : public adf::graph
{
public:
    SoftmaxGraph SoftmaxLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
        top_cpp = top_cpp_template
    else:
        graph = "Placeholder"
        opargs = "Placeholder"
        top_cpp = "Placeholder"

    return graph, opargs, top_cpp


###############################################################################
#
###############################################################################
def gen_gemm_w4abf16_ukernel(output_path, m, n, k, g):
    bits = 4
    mlir_template = """module {{
  func.func @GemmW4ABf16(%arg0: tensor<1x{m}x{k}xbf16>) -> tensor<1x{m}x{n}xbf16> {{
    %0 = tensor.empty() : tensor<{m}x{n}xbf16>
    %2 = xten_nn.subgraph (%arg0 : tensor<1x{m}x{k}xbf16>)
			   attributes {{LayerName = "",
			               OperatorArgs = {{config.block_size = {g} : ui32,
                               config.bits = {bits} : ui32,
                               iteration_domains.M.size = {m} : ui32,
                               iteration_domains.K.size = {k} : ui32,
                               iteration_domains.N.size = {n} : ui32}},
						       OperatorName = "GemmW4ABf16TGTiling",
						       OutputName = "",
						       Reason = "Microkernel"}} -> tensor<1x{m}x{n}xbf16>
    return %2 : tensor<1x{m}x{n}xbf16>
  }}
}}
"""
    graph = mlir_template.format(m=m, n=n, k=k, g=g, bits=bits)

    #########################################################
    # Generate opargs for microkernel-tile
    #########################################################
    opargs_template = """ml_adf_header_file = "ropebf16TestOutput" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
iteration_domains.M.size = {m} : ui32
iteration_domains.K.size = {k} : ui32
"""
    opargs = opargs_template.format(m=m, k=k)

    #########################################################
    # Generate top MLADF wrapper
    #########################################################
    top_cpp_content = """#include "multi_layer_overlay_gemm_w4abf16.h"
#include "tiling.h"
#define DTYPE bfloat16
using graph_t = mllib_graphs::multi_layer_overlay_gemm_w4abf16<DTYPE, 4, 4, 0xb580, 1408, 1792>;
class test_graph : public adf::graph {
public:
  using ddr_t = adf::external_buffer<DTYPE>;
  ddr_t ifm_ddr;
  ddr_t wts_ddr;
  ddr_t ofm_ddr;
  test_graph(struct mllib_graphs::Gemm_w4abf16_param_t& gemm_int4_param_set) {
    using namespace adf;
    ifm_ddr = ddr_t::create(gemm_int4_param_set.tiling_params[0].ifm_ddr_size, 0, 1); 
    wts_ddr = ddr_t::create(gemm_int4_param_set.tiling_params[0].wts_ddr_size, 0, 1);
    ofm_ddr = ddr_t::create(gemm_int4_param_set.tiling_params[0].ofm_ddr_size, 1, 0);
    graph_t tg(ifm_ddr, wts_ddr, ofm_ddr, gemm_int4_param_set);
  }
};
graph_t::Gemm_w4abf16_param_t gemm_int4_param_set {
    .tiling_params = GemmW4Bf16_tiling_params
};
test_graph compute_graph(gemm_int4_param_set);
int main()
{
    return 0;
}
"""
    top_cpp = os.path.join(output_path, f"top_gemm_w4abf16.cpp")
    with open(top_cpp, "w") as f:
        f.write(top_cpp_content)

    return graph, opargs


###############################################################################
# gen_gemm_w4abf16_11k/OgaGemmW4ABf1611kTGTiling
###############################################################################
def gen_gemm_w4abf16_11k(overlay, m, k, n, g, s):
    """
    b: batch size
    m,k,n: matrix dim
    g: group_size
    s: sign
    """
    if overlay == "2x4x4":
        mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
        graph = mlir_template.format(m=m, k=k)
        # ----------------------------------------------------------------------
        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "gemm" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
config.generator.operator_args.iteration_domains.N.size = {n} : ui32
config.generator.operator_args.config.GROUP_SIZE = {g} : ui32
config.generator.operator_args.config.SIGN = {s} : ui32
"""
        opargs = opargs_template.format(m=m, k=k, n=n, g=g, s=s)

        # ----------------------------------------------------------------------
        top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_gemm_abf16_w4_11k.h"
class FlexMLGraph : public adf::graph
{
public:
    // subgraphs
    gemmLayer1::GemmGraph Layer1;
    gemmLayer2::GemmGraph Layer2;
    Add1dGraph Layer3;
    adf::external_buffer<int16_t> ifm_ddr;
    adf::external_buffer<int16_t> wts_ddr;
    adf::external_buffer<int16_t> ofm_ddr;
    adf::external_buffer<int16_t> ofm_tmp_ddr;
public:
    FlexMLGraph()
    {
        ifm_ddr = adf::external_buffer<int16_t>::create(gemm_layer1::ifm_L3_dim, 0, gemm_layer1::COLS * 2);
        wts_ddr = adf::external_buffer<int16_t>::create({gemm_layer1::wts_L3_dim[0]}, 0, gemm_layer1::COLS * 2);
        ofm_ddr = adf::external_buffer<int16_t>::create(add::ofm_L3_dim, add::COLS, 0);
        ofm_tmp_ddr = adf::external_buffer<int16_t>::create(gemm_layer1::ofm_L3_dim, gemm_layer1::COLS * 2, gemm_layer1::COLS * 2);
        Layer1.connectExternalBuffers(ifm_ddr, wts_ddr, ofm_tmp_ddr);     // 1st half-k
        Layer2.connectExternalBuffers(ifm_ddr, wts_ddr, ofm_tmp_ddr);     // 2nd half-k
        Layer3.connectExternalBuffers(ofm_tmp_ddr, ofm_tmp_ddr, ofm_ddr); // final nm result
        for (int col = 0; col < add::COLS; col++)
        {
            for (int row = 0; row < add::ROWS; row++)
            {
                execution_order(Layer1.superkernel[col][row], Layer2.superkernel[col][row]);
                execution_order(Layer2.superkernel[col][row], Layer3.superkernel[col][row]);
            }
        }
    }
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
        top_cpp = top_cpp_template
    else:
        graph = "Placeholder"
        opargs = "Placeholder"
        top_cpp = "Placeholder"

    return graph, opargs, top_cpp


###############################################################################
# gen_gemm_w4abf16_4k/OgaGemmW4ABf164kTGTiling
###############################################################################
def gen_gemm_w4abf16_4k(overlay, m, k, n, g, s):
    """
    b: batch size
    m,k,n: matrix dim
    g: group_size
    s: sign
    """
    if overlay == "2x4x4":
        mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
        graph = mlir_template.format(m=m, k=k)
        # ----------------------------------------------------------------------
        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "gemm" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
config.generator.operator_args.iteration_domains.N.size = {n} : ui32
config.generator.operator_args.config.GROUP_SIZE = {g} : ui32
config.generator.operator_args.config.SIGN = {s} : ui32
"""
        opargs = opargs_template.format(m=m, k=k, n=n, g=g, s=s)

        # ----------------------------------------------------------------------
        top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_gemm_abf16_w4_4k.h"
class FlexMLGraph : public adf::graph
{
public:
    GemmGraph gemmLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
        top_cpp = top_cpp_template
    else:
        graph = "Placeholder"
        opargs = "Placeholder"
        top_cpp = "Placeholder"

    return graph, opargs, top_cpp


def gen_rope_ukernel(overlay, b, m, k):
    mlir_template = """module {{
  func.func @RoPEBf16(%arg0: tensor<{b}x{m}x{k}xbf16>) -> tensor<{b}x{m}x{k}xbf16> {{
    %0 = tensor.empty() : tensor<{b}x{m}x{k}xbf16>
    %2 = xten_nn.subgraph (%arg0 : tensor<{b}x{m}x{k}xbf16>)
			   attributes {{LayerName = "",
			               OperatorArgs = {{iteration_domains.B.size = {b} : ui32,
                                           iteration_domains.M.size = {m} : ui32,
                                           iteration_domains.K.size = {k} : ui32}},
						       OperatorName = "RoPEBf16TGTiling",
						       OutputName = "",
						       Reason = "Microkernel"}} -> tensor<32x{m}x{k}xbf16>
    return %2 : tensor<32x{m}x{k}xbf16>
  }}
}}    
"""
    graph = mlir_template.format(b=b, m=m, k=k)

    #########################################################
    # Generate top MLADF wrapper
    #########################################################
    top_cpp_content = """#include "multi_layer_overlay_rope.h"
#include "tiling.h"
#define DTYPE bfloat16
using graph_t = mllib_graphs::multi_layer_overlay_rope<DTYPE, 4, 4, 0x3000, 0x400, 0x400>;
class test_graph : public adf::graph {
   public:
    using ddr_t = adf::external_buffer<DTYPE>;
   public:
    ddr_t ifm_ddr;
    ddr_t wts_ddr;
    ddr_t ofm_ddr;
    test_graph(struct mllib_graphs::RoPEAdf_param_t& rope_param_set) {
        using namespace adf;
        ifm_ddr = ddr_t::create(rope_param_set.tiling_params[0].ifm_ddr_size, 0, 1);
        wts_ddr = ddr_t::create(rope_param_set.tiling_params[0].wts_ddr_size, 0, 1);
        ofm_ddr = ddr_t::create(rope_param_set.tiling_params[0].ofm_ddr_size, 1, 0);
        graph_t tg(ifm_ddr, wts_ddr, ofm_ddr, rope_param_set);
 }
};
graph_t::RoPEAdf_param_t rope_param_set {
    .tiling_params = rope_tiling_params
};
test_graph compute_graph(rope_param_set);
int main()
{
    return 0;
}
"""
    return graph, opargs


###############################################################################
# rope_bmk_bmk
# rope_mbk_mbk
# rope_mbk_bmk
###############################################################################
def gen_rope(overlay, b, m, k):
    if overlay == "2x4x4":
        mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
        graph = mlir_template.format(m=m, k=k)

        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "rope" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.B.size = {b} : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
"""
        opargs = opargs_template.format(b=b, m=m, k=k)

        # ----------------------------------------------------------------------
        top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_rope.h"
class FlexMLGraph : public adf::graph
{
public:
    RoPEGraph RopeLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
        top_cpp = top_cpp_template
    else:
        graph = "Placeholder"
        opargs = "Placeholder"
        top_cpp = "Placeholder"

    return graph, opargs, top_cpp


###############################################################################
# rope_variant_bmk_bmk
# rope_variant_mbk_mbk
# rope_variant_mbk_bmk
###############################################################################
def gen_rope_variant(overlay, b, m, k):
    if overlay == "2x4x4":
        mlir_template = """module {{
func.func @dummy(%arg0: tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> {{
    return %arg0 : tensor<1x1x1xbf16>
  }}
}}
"""
        graph = mlir_template.format(m=m, k=k)

        opargs_template = """ml_adf_header_file = "NA" : str
namespace_name = "rope_variant" : str
tg.config.stack_location = 0 : ui32
tg.config.tg_stack_size = 0 : ui32
tg.config.tg_heap_size = 0 : ui32
config.generator.operator_args.iteration_domains.B.size = {b} : ui32
config.generator.operator_args.iteration_domains.M.size = {m} : ui32
config.generator.operator_args.iteration_domains.K.size = {k} : ui32
"""
        opargs = opargs_template.format(b=b, m=m, k=k)

        # ----------------------------------------------------------------------
        top_cpp_template = """// This file is automatically generated by VAIML
#include <adf.h>
#include "tiling.h"    
#include "overlay/single_layer_2x4x4_overlay_rope_variant.h"
class FlexMLGraph : public adf::graph
{
public:
    RoPE_Variant_Graph RopeLayer;
};
FlexMLGraph compute_graph;
int main() {
  return 0;
}
"""
        top_cpp = top_cpp_template
    else:
        graph = "Placeholder"
        opargs = "Placeholder"
        top_cpp = "Placeholder"

    return graph, opargs, top_cpp


###############################################################################
# Entry point for all operators
###############################################################################
def gen_op(output_path, model_name, overlay, op_type, b0, b1, m, k, n, h, g, s, hs):
    """
    output_path: output path
    model_name: model name. Currently it matches the name in DD
    overlay: target overlay. 4x4 or 2x4x4
    op_type: operator type
    b: batch
    m/k/n: matrix dim
    h: head/headsize
    g: group size/block size
    s: sign
    """
    print(f"Generating model files for {op_type} for overlay {overlay}")
    model_type = "onnx"
    if op_type == "silu":
        graph, opargs, top_cpp = gen_silu(overlay, m, k)
        model_type = "ukernel.mlir"
    elif op_type == "mul":
        graph, opargs, top_cpp = gen_mul(overlay, m, k)
        model_type = "ukernel.mlir"
    elif op_type == "add":
        graph, opargs, top_cpp = gen_add(overlay, m, k)
        model_type = "ukernel.mlir"
    elif op_type == "rmsnorm":
        graph, opargs, top_cpp = gen_rmsnorm(overlay, m, k)
        model_type = "ukernel.mlir"
    elif op_type in ["matmul"]:
        graph = gen_matmul(b0, m, n, k)
    elif op_type in ["bmm1"]:
        graph, opargs, top_cpp = gen_bmm1(overlay, b0, b1, m, n, k)
        model_type = "ukernel.mlir"
    elif op_type in ["bmm2"]:
        graph, opargs, top_cpp = gen_bmm2(overlay, b0, b1, m, n, k)
        model_type = "ukernel.mlir"
    elif op_type == "masked_softmax":
        graph, opargs, top_cpp = gen_masked_softmax(overlay, b0, m, k, hs)
        model_type = "ukernel.mlir"
    elif op_type in ["rope_bmk_bmk", "rope_mbk_mbk", "rope_mbk_bmk"]:
        graph, opargs, top_cpp = gen_rope(overlay, b0, m, k)
        model_type = "ukernel.mlir"
    elif op_type in [
        "rope_variant_bmk_bmk",
        "rope_variant_mbk_mbk",
        "rope_variant_mbk_bmk",
    ]:
        graph, opargs, top_cpp = gen_rope_variant(overlay, b0, m, k)
        model_type = "ukernel.mlir"
    elif op_type == "gemm_w4abf16":
        # set sign to 0 for now
        if k > 4096:
            graph, opargs, top_cpp = gen_gemm_w4abf16_11k(overlay, m, k, n, g, s)
        else:
            graph, opargs, top_cpp = gen_gemm_w4abf16_4k(overlay, m, k, n, g, s)

        model_type = "ukernel.mlir"
    else:
        print("WARNING: Unsupported operator type: {}", op_type)
        return -1

    # Create model
    if model_type == "onnx":
        model = helper.make_model(
            graph,
            producer_name="onnx-add-example",
            opset_imports=[helper.make_operatorsetid("", 13)],
        )
        # Validate and save the model
        onnx.checker.check_model(model)
        model_out = os.path.join(output_path, model_name)
        onnx.save(model, model_out)
        print(f"ONNX model saved to {model_out}")
    elif model_type == "ukernel.mlir":
        model_out = os.path.join(output_path, f"{model_name}.ukernel.mlir")
        with open(model_out, "w") as f:
            f.write(graph)
        print(f"MLIR model saved to {model_out}")

        opargs_out = os.path.join(output_path, f"{model_name}.opargs")
        with open(opargs_out, "w") as f:
            f.write(opargs)
        print(f"OPARGS saved to {opargs_out}")

        top_cpp_out = os.path.join(output_path, f"top_{op_type}.cpp")
        with open(top_cpp_out, "w") as f:
            f.write(top_cpp)
        print(f"Top CPP saved to {top_cpp_out}")

    return 0


###############################################################################
###############################################################################
# Transaction library generation
###############################################################################
###############################################################################
def get_var_name(file_path):
    file_name = file_path.name
    dir_name = file_path.parent.name
    var_name = dir_name + "_" + file_name.split(".")[0]
    return var_name


def bin_to_cpp(file_path):
    variable_name = get_var_name(file_path)
    decompressed_size = 0

    with open(file_path, "rb") as file:
        binary_data = file.read()
        decompressed_size = len(binary_data)
        compressed_data = zlib.compress(binary_data)
        hex_data = "".join(f"\\x{byte:02x}" for byte in compressed_data)
        # Windows has a 16380-char string limit. Break up long strings with ""
        max_string_len = 16380
        hex_data = '""'.join(
            hex_data[i : i + max_string_len]
            for i in range(0, len(hex_data), max_string_len)
        )

    return file_path, variable_name, decompressed_size, len(compressed_data), hex_data


def write_bin_to_file(txn_hdrf, txn_srcf, data):
    file_path, variable_name, decompressed_size, len_compressed_data, hex_data = data
    with open(txn_hdrf, "a") as hdr:
        hdr.write(f"const std::string& get{variable_name.capitalize()}();\n")

    with open(txn_srcf, "a") as src:
        src.write(f'static char {variable_name}[] = "{hex_data}";\n')
        # write a function
        src.write(f"static std::string initialize_{variable_name}()" + "{\n")
        src.write('std::string ret = "";\n')
        src.write("uLongf ret_size = 0;\n")
        src.write(f"ret.resize({decompressed_size});\n")
        src.write("z_stream infstream = {};\n")
        src.write("infstream.zalloc = Z_NULL;\n")
        src.write("infstream.zfree = Z_NULL;\n")
        src.write("infstream.opaque = Z_NULL;\n")
        src.write(f"infstream.avail_in = {len_compressed_data};\n")
        src.write(f"infstream.next_in = reinterpret_cast<Bytef*>({variable_name});\n")
        src.write(f"infstream.avail_out = {decompressed_size};\n")
        src.write(f"infstream.next_out = reinterpret_cast<Bytef*>(&ret[0]);\n")
        src.write("inflateInit(&infstream);\n")
        src.write("inflate(&infstream, Z_NO_FLUSH);\n")
        src.write("inflateEnd(&infstream);\n")
        src.write("return ret;\n")
        src.write("}\n")

        src.write(f"const std::string& get{variable_name.capitalize()}() {{\n")
        src.write(
            f"static const std::string {variable_name}_str = initialize_{variable_name}();\n"
        )
        src.write(f"return {variable_name}_str;\n")
        src.write("}\n")


def write_transaction_src(out_dir, txn_data, txn_hdr):
    # txn_list = [i[0] for i in txn_data]
    txn_list = txn_data
    with open(Path(out_dir) / Path("transaction.cpp"), "w") as txn_src:
        txn_src.write('#include "txn_container.hpp"\n')
        txn_src.write(f'#include "{txn_hdr}"\n')
        txn_src.write("\nTransaction::Transaction(){{}};\n")
        txn_src.write(
            "const std::string& Transaction::get_txn_str(const std::string& name) {\n"
        )
        txn_src.write("\tstd::string op_name;\n")
        txn_src.write("\top_name = name;\n")
        for txn_file in txn_list:
            var_name = get_var_name(txn_file)
            func_name = get_var_name(txn_file).capitalize()
            txn_src.write(f'\tif (op_name == "{var_name}")\n')
            txn_src.write(f"\t\treturn get{func_name}();\n")
        txn_src.write("\telse\n")
        txn_src.write(
            '\t\tthrow std::runtime_error("Invalid transaction binary string: " + op_name);\n'
        )
        txn_src.write("}\n")

        filtering_words = ["param", "ddr_buffer_info"]  # getting rid of repetition
        filtering_fun = lambda x: all(
            [not substr in get_var_name(x) for substr in filtering_words]
        )
        txn_list_filtered = list(filter(filtering_fun, txn_list))
        txn_src.write(
            "std::vector<std::string> Transaction::match_prefix(const std::string& prefix) {\n"
        )
        txn_src.write(
            f"\tconst static std::array<std::string, {len(txn_list_filtered)}> all_txn_strings =\u007b\n"
        )
        for i, txn_file in enumerate(txn_list_filtered):
            var_name = get_var_name(txn_file)
            if i == len(txn_list) - 1:
                txn_src.write(f'\t"{var_name}"\n')
            else:
                txn_src.write(f'\t"{var_name}",\n')
        txn_src.write("\t};\n")
        txn_src.write("\tstd::vector<std::string> matched_strings;\n")
        txn_src.write(
            "\tstd::copy_if(all_txn_strings.begin(), all_txn_strings.end(), std::back_inserter(matched_strings),\n"
        )
        txn_src.write(
            "\t\t[prefix](const std::string& txn_var_name){ return txn_var_name.rfind(prefix, 0) == 0; });\n"
        )
        txn_src.write("\treturn matched_strings;\n")
        txn_src.write("}\n")


def get_bin_file(large_txn_ops, txn_bin_path):
    for root, dirs, files in os.walk(txn_bin_path):
        for filename in fnmatch.filter(files, "*.bin"):
            file_path = Path(root) / filename
            fname = str(file_path)
            fname = fname.replace("\\", "/")
            if any(map(lambda large_op: large_op in fname, large_txn_ops)):
                continue
            yield file_path
        for filename in fnmatch.filter(files, "*.json"):
            file_path = Path(root) / filename
            fname = str(file_path)
            fname = fname.replace("\\", "/")
            if any(map(lambda large_op: large_op in fname, large_txn_ops)):
                continue
            yield file_path


def gen_txn_container(out_dir):
    txn_containder_cpp = Path(out_dir) / Path("txn_container.cpp")
    txn_containder_cpp_content = """// This file is automatically generated by VAIML
    #include "txn_container.hpp"
std::vector<std::uint8_t> Transaction::get_txn_bvec(const std::string &name) {
  const std::string &txn_string = Transaction::get_txn_str(name);
  std::vector<std::uint8_t> txnData(txn_string.begin(), txn_string.end());
  return txnData;
}
"""
    with open(txn_containder_cpp, "w") as f:
        f.write(txn_containder_cpp_content)

    txn_containder_hpp = Path(out_dir) / Path("txn_container.hpp")

    txn_containder_hpp_content = """// This file is automatically generated by VAIML
#ifndef TRANSACTION_H
#define TRANSACTION_H
#include <algorithm>
#include <array>
#include <iterator>
#include <map>
#include <mutex>
#include <string>
#include <vector>
class Transaction {
public:
  static Transaction &getInstance() {
    static Transaction instance;
    return instance;
  }
  Transaction(Transaction const &) = delete;
  void operator=(Transaction const &) = delete;
  const std::string &get_txn_str(const std::string &);
  std::vector<std::uint8_t> get_txn_bvec(const std::string &);
  std::vector<std::string> match_prefix(const std::string &);
  Transaction();
  template <typename T>
  void GetBinData(const std::string &binaryData, std::vector<T> &outVector,
                  bool append = false) {
    const char *dataPtr = binaryData.data();
    size_t dataLen = binaryData.size();
    if (!append) {
      outVector.clear();
    }
    size_t currentSize = outVector.size();
    size_t newSize = dataLen / sizeof(T);
    size_t minSize = std::min(currentSize, newSize);
    for (size_t i = 0; i < minSize; ++i) {
      memcpy(&outVector[i], dataPtr + i * sizeof(T), sizeof(T));
    }
    for (size_t i = currentSize; i < newSize; ++i) {
      T value;
      memcpy(&value, dataPtr + i * sizeof(T), sizeof(T));
      outVector.push_back(value);
    }
    if (append && dataLen % sizeof(T) != 0) {
      T value = 0; // Zero-initialize the value
      memcpy(&value, dataPtr + newSize * sizeof(T), dataLen % sizeof(T));
      outVector.push_back(value);
    }
  }
};
#endif
"""
    with open(txn_containder_hpp, "w") as f:
        f.write(txn_containder_hpp_content)

    print(
        f"INFO: Transaction container generated: {txn_containder_cpp} {txn_containder_hpp}"
    )


def gen_txnbin_plugin_installer(out_dir):
    # objects.txt is needed to generate symbol export file for transaction.dll
    # Cannot have empty line in objects.txt
    objects_txt = Path(out_dir) / Path("objects.txt")
    objects_txt_content = """transaction.obj
all_txn_pkg.obj
txn_container.obj
"""
    with open(objects_txt, "w") as f:
        f.write(objects_txt_content)

    install_script = Path(out_dir) / Path("install-transaction-plugin.bat")
    install_script_content = """
@echo off
REM This script installs transaction.dll to LLM NPU package instalaltion
REM Check if the first argument is provided
if "%1"=="" (
    echo Usage: %0 LLM-NPU-package-root
    exit /b 1
)
REM Get the destination directory from the first argument
set LLM_NPU_ROOT=%1
REM Set up VS compiler and linker environment
call vcvars64.bat
CL.exe /c /nologo /W1 /WX- /diagnostics:column /O2 /Ob2 /D _WINDLL /D _MBCS /D WIN32 /D _WINDOWS /D NDEBUG /D transaction_EXPORTS /Gm- /EHsc /MD /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /std:c++17 /Fd /external:W0 /Gd /TP /errorReport:queue  /external:I %CONDA_PREFIX%/Library/include /bigobj all_txn_pkg.cpp transaction.cpp txn_container.cpp
dir *.obj
cmake.exe -E __create_def exports.def objects.txt
link.exe /ERRORREPORT:QUEUE /OUT:transaction.dll /INCREMENTAL:NO /NOLOGO %CONDA_PREFIX%\\Library\\lib\\zlibstatic.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /DEF:exports.def /MANIFEST /MANIFESTUAC:"level='asInvoker' uiAccess='false'" /manifest:embed /SUBSYSTEM:CONSOLE /TLBID:1 /DYNAMICBASE /NXCOMPAT /IMPLIB:"transaction.lib" /MACHINE:X64  /machine:x64 /DLL transaction.obj all_txn_pkg.obj txn_container.obj
copy /y transaction.dll %LLM_NPU_ROOT%\\libs
dir %LLM_NPU_ROOT%\\libs\\transaction.dll
"""

    with open(install_script, "w") as f:
        f.write(install_script_content)

    print(f"INFO: Transaction binary plugin installer generated: {install_script}")


###############################################################################
#
###############################################################################
def generate_transaction_plugin(root_dir):
    disable_large_txn_ops = 0
    out_dir = "transaction-plugin"
    large_txn_ops = []
    if disable_large_txn_ops:
        large_txn_ops = ["/mladfmatmulbias/", "/elwmul/"]

    txn_bin_path = root_dir / Path("transaction/stx")

    txn_hdr_fname = "all_txn_pkg.hpp"
    txn_src_fname = "all_txn_pkg.cpp"

    if not os.path.exists(out_dir):
        os.mkdir(out_dir)
    txn_list = []
    txn_hdrf = Path(out_dir) / Path(txn_hdr_fname)
    txn_srcf = Path(out_dir) / Path(txn_src_fname)
    with open(txn_srcf, "w") as src:
        src.write("#include <string>\n")
        src.write("#include <zlib.h>\n")
        src.write(f'#include "{txn_hdr_fname}"\n')

    with open(txn_hdrf, "w") as hdr:
        hdr.write("#pragma once\n")
        hdr.write("#include <string>\n")
        hdr.write("#include <tuple>\n")

    files_iter = get_bin_file(large_txn_ops, txn_bin_path)
    txn_list = []
    total_txn_bins = 0
    for f in files_iter:
        # print(f"Processing transaction binary {f}")
        data = bin_to_cpp(f)
        if len(data) == 0:
            continue
        write_bin_to_file(txn_hdrf, txn_srcf, data)
        txn_list.append(data[0])
        total_txn_bins += 1

    print(f"INFO: Processed {total_txn_bins} transaction binaries")
    write_transaction_src(out_dir, txn_list, txn_hdr_fname)

    print(
        f"Generated package with all transaction binaries generated : {txn_hdr_fname} {txn_src_fname}"
    )

    gen_txn_container(out_dir)
    gen_txnbin_plugin_installer(out_dir)


if __name__ == "__main__":
    # gen_op(output_path, op_type, b, m, k, n, h, g)
    # gen_op("llama2-dummy", "2x4x4", "silu", 1, 11008, 1024, 1, 0, 0)
    # gen_op("llama2-dummy", "2x4x4", "mul", 1, 11008, 1024, 1, 0, 0)
    # gen_op("llama2-dummy", "2x4x4", "add", b=1, m=1, k=4096, n=1, h=0, g=0)
    # gen_op("llama2-dummy", "2x4x4", "rmsnorm", b=1, m=1, k=4096, n=1, h=0, g=0)
    # gen_op("llama2-dummy", "2x4x4", "matmul", 32, 512, 512, 128, 0, 0)
    # gen_op(output_path, model_name, overlay, op_type, b0, b1, m, k, n, h, g, s):

    gen_op(
        "llama2-dummy",
        "bmm1",
        "2x4x4",
        "bmm1",
        b0=32,
        b1=4,
        m=512,
        k=512,
        n=128,
        h=0,
        g=0,
        s=0,
    )
    gen_op(
        "llama2-dummy",
        "bmm2",
        "2x4x4",
        "bmm2",
        b0=32,
        b1=4,
        m=512,
        k=512,
        n=128,
        h=0,
        g=0,
        s=0,
    )
    # gen_op("llama2-dummy", "2x4x4", "masked_softmax", b=32, m=1024, n=1024, k=1, h=0, g=0)
    # gen_op("llama2-dummy", "2x4x4", "masked_softmax", b=32, m=1024, n=1024, k=1, h=0, g=0)
    # gen_op("llama2-dummy", "2x4x4", "rope_bmk_bmk", b=32, m=2048, k=128, n=0, h=0, g=0)
    # gen_op("llama2-dummy", "2x4x4", "rope_mbk_mbk", b=32, m=2048, k=128, n=0, h=0, g=0)
    # gen_op("llama2-dummy", "2x4x4", "rope_mbk_bmk", b=32, m=2048, k=128, n=0, h=0, g=0)
    # gen_op(
    #    "llama2-dummy",
    #    "2x4x4",
    #    "rope_variant_bmk_bmk",
    #    b=32,
    #    m=2048,
    #    k=128,
    #    n=0,
    #    h=0,
    #    g=0,
    # )
    # gen_op(
    #    "llama2-dummy",
    #    "2x4x4",
    #    "rope_variant_mbk_mbk",
    #    b=32,
    #    m=2048,
    #    k=128,
    #    n=0,
    #    h=0,
    #    g=0,
    # )
    # gen_op(
    #    "llama2-dummy",
    #    "2x4x4",
    #    "rope_variant_mbk_bmk",
    #    b=32,
    #    m=2048,
    #    k=128,
    #    n=0,
    #    h=0,
    #    g=0,
    # )
    # gen_op(
    #    "llama2-dummy",
    #    "gemm_w4abf16_1",
    #    "2x4x4",
    #    "gemm_w4abf16",
    #    b=0,
    #    m=64,
    #    k=13696,
    #    n=3584,
    #    h=0,
    #    g=128,
    #    s=0,
    # )
    # gen_op(
    #    "llama2-dummy",
    #    "gemm_w4abf16_2",
    #    "2x4x4",
    #    "gemm_w4abf16",
    #    b=0,
    #    m=2048,
    #    k=4096,
    #    n=11008,
    #    h=0,
    #    g=128,
    #    s=1,
    # )
    generate_transaction_plugin("llama2-dummy")
