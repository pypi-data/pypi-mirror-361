{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation Tutorial using LLaMA and Mixtral\n",
    "\n",
    "This tutorial demonstrates how to use SDG repository to generate synthetic question-answer pairs from documents using large language models like LLaMA 3.3 70B. We will also generate data using Mixtral model for comparison. We'll cover:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Connecting to LLM servers\n",
    "3. Configuring the data generation pipeline\n",
    "4. Generating data with different models\n",
    "5. Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reloading of modules - useful during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Instructions\n",
    "\n",
    "Before running this notebook, you'll need to:\n",
    "\n",
    "```bash \n",
    "pip install sdg-hub==0.1.0a4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# datasets: For handling our data\n",
    "# OpenAI: For interfacing with the LLM servers\n",
    "# SDG components: For building our data generation pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "from openai import OpenAI\n",
    "\n",
    "from sdg_hub.flow import Flow\n",
    "from sdg_hub.pipeline import Pipeline\n",
    "from sdg_hub.sdg import SDG\n",
    "from sdg_hub.registry import PromptRegistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up LLaMA 3.3 70B Model\n",
    "\n",
    "First, we need to host the LLaMA model using vLLM. This creates an OpenAI-compatible API endpoint.\n",
    "\n",
    "1. Start the vLLM server (run in terminal):\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model meta-llama/Llama-3.3-70B-Instruct \\\n",
    "    --dtype float16 \\\n",
    "    --tensor-parallel-size 8 \n",
    "```\n",
    "\n",
    "2. Connect to the model using OpenAI client below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI client to connect to our local vLLM server\n",
    "endpoint = f\"http://localhost:8000/v1\"\n",
    "openai_api_key = \"EMPTY\"  # vLLM doesn't require real API key\n",
    "openai_api_base = endpoint\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "# Verify we can see the model\n",
    "teacher_model = client.models.list().data[0].id\n",
    "print(f\"Connected to model: {teacher_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Data Generation Pipeline\n",
    "\n",
    "Now we'll set up our Synthetic Data Generation (SDG) pipeline with the following components:\n",
    "1. SDG Flow configuration from YAML\n",
    "2. SDG Pipeline setup\n",
    "3. SDG configuration with batch processing, number of workers, and save frequency parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the flow configuration from YAML file\n",
    "flow_cfg = Flow(client).get_flow_from_file(\"synth_knowledge1.5_llama3.3.yaml\")\n",
    "\n",
    "# Initialize the SDG pipeline with processing parameters\n",
    "sdg = SDG(\n",
    "    [Pipeline(flow_cfg)],\n",
    "    num_workers=1,      # Number of parallel workers\n",
    "    batch_size=1,       # Batch size for processing\n",
    "    save_freq=1000,     # How often to save checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Seed Data\n",
    "\n",
    "We'll load our seed data (documents) that will be used to generate question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seed data from JSON file\n",
    "seed_data_path = \"Your seed data path\"  # Replace with your data path\n",
    "ds = load_dataset('json', data_files=seed_data_path, split='train')\n",
    "\n",
    "# For testing, we'll use just one example\n",
    "ds = ds.select(range(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with LLaMA 3.3\n",
    "\n",
    "Now we'll use our configured pipeline to generate synthetic question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data and save checkpoints\n",
    "generated_data = sdg.generate(ds, checkpoint_dir=\"Tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Mixtral Model\n",
    "\n",
    "For comparison, we'll also generate data using the Mixtral model. First, start the Mixtral server:\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model meta-llama/Llama-3.3-70B-Instruct \\\n",
    "    --dtype float16 \\\n",
    "    --tensor-parallel-size 8 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Mixtral model running on a different server\n",
    "mistral_client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=f\"http://10.7.0.15:8000/v1\",  # Update with your Mixtral server address\n",
    ")\n",
    "\n",
    "# Verify connection to Mixtral model\n",
    "mistral_client_teacher_model = mistral_client.models.list().data[0].id\n",
    "print(f\"Connected to Mixtral model: {mistral_client_teacher_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Mixtral Pipeline\n",
    "\n",
    "Set up a similar pipeline for Mixtral model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flow configuration for Mixtral\n",
    "flow_cfg_mistral = Flow(mistral_client).get_flow_from_file(\n",
    "    \"../../src/sdg_hub/flows/generation/knowledge/synth_knowledge1.5.yaml\"\n",
    ")\n",
    "\n",
    "# Initialize SDG pipeline for Mixtral\n",
    "sdg_mistral = SDG(\n",
    "    [Pipeline(flow_cfg_mistral)],\n",
    "    num_workers=1,\n",
    "    batch_size=1,\n",
    "    save_freq=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with Mixtral\n",
    "\n",
    "Generate synthetic data using the Mixtral model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data using Mixtral model\n",
    "generated_data_mistral = sdg_mistral.generate(ds, checkpoint_dir=\"Tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Generated Data\n",
    "\n",
    "Let's compare the outputs from both models by saving them to a markdown file for easy review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results to markdown file\n",
    "k = 5  # Number of examples to compare\n",
    "output_file = \"model_comparison.md\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Write the source document first\n",
    "    f.write(f\"### Document \\n{generated_data[0]['document']}\")\n",
    "    \n",
    "    # Compare generated Q&A pairs\n",
    "    for i in range(min(len(generated_data), len(generated_data_mistral))):\n",
    "        f.write(\"Example #{}\\n\".format(i+1))\n",
    "        \n",
    "        # LLaMA 3.3 results\n",
    "        f.write(\"### Result from llama3.3\\n\")\n",
    "        f.write(generated_data[i]['question'] + \"\\n\")\n",
    "        f.write(\"*******************************\\n\")\n",
    "        f.write(generated_data[i]['response'] + \"\\n\")\n",
    "        f.write(\"=================================\\n\")\n",
    "        \n",
    "        # Mixtral results\n",
    "        f.write(\"### Result from mistral\\n\") \n",
    "        f.write(generated_data_mistral[i]['question'] + \"\\n\")\n",
    "        f.write(\"*******************************\\n\")\n",
    "        f.write(generated_data_mistral[i]['response'] + \"\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"Wrote {k} examples to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Usage\n",
    "\n",
    "For large-scale data generation, use the command-line script instead of this notebook:\n",
    "\n",
    "```bash\n",
    "python scripts/generate.py --ds_path seed_data.jsonl \\\n",
    "    --bs 2 --num_workers 10 \\\n",
    "    --save_path <your_save_path> \\\n",
    "    --flow ../src/sdg_hub/flows/generation/knowledge/synth_knowledge1.5.yaml \\\n",
    "    --checkpoint_dir <your_checkpoint_dir> \\\n",
    "    --endpoint <your_endpoint>\n",
    "```\n",
    "\n",
    "Note: For LLaMA 3.3, use `synth_knowledge1.5_llama3.3.yaml` as the flow configuration file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
