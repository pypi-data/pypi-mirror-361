{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.priomptipy import render, UserMessage, SystemMessage, AssistantMessage, Scope, Isolate\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Messages with 91 Tokens Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"You are Quarkle, an AI Developmental Editor\"),\n",
    "    Isolate(token_limit=10, children=[\n",
    "        Scope([                                                        \n",
    "        UserMessage(\"Hello Quarkle, how are you?\"),\n",
    "        AssistantMessage(\"Hello, I am doing well. How can I help you\")\n",
    "    ], absolute_priority=5),\n",
    "    Scope(children=[\n",
    "            UserMessage(\"Write me a haiku on the number 17\"),\n",
    "            AssistantMessage(\"Seventeen whispers, In life's mosaic unseen, Quiet steps of time.\")\n",
    "    ], absolute_priority=10),\n",
    "]),\n",
    "    UserMessage(\"Okay nice, now give me a title for it\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case with 50 Max Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration_ms': None,\n",
      " 'output_handlers': [],\n",
      " 'priority_cutoff': 1000000000.0,\n",
      " 'prompt': {'messages': [{'content': 'You are Quarkle, an AI Developmental '\n",
      "                                     'Editor',\n",
      "                          'role': 'system'},\n",
      "                         {'content': 'Okay nice, now give me a title for it',\n",
      "                          'role': 'user'}],\n",
      "            'type': 'chat'},\n",
      " 'stream_handlers': [],\n",
      " 'token_count': 28,\n",
      " 'token_limit': 128000,\n",
      " 'tokenizer': 'o200k_base',\n",
      " 'tokens_reserved': 0}\n"
     ]
    }
   ],
   "source": [
    "render_options = {\"model\": \"gpt-4o\"}\n",
    "pprint.pprint(await render(messages, render_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only First and Last Message included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case with 75 Max Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration_ms': None,\n",
      " 'output_handlers': [],\n",
      " 'priority_cutoff': 1000000000.0,\n",
      " 'prompt': {'messages': [{'content': 'You are Quarkle, an AI Developmental '\n",
      "                                     'Editor',\n",
      "                          'role': 'system'},\n",
      "                         {'content': 'Okay nice, now give me a title for it',\n",
      "                          'role': 'user'}],\n",
      "            'type': 'chat'},\n",
      " 'stream_handlers': [],\n",
      " 'token_count': 28,\n",
      " 'token_limit': 75,\n",
      " 'tokenizer': 'cl100k_base',\n",
      " 'tokens_reserved': 0}\n"
     ]
    }
   ],
   "source": [
    "render_options = {\"token_limit\": 75, \"tokenizer\": \"cl100k_base\"}\n",
    "pprint.pprint(await render(messages, render_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional messages included based on priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case with 100 Max Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration_ms': None,\n",
      " 'output_handlers': [],\n",
      " 'priority_cutoff': 5,\n",
      " 'prompt': {'messages': [{'content': 'You are Quarkle, an AI Developmental '\n",
      "                                     'Editor',\n",
      "                          'role': 'system'},\n",
      "                         {'content': 'Hello Quarkle, how are you?',\n",
      "                          'role': 'user'},\n",
      "                         {'content': 'Hello, I am doing well. How can I help '\n",
      "                                     'you',\n",
      "                          'role': 'assistant'},\n",
      "                         {'content': 'Write me a haiku on the number 17',\n",
      "                          'role': 'user'},\n",
      "                         {'content': \"Seventeen whispers, In life's mosaic \"\n",
      "                                     'unseen, Quiet steps of time.',\n",
      "                          'role': 'assistant'},\n",
      "                         {'content': 'Okay nice, now give me a title for it',\n",
      "                          'role': 'user'}],\n",
      "            'type': 'chat'},\n",
      " 'stream_handlers': [],\n",
      " 'token_count': 91,\n",
      " 'token_limit': 100,\n",
      " 'tokenizer': 'cl100k_base',\n",
      " 'tokens_reserved': 0}\n"
     ]
    }
   ],
   "source": [
    "render_options = {\"token_limit\": 100, \"tokenizer\": \"cl100k_base\"}\n",
    "pprint.pprint(await render(messages, render_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All messages included since they can fit in context window limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"You are Quarkle, an AI Developmental Editor\"),\n",
    "    Isolate(token_limit=15, children=\n",
    "        [\n",
    "            UserMessage([\n",
    "                \"Hi there\", \n",
    "            Scope([\"Hello Quarkle, how are you?\"], relative_priority=5)\n",
    "            ])\n",
    "        ],\n",
    "    ),\n",
    "    Scope(children=[\n",
    "            UserMessage(\"Write me a haiku on the number 17\"),\n",
    "            AssistantMessage(\"Seventeen whispers, In life's mosaic unseen, Quiet steps of time.\")\n",
    "    ], absolute_priority=10),\n",
    "    UserMessage(\"Okay nice, now give me a title for it\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Child scope has a higher priority (1000000005.0) than its parent (1000000000.0). This is discouraged.\n",
      "WARNING: Child scope has a higher priority (1000000005.0) than its parent (1000000000.0). This is discouraged.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Base prompt estimated token count is 14 with 0 tokens reserved, which is higher than the limit 10.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m render_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(\u001b[38;5;28;01mawait\u001b[39;00m render(messages, render_options))\n",
      "File \u001b[0;32m~/dev/priomptpy/src/priomptipy/lib.py:325\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(elem, options)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03mRender a PromptElement into a RenderOutput object.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03mRenderOutput: The RenderOutput object.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m render_options \u001b[38;5;241m=\u001b[39m RenderOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m render_binary_search(elem, render_options)\n",
      "File \u001b[0;32m~/dev/priomptpy/src/priomptipy/lib.py:352\u001b[0m, in \u001b[0;36mrender_binary_search\u001b[0;34m(elem, options)\u001b[0m\n\u001b[1;32m    349\u001b[0m sorted_priority_levels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(priority_levels)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# Hydrate isolates\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m hydrate_isolates(elem, tokenizer)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Binary search logic\u001b[39;00m\n\u001b[1;32m    355\u001b[0m exclusive_lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/dev/priomptpy/src/priomptipy/lib.py:816\u001b[0m, in \u001b[0;36mhydrate_isolates\u001b[0;34m(elem, tokenizer)\u001b[0m\n\u001b[1;32m    814\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [hydrate_isolates(e, tokenizer) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem]\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;66;03m# Run tasks concurrently and wait for them to finish\u001b[39;00m\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mtype:\n",
      "File \u001b[0;32m~/dev/priomptpy/src/priomptipy/lib.py:828\u001b[0m, in \u001b[0;36mhydrate_isolates\u001b[0;34m(elem, tokenizer)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mcached_render_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m         elem\u001b[38;5;241m.\u001b[39mcached_render_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m render(\n\u001b[1;32m    829\u001b[0m             elem\u001b[38;5;241m.\u001b[39mchildren,\n\u001b[1;32m    830\u001b[0m             {\n\u001b[1;32m    831\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenizer,\n\u001b[1;32m    832\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: elem\u001b[38;5;241m.\u001b[39mtoken_limit,\n\u001b[1;32m    833\u001b[0m             },\n\u001b[1;32m    834\u001b[0m         )\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscope\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hydrate_isolates(elem\u001b[38;5;241m.\u001b[39mchildren, tokenizer)\n",
      "File \u001b[0;32m~/dev/priomptpy/src/priomptipy/lib.py:325\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(elem, options)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03mRender a PromptElement into a RenderOutput object.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03mRenderOutput: The RenderOutput object.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m render_options \u001b[38;5;241m=\u001b[39m RenderOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m render_binary_search(elem, render_options)\n",
      "File \u001b[0;32m~/dev/priomptpy/src/priomptipy/lib.py:383\u001b[0m, in \u001b[0;36mrender_binary_search\u001b[0;34m(elem, options)\u001b[0m\n\u001b[1;32m    380\u001b[0m token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m count_tokens_exact(tokenizer, final_prompt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), options)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_count \u001b[38;5;241m+\u001b[39m final_prompt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty_token_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m token_limit:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBase prompt estimated token count is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_prompt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempty_token_count\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens reserved, which is higher than the limit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_limit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    387\u001b[0m duration_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: final_prompt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: token_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpriority_cutoff\u001b[39m\u001b[38;5;124m\"\u001b[39m: sorted_priority_levels[inclusive_upper_bound],\n\u001b[1;32m    399\u001b[0m }\n",
      "\u001b[0;31mValueError\u001b[0m: Base prompt estimated token count is 14 with 0 tokens reserved, which is higher than the limit 10."
     ]
    }
   ],
   "source": [
    "render_options = {\"token_limit\": 100, \"tokenizer\": \"cl100k_base\"}\n",
    "pprint.pprint(await render(messages, render_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
