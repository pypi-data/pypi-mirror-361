Metadata-Version: 2.4
Name: ailogx
Version: 1.1.0
Summary: LLM-optimized structured logging and summarization for large-scale debugging.
Author: Kunwar Vikrant
Author-email: kunwar.vikrant3@gmail.com
License: Proprietary
Classifier: License :: Other/Proprietary License
Classifier: Programming Language :: Python :: 3
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: tiktoken
Requires-Dist: requests
Requires-Dist: rich
Requires-Dist: groq
Requires-Dist: openai
Requires-Dist: ollama
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: license
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ğŸ§  LLM Logger

**LLM Logger** is a Python logging framework designed for seamless integration with **Large Language Models (LLMs)**.  
It produces structured, LLM-friendly logs that can be easily summarized or reasoned about â€” even across massive, deeply nested codebases.

---

## ğŸš€ Features

- ğŸªµ **Structured JSON logs** (timestamped, contextual, machine-readable)
- ğŸ§  **LLM-optimized format** with `reason`, `inputs`, `outputs`, and semantic tags
- ğŸ“‚ **Log grouping** (`start_group`, `end_group`) and function spans
- ğŸ”Œ **Modular LLM backend** support via environment variable:
  - `Ollama` (local models)
  - `Groq` (LLama, Gemma via API)
  - `OpenAI` (GPT-3.5 / GPT-4)
- ğŸ“Š **Summarization CLI** with smart filtering and token-aware chunking
- ğŸ’¾ **Cache** for LLM calls with expiration/cleanup
- ğŸ§ª **Test harness** to simulate deeply nested logs

---

## ğŸ“¦ Installation

```bash
pip install ailogx
```

## ğŸ› ï¸ Basic Usage

```python
from ailogx.core import LLMLogger

log = LLMLogger("my-service")

log.llm_info("User login started", inputs={"username": "admin"})
log.llm_decision("Using 2FA", reason="high-risk user")
log.llm_error("Login failed", reason="Invalid OTP")
```

## ğŸ” Function Span

```python
with log.function_span("process_payment", reason="checkout flow"):
    # your logic
    pass
```

## ğŸ“‚ Grouping Logs

```python
log.start_group("req-42", reason="incoming API request")
# your logs here
log.end_group("req-42")
```

## ğŸ“Š LLM Summarization

### ğŸ§  Environment-based Backend Selection

Supports:

- `LLM_LOGGER_BACKEND=ollama` (default)
- `LLM_LOGGER_BACKEND=groq`
- `LLM_LOGGER_BACKEND=openai`

### ğŸ§¾ Example

```bash
export LLM_LOGGER_BACKEND=groq  # or 'openai', 'ollama'
export GROQ_API_KEY="your-groq-api-key"
- You must have a Groq account.
- Supported models: gemma3, llama3-70b, etc.

export OPENAI_API_KEY="your-openai-api-key"
- You must have an OpenAI API key.Models like gpt-3.5-turbo, gpt-4, etc. are supported.

For selecting Models : 
| Backend  | Env Var to Set | Example Value         |
|----------|----------------|-----------------------|
| groq     | GROQ_MODEL     | llama-3-70b-8192      |
| openai   | OPENAI_MODEL   | gpt-4o                |
| ollama   | OLLAMA_MODEL   | llama3                |

python -m ailogx.summarize simulated_logs/deep_nested_logs.jsonl --filter=smart --fast

For using with relevant intent:
python -m ailogx.summarize huge_app_logs.jsonl --filter=smart --fast --intent "focus on authentication and signup failures"
```

Or call from Python:

```python
from ailogx.summarizer.summarizer import multi_pass_summarize
from ailogx.backends.registry import get_analyzer
import json

with open("llm_logs.jsonl") as f:
    logs = [json.loads(line) for line in f]

summary = multi_pass_summarize(logs, get_analyzer())
print(summary)
```

## ğŸ§ª Test Harness

Generate deep, nested logs for benchmarking:

```bash
python ailogx/core.py
```

Outputs:

- `llm_simulated_logs.jsonl` (LLMLogger)
- `standard_simulated_logs.log` (Python logging)

## ğŸ” Cache & Optimization

- âœ… LLM responses cached to `.cache/`
- ğŸ§  Token-aware chunking
- ğŸ” Smart filtering (`--filter=smart`, `--intent="auth errors"`)
- âš¡ `--fast` mode for shallow summaries before full deep dives
