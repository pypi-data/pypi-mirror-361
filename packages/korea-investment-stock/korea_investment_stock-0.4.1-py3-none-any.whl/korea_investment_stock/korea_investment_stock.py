'''
í•œêµ­íˆ¬ìì¦ê¶Œ python wrapper
'''
import datetime
import json
import os
import pickle
import random
import threading
import time
import zipfile
import logging
import re
from collections import deque, defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import wraps
from pathlib import Path
from typing import Literal, Optional, List
from zoneinfo import ZoneInfo  # Requires Python 3.9+
from datetime import datetime, timedelta
import atexit

import pandas as pd
import requests
from typing import Dict, Any

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# Enhanced RateLimiter import
try:
    from .rate_limiting.enhanced_rate_limiter import EnhancedRateLimiter
    from .rate_limiting.enhanced_backoff_strategy import get_backoff_strategy
    from .rate_limiting.enhanced_retry_decorator import retry_on_rate_limit, retry_on_network_error
    from .error_handling.error_recovery_system import get_error_recovery_system
    from .monitoring.stats_manager import get_stats_manager
    from .caching import TTLCache, cacheable, CACHE_TTL_CONFIG
except ImportError:
    from rate_limiting.enhanced_rate_limiter import EnhancedRateLimiter
    from rate_limiting.enhanced_backoff_strategy import get_backoff_strategy
    from rate_limiting.enhanced_retry_decorator import retry_on_rate_limit, retry_on_network_error
    from error_handling.error_recovery_system import get_error_recovery_system
    from monitoring.stats_manager import get_stats_manager
    from caching import TTLCache, cacheable, CACHE_TTL_CONFIG

# Visualization ëª¨ë“ˆ
try:
    from .visualization import PlotlyVisualizer, DashboardManager
    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False
    logger.warning("Visualization ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. plotlyë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")

EXCHANGE_CODE = {
    "í™ì½©": "HKS",
    "ë‰´ìš•": "NYS",
    "ë‚˜ìŠ¤ë‹¥": "NAS",
    "ì•„ë©•ìŠ¤": "AMS",
    "ë„ì¿„": "TSE",
    "ìƒí•´": "SHS",
    "ì‹¬ì²œ": "SZS",
    "ìƒí•´ì§€ìˆ˜": "SHI",
    "ì‹¬ì²œì§€ìˆ˜": "SZI",
    "í˜¸ì¹˜ë¯¼": "HSX",
    "í•˜ë…¸ì´": "HNX"
}

# í•´ì™¸ì£¼ì‹ ì£¼ë¬¸
# í•´ì™¸ì£¼ì‹ ì”ê³ 
EXCHANGE_CODE2 = {
    "ë¯¸êµ­ì „ì²´": "NASD",
    "ë‚˜ìŠ¤ë‹¥": "NAS",
    "ë‰´ìš•": "NYSE",
    "ì•„ë©•ìŠ¤": "AMEX",
    "í™ì½©": "SEHK",
    "ìƒí•´": "SHAA",
    "ì‹¬ì²œ": "SZAA",
    "ë„ì¿„": "TKSE",
    "í•˜ë…¸ì´": "HASE",
    "í˜¸ì¹˜ë¯¼": "VNSE"
}

EXCHANGE_CODE3 = {
    "ë‚˜ìŠ¤ë‹¥": "NASD",
    "ë‰´ìš•": "NYSE",
    "ì•„ë©•ìŠ¤": "AMEX",
    "í™ì½©": "SEHK",
    "ìƒí•´": "SHAA",
    "ì‹¬ì²œ": "SZAA",
    "ë„ì¿„": "TKSE",
    "í•˜ë…¸ì´": "HASE",
    "í˜¸ì¹˜ë¯¼": "VNSE"
}

EXCHANGE_CODE4 = {
    "ë‚˜ìŠ¤ë‹¥": "NAS",
    "ë‰´ìš•": "NYS",
    "ì•„ë©•ìŠ¤": "AMS",
    "í™ì½©": "HKS",
    "ìƒí•´": "SHS",
    "ì‹¬ì²œ": "SZS",
    "ë„ì¿„": "TSE",
    "í•˜ë…¸ì´": "HNX",
    "í˜¸ì¹˜ë¯¼": "HSX",
    "ìƒí•´ì§€ìˆ˜": "SHI",
    "ì‹¬ì²œì§€ìˆ˜": "SZI"
}

CURRENCY_CODE = {
    "ë‚˜ìŠ¤ë‹¥": "USD",
    "ë‰´ìš•": "USD",
    "ì•„ë©•ìŠ¤": "USD",
    "í™ì½©": "HKD",
    "ìƒí•´": "CNY",
    "ì‹¬ì²œ": "CNY",
    "ë„ì¿„": "JPY",
    "í•˜ë…¸ì´": "VND",
    "í˜¸ì¹˜ë¯¼": "VND"
}

MARKET_TYPE_MAP = {
    "KR": ["300"],  # "301", "302"
    "KRX": ["300"],  # "301", "302"
    "NASDAQ": ["512"],
    "NYSE": ["513"],
    "AMEX": ["529"],
    "US": ["512", "513", "529"],
    "TYO": ["515"],
    "JP": ["515"],
    "HKEX": ["501"],
    "HK": ["501", "543", "558"],
    "HNX": ["507"],
    "HSX": ["508"],
    "VN": ["507", "508"],
    "SSE": ["551"],
    "SZSE": ["552"],
    "CN": ["551", "552"]
}

MARKET_TYPE = Literal[
    "KRX",
    "NASDAQ",
    "NYSE",
    "AMEX",
    "TYO",
    "HKEX",
    "HNX",
    "HSX",
    "SSE",
    "SZSE",
]

EXCHANGE_TYPE = Literal[
    "NAS",
    "NYS",
    "AMS"
]

MARKET_CODE_MAP: dict[str, MARKET_TYPE] = {
    "300": "KRX",
    "301": "KRX",
    "302": "KRX",
    "512": "NASDAQ",
    "513": "NYSE",
    "529": "AMEX",
    "515": "TYO",
    "501": "HKEX",
    "543": "HKEX",
    "558": "HKEX",
    "507": "HNX",
    "508": "HSX",
    "551": "SSE",
    "552": "SZSE",
}

EXCHANGE_CODE_MAP: dict[str, EXCHANGE_TYPE] = {
    "NASDAQ": "NAS",
    "NYSE": "NYS",
    "AMEX": "AMS"
}

API_RETURN_CODE = {
    "SUCCESS": "0",  # ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤
    "EXPIRED_TOKEN": "1",  # ê¸°ê°„ì´ ë§Œë£Œëœ token ì…ë‹ˆë‹¤
    "NO_DATA": "7",  # ì¡°íšŒí•  ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤
    "RATE_LIMIT_EXCEEDED": "EGW00201",  # Rate limit ì´ˆê³¼
}


# Note: retry_on_rate_limit decoratorëŠ” enhanced_retry_decorator ëª¨ë“ˆì—ì„œ importë¨


class KoreaInvestment:
    '''
    í•œêµ­íˆ¬ìì¦ê¶Œ REST API
    '''

    def __init__(self, api_key: str, api_secret: str, acc_no: str,
                 mock: bool = False, cache_enabled: bool = True, 
                 cache_config: Optional[dict] = None):
        """ìƒì„±ì
        Args:
            api_key (str): ë°œê¸‰ë°›ì€ API key
            api_secret (str): ë°œê¸‰ë°›ì€ API secret
            acc_no (str): ê³„ì¢Œë²ˆí˜¸ ì²´ê³„ì˜ ì• 8ìë¦¬-ë’¤ 2ìë¦¬
            exchange (str): "ì„œìš¸", "ë‚˜ìŠ¤ë‹¥", "ë‰´ìš•", "ì•„ë©•ìŠ¤", "í™ì½©", "ìƒí•´", "ì‹¬ì²œ", # todo: exchangeëŠ” ì œê±° ì˜ˆì •
                            "ë„ì¿„", "í•˜ë…¸ì´", "í˜¸ì¹˜ë¯¼"
            mock (bool): True (mock trading), False (real trading)
            cache_enabled (bool): True if cache is enabled, False otherwise
            cache_config (dict, optional): Configuration for the cache
        """
        self.mock = mock
        self.set_base_url(mock)
        self.api_key = api_key
        self.api_secret = api_secret

        # account number
        self.acc_no = acc_no
        self.acc_no_prefix = acc_no.split('-')[0]
        self.acc_no_postfix = acc_no.split('-')[1]
        
        # Enhanced RateLimiter ì„¤ì •
        self.rate_limiter = EnhancedRateLimiter(
            max_calls=15,  # ê¸°ë³¸ê°’ 20ì—ì„œ 15ë¡œ ê°ì†Œ
            per_seconds=1.0,
            safety_margin=0.8,  # ì‹¤ì œë¡œëŠ” 12íšŒ/ì´ˆ
            enable_min_interval=True,  # ìµœì†Œ ê°„ê²© ë³´ì¥
            enable_stats=True  # í†µê³„ ìˆ˜ì§‘ í™œì„±í™”
        )
        
        # ThreadPoolExecutor ê°œì„ 
        # ë™ì‹œ ì‹¤í–‰ ì œí•œì„ ìœ„í•œ ì„¸ë§ˆí¬ì–´ (ìµœëŒ€ 3ê°œë§Œ ë™ì‹œ ì‹¤í–‰)
        self.concurrent_limit = threading.Semaphore(3)
        # ì›Œì»¤ ìˆ˜ ê°ì†Œ (8 -> 3)
        self.executor = ThreadPoolExecutor(max_workers=3)
        # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ìë™ ì •ë¦¬
        atexit.register(self.shutdown)

        # access token
        self.token_file = Path("~/.cache/mojito2/token.dat").expanduser()
        self.access_token = None
        if self.check_access_token():
            self.load_access_token()
        else:
            self.issue_access_token()

        # Cache configuration
        self._cache_enabled = cache_enabled
        if cache_enabled:
            # ìºì‹œ ì„¤ì • ë³‘í•©
            default_cache_config = {
                'default_ttl': 300,  # 5ë¶„
                'max_size': 10000,
                'ttl_config': CACHE_TTL_CONFIG
            }
            if cache_config:
                default_cache_config.update(cache_config)
            
            # TTLCache ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self._cache = TTLCache(
                default_ttl=default_cache_config['default_ttl'],
                max_size=default_cache_config['max_size']
            )
            logger.info(f"TTL ìºì‹œ í™œì„±í™” (ê¸°ë³¸ TTL: {default_cache_config['default_ttl']}ì´ˆ, "
                       f"ìµœëŒ€ í¬ê¸°: {default_cache_config['max_size']})")
        else:
            self._cache = None
            logger.info("TTL ìºì‹œ ë¹„í™œì„±í™”")
        
        # Visualization ì´ˆê¸°í™”
        self.visualizer = None
        self.dashboard_manager = None
        if VISUALIZATION_AVAILABLE:
            try:
                self.visualizer = PlotlyVisualizer()
                self.dashboard_manager = DashboardManager(self.visualizer)
                logger.info("Visualization ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Visualization ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.shutdown()
        return False  # ì˜ˆì™¸ë¥¼ ì „íŒŒ

    def __execute_concurrent_requests(self, method, stock_list, 
                                     batch_size: Optional[int] = None,
                                     batch_delay: float = 0.0,
                                     progress_interval: int = 10,
                                     dynamic_batch_controller=None):
        """ë³‘ë ¬ ìš”ì²­ ì‹¤í–‰ (ê°œì„ ëœ ë²„ì „ with ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” ë° ë°°ì¹˜ ì²˜ë¦¬)
        
        Phase 3.4: ThreadPoolExecutor ì—ëŸ¬ ì²˜ë¦¬ í†µí•©
        Phase 4.1: ë°°ì¹˜ í¬ê¸° íŒŒë¼ë¯¸í„°í™”
        Phase 4.2: ë™ì  ë°°ì¹˜ ì¡°ì •
        
        Args:
            method: ì‹¤í–‰í•  ë©”ì„œë“œ
            stock_list: (symbol, market) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ì „ì²´ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬)
            batch_delay: ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            progress_interval: ì§„í–‰ ìƒí™© ì¶œë ¥ ê°„ê²©
            dynamic_batch_controller: DynamicBatchController ì¸ìŠ¤í„´ìŠ¤ (ë™ì  ì¡°ì •ìš©)
        """
        from .rate_limiting.enhanced_retry_decorator import RateLimitError, APIError
        from .rate_limiting.enhanced_backoff_strategy import get_backoff_strategy
        
        futures = {}
        results = []
        
        # Rate Limit ì—ëŸ¬ ë°œìƒ ì‹œ ì „ì²´ ì‘ì—… ì¤‘ë‹¨ í”Œë˜ê·¸
        rate_limit_error_occurred = False
        rate_limit_error = None
        
        def wrapped_method(symbol, market):
            """ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ"""
            with self.concurrent_limit:
                return method(symbol, market)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        if dynamic_batch_controller:
            # ë™ì  ë°°ì¹˜ ì¡°ì • ì‚¬ìš©
            current_batch_size, current_batch_delay = dynamic_batch_controller.get_current_parameters()
            batch_size = current_batch_size
            batch_delay = current_batch_delay
            print(f"ğŸ¯ ë™ì  ë°°ì¹˜ ì¡°ì • ëª¨ë“œ: ì´ˆê¸° ë°°ì¹˜ í¬ê¸°={batch_size}, ëŒ€ê¸° ì‹œê°„={batch_delay:.1f}s")
        
        if batch_size is None:
            batches = [stock_list]  # ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ
        else:
            # stock_listë¥¼ batch_size í¬ê¸°ë¡œ ë‚˜ëˆ„ê¸°
            batches = [stock_list[i:i + batch_size] for i in range(0, len(stock_list), batch_size)]
            print(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ: {len(stock_list)}ê°œ í•­ëª©ì„ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ì²˜ë¦¬ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        # ì „ì²´ ì‘ì—…ì„ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡ ê°ì‹¸ê¸°
        max_retries = 3  # ì „ì²´ ì‘ì—… ì¬ì‹œë„ íšŸìˆ˜
        retry_count = 0
        
        while retry_count < max_retries:
            futures.clear()
            results.clear()
            rate_limit_error_occurred = False
            rate_limit_error = None
            
            # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
            for batch_idx, batch in enumerate(batches):
                # ë™ì  ë°°ì¹˜ ì¡°ì •: ê° ë°°ì¹˜ë§ˆë‹¤ ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
                if dynamic_batch_controller and batch_idx > 0:
                    new_batch_size, new_batch_delay = dynamic_batch_controller.get_current_parameters()
                    if new_batch_size != batch_size or new_batch_delay != batch_delay:
                        batch_size = new_batch_size
                        batch_delay = new_batch_delay
                        print(f"ğŸ“Š ë°°ì¹˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: í¬ê¸°={batch_size}, ëŒ€ê¸°={batch_delay:.1f}s")
                        # ìƒˆë¡œìš´ ë°°ì¹˜ í¬ê¸°ë¡œ ì¬êµ¬ì„±ì´ í•„ìš”í•œ ê²½ìš° (ë‹¤ìŒ ë£¨í”„ì—ì„œ ì ìš©)
                
                if len(batches) > 1:
                    print(f"\nğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{len(batches)} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ í•­ëª©)")
                
                # ë°°ì¹˜ ì‹œì‘ ì‹œê°„ ê¸°ë¡
                batch_start_time = time.time()
                
                # ë°°ì¹˜ ë‚´ ìˆœì°¨ì  ì œì¶œë¡œ ì´ˆê¸° ë²„ìŠ¤íŠ¸ ë°©ì§€
                batch_futures = {}
                submit_delay = 0.01  # ê° ì œì¶œ ê°„ 10ms ëŒ€ê¸°
                
                # ë°°ì¹˜ í†µê³„ ì´ˆê¸°í™”
                batch_stats = {
                    'batch_idx': batch_idx,
                    'batch_size': len(batch),
                    'submit_start': time.time(),
                    'symbols': []
                }
                
                for idx, (symbol, market) in enumerate(batch):
                    # ìˆœì°¨ì  ì œì¶œë¡œ ì´ˆê¸° ë²„ìŠ¤íŠ¸ ë°©ì§€
                    if idx > 0 and submit_delay > 0:
                        time.sleep(submit_delay)
                    
                    future = self.executor.submit(wrapped_method, symbol, market)
                    batch_futures[future] = (symbol, market)
                    futures[future] = (symbol, market)
                    batch_stats['symbols'].append(symbol)
                
                batch_stats['submit_end'] = time.time()
                batch_stats['submit_duration'] = batch_stats['submit_end'] - batch_stats['submit_start']
                
                # ë°°ì¹˜ ë‚´ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                batch_completed = 0
                batch_total = len(batch)
                batch_success_count = 0
                batch_error_count = 0
                
                try:
                    for future in as_completed(batch_futures, timeout=30):  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                        symbol, market = batch_futures[future]
                        batch_completed += 1
                        
                        try:
                            result = future.result()
                            results.append(result)
                            batch_success_count += 1
                            
                            # ì§„í–‰ ìƒí™© ì¶œë ¥
                            if batch_completed % progress_interval == 0 or batch_completed == batch_total:
                                if len(batches) > 1:
                                    print(f"  ë°°ì¹˜ ì§„í–‰ë¥ : {batch_completed}/{batch_total} ({batch_completed/batch_total*100:.1f}%)")
                                else:
                                    total = len(stock_list)
                                    completed = len(results)
                                    print(f"ì²˜ë¦¬ ì§„í–‰ë¥ : {completed}/{total} ({completed/total*100:.1f}%)")
                                
                        except Exception as e:
                            error_info = {
                                'rt_cd': '9',  # ì—ëŸ¬ ì½”ë“œ
                                'msg1': f'Error: {str(e)}',
                                'error': True,
                                'symbol': symbol,
                                'market': market,
                                'error_type': type(e).__name__,
                                'error_details': str(e)
                            }
                            
                            # Rate Limit ì—ëŸ¬ ê°ì§€
                            if (isinstance(e, RateLimitError) or 
                                (hasattr(e, 'response') and isinstance(e.response, dict) and 
                                 e.response.get('rt_cd') == 'EGW00201') or
                                'EGW00201' in str(e)):
                                
                                print(f"âš ï¸ Rate Limit ì—ëŸ¬ ê°ì§€ - {symbol} ({market})")
                                rate_limit_error_occurred = True
                                rate_limit_error = e
                                
                                # ë‚¨ì€ ì‘ì—…ë“¤ ì·¨ì†Œ
                                for future in futures:
                                    if not future.done():
                                        future.cancel()
                                break
                            
                            # ì¼ë°˜ ì—ëŸ¬ ì²˜ë¦¬
                            print(f"âŒ ì—ëŸ¬ ë°œìƒ - {symbol} ({market}): {e}")
                            results.append(error_info)
                            batch_error_count += 1
                            
                            # Rate limit ì—ëŸ¬ì¸ ê²½ìš° ê¸°ë¡
                            if hasattr(self.rate_limiter, 'record_error'):
                                self.rate_limiter.record_error()
                    
                    # Rate Limit ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš° ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨
                    if rate_limit_error_occurred:
                        break
                    
                    # ë™ì  ë°°ì¹˜ ì¡°ì •: ë°°ì¹˜ ê²°ê³¼ ê¸°ë¡
                    if dynamic_batch_controller:
                        batch_elapsed_time = time.time() - batch_start_time
                        dynamic_batch_controller.record_batch_result(
                            batch_size=len(batch),
                            success_count=batch_success_count,
                            error_count=batch_error_count,
                            elapsed_time=batch_elapsed_time
                        )
                    
                    # ë°°ì¹˜ë³„ ê²°ê³¼ í†µê³„ ìˆ˜ì§‘ ë° ë¡œê¹…
                    batch_elapsed_time = time.time() - batch_start_time
                    batch_stats['process_end'] = time.time()
                    batch_stats['total_duration'] = batch_elapsed_time
                    batch_stats['success_count'] = batch_success_count
                    batch_stats['error_count'] = batch_error_count
                    batch_stats['throughput'] = (batch_success_count + batch_error_count) / batch_elapsed_time if batch_elapsed_time > 0 else 0
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹…
                    if len(batches) > 1:
                        print(f"\nğŸ“Š ë°°ì¹˜ {batch_idx + 1} í†µê³„:")
                        print(f"   - ì œì¶œ ì‹œê°„: {batch_stats['submit_duration']:.2f}ì´ˆ ({len(batch)}ê°œ)")
                        print(f"   - ì²˜ë¦¬ ì‹œê°„: {batch_elapsed_time:.2f}ì´ˆ")
                        print(f"   - ì„±ê³µ/ì‹¤íŒ¨: {batch_success_count}/{batch_error_count}")
                        print(f"   - ì²˜ë¦¬ëŸ‰: {batch_stats['throughput']:.1f} TPS")
                        
                        # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì—ëŸ¬ íƒ€ì…ë³„ë¡œ ë¶„ì„
                        if batch_error_count > 0:
                            error_types = {}
                            for r in results[-len(batch):]:  # í˜„ì¬ ë°°ì¹˜ì˜ ê²°ê³¼ë§Œ
                                if r.get('error'):
                                    error_type = r.get('error_type', 'Unknown')
                                    error_types[error_type] = error_types.get(error_type, 0) + 1
                            print(f"   - ì—ëŸ¬ íƒ€ì…: {error_types}")
                    
                    # ë°°ì¹˜ ê°„ ëŒ€ê¸° (ë§ˆì§€ë§‰ ë°°ì¹˜ ì œì™¸)
                    if batch_delay > 0 and batch_idx < len(batches) - 1:
                        print(f"â±ï¸ ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ {batch_delay}ì´ˆ ëŒ€ê¸°...")
                        time.sleep(batch_delay)
                        
                except TimeoutError:
                    print(f"â±ï¸ íƒ€ì„ì•„ì›ƒ ë°œìƒ - 30ì´ˆ ë‚´ ì™„ë£Œë˜ì§€ ì•Šì€ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤.")
                    # íƒ€ì„ì•„ì›ƒëœ ì‘ì—…ë“¤ ì²˜ë¦¬
                    for future, (symbol, market) in batch_futures.items():
                        if not future.done():
                            future.cancel()
                            results.append({
                                'rt_cd': '9',
                                'msg1': 'Timeout - operation took too long',
                                'error': True,
                                'symbol': symbol,
                                'market': market,
                                'error_type': 'TimeoutError'
                            })
                    # íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•˜ë©´ ì „ì²´ ì²˜ë¦¬ ì¤‘ë‹¨
                    rate_limit_error_occurred = True
                    break
                
                # Rate Limit ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš° ë°°ì¹˜ ë£¨í”„ ì¢…ë£Œ
                if rate_limit_error_occurred:
                    break
            
            # Rate Limit ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš° ì¬ì‹œë„
            if rate_limit_error_occurred:
                retry_count += 1
                if retry_count < max_retries:
                    # Backoff ì „ëµ ì‚¬ìš©
                    backoff = get_backoff_strategy()
                    wait_time, reason = backoff.calculate_backoff(retry_count - 1)
                    
                    print(f"\nâ³ Rate Limit ì´ˆê³¼ë¡œ ì „ì²´ ì‘ì—… ì¬ì‹œë„ ì¤‘...")
                    print(f"   ëŒ€ê¸° ì‹œê°„: {wait_time:.2f}ì´ˆ ({reason})")
                    print(f"   ì¬ì‹œë„: {retry_count}/{max_retries}")
                    
                    time.sleep(wait_time)
                    continue  # ì „ì²´ ì‘ì—… ì¬ì‹œë„
                else:
                    # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
                    print(f"\nâŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜.")
                    # ì‹¤íŒ¨í•œ ì‘ì—…ë“¤ë„ ì—ëŸ¬ ì •ë³´ë¡œ ì¶”ê°€
                    for future, (symbol, market) in futures.items():
                        if not future.done() or future.cancelled():
                            results.append({
                                'rt_cd': 'EGW00201',
                                'msg1': 'Rate limit exceeded - max retries reached',
                                'error': True,
                                'symbol': symbol,
                                'market': market,
                                'error_type': 'RateLimitError'
                            })
            
            # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ
            break
        
        # í†µê³„ ì¶œë ¥
        if hasattr(self.rate_limiter, 'print_stats'):
            self.rate_limiter.print_stats()
        
        # ì„±ê³µ/ì‹¤íŒ¨ ìš”ì•½
        success_count = sum(1 for r in results if not r.get('error', False))
        error_count = len(results) - success_count
        
        print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ - ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {error_count}")
        if error_count > 0:
            error_types = {}
            for r in results:
                if r.get('error'):
                    error_type = r.get('error_type', 'Unknown')
                    error_types[error_type] = error_types.get(error_type, 0) + 1
            print(f"   ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬: {error_types}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ í†µê³„
        if len(batches) > 1:
            print(f"   ë°°ì¹˜ ìˆ˜: {len(batches)}, ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ë™ì  ë°°ì¹˜ ì¡°ì • í†µê³„
        if dynamic_batch_controller:
            controller_stats = dynamic_batch_controller.get_stats()
            print(f"\nğŸ¯ ë™ì  ë°°ì¹˜ ì¡°ì • í†µê³„:")
            print(f"   ìµœì¢… ë°°ì¹˜ í¬ê¸°: {controller_stats['current_batch_size']}")
            print(f"   ìµœì¢… ëŒ€ê¸° ì‹œê°„: {controller_stats['current_batch_delay']:.1f}s")
            print(f"   íŒŒë¼ë¯¸í„° ì¡°ì • íšŸìˆ˜: {controller_stats['adjustment_count']}")
            print(f"   ëª©í‘œ ì—ëŸ¬ìœ¨: {controller_stats['target_error_rate']:.1%}")
            print(f"   ì‹¤ì œ ì—ëŸ¬ìœ¨: {controller_stats['overall_error_rate']:.1%}")
        
        return results
    
    def __handle_rate_limit_error(self, retry_count: int):
        """Rate limit ì—ëŸ¬ ì²˜ë¦¬ (Exponential Backoff)
        
        DEPRECATED: Enhanced Backoff Strategyë¡œ ëŒ€ì²´ë¨
        ì´ ë©”ì„œë“œëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë˜ë©°, í–¥í›„ ì œê±°ë  ì˜ˆì •ì…ë‹ˆë‹¤.
        
        Args:
            retry_count: ì¬ì‹œë„ íšŸìˆ˜ (0ë¶€í„° ì‹œì‘)
        """
        # Exponential backoff: 1, 2, 4, 8, 16, 32ì´ˆ
        wait_time = min(2 ** retry_count, 32)
        
        # Jitter ì¶”ê°€ (0~10% ëœë¤ ì¶”ê°€ ëŒ€ê¸°)
        jitter = random.uniform(0, 0.1 * wait_time)
        total_wait = wait_time + jitter
        
        print(f"Rate limit ì´ˆê³¼. {total_wait:.2f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... (ì‹œë„ {retry_count + 1}/5)")
        time.sleep(total_wait)

    def shutdown(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ThreadPoolExecutor ì¢…ë£Œ"""
        if hasattr(self, 'executor') and self.executor:
            print("ThreadPoolExecutor ì¢…ë£Œ ì¤‘...")
            self.executor.shutdown(wait=True)
            self.executor = None
            print("ThreadPoolExecutor ì¢…ë£Œ ì™„ë£Œ")
        
        # Rate limiter í†µê³„ ìµœì¢… ì¶œë ¥ ë° ì €ì¥
        if hasattr(self, 'rate_limiter'):
            if hasattr(self.rate_limiter, 'get_stats'):
                stats = self.rate_limiter.get_stats()
                if stats.get('total_calls', 0) > 0:
                    print(f"\nìµœì¢… Rate Limiter í†µê³„:")
                    print(f"- ì´ í˜¸ì¶œ ìˆ˜: {stats['total_calls']}")
                    print(f"- ì—ëŸ¬ ìˆ˜: {stats['error_count']}")
                    print(f"- ì—ëŸ¬ìœ¨: {stats['error_rate']:.1%}")
            
            # í†µê³„ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            if hasattr(self.rate_limiter, 'save_stats'):
                filepath = self.rate_limiter.save_stats(include_timestamp=True)
                if filepath:
                    print(f"- í†µê³„ ì €ì¥ë¨: {filepath}")
            
            # ìë™ ì €ì¥ ë¹„í™œì„±í™”
            if hasattr(self.rate_limiter, 'disable_auto_save'):
                self.rate_limiter.disable_auto_save()
        
        # Backoff ì „ëµ í†µê³„ ì¶œë ¥
        backoff_strategy = get_backoff_strategy()
        backoff_stats = backoff_strategy.get_stats()
        if backoff_stats['total_attempts'] > 0:
            print(f"\nìµœì¢… Backoff ì „ëµ í†µê³„:")
            print(f"- Circuit ìƒíƒœ: {backoff_stats['state']}")
            print(f"- ì´ ì‹œë„: {backoff_stats['total_attempts']}")
            print(f"- ì´ ì‹¤íŒ¨: {backoff_stats['total_failures']}")
            print(f"- ì„±ê³µë¥ : {backoff_stats['success_rate']:.1%}")
            print(f"- Circuit Open íšŸìˆ˜: {backoff_stats['circuit_opens']}")
            print(f"- í‰ê·  ë°±ì˜¤í”„ ì‹œê°„: {backoff_stats['avg_backoff_time']:.2f}ì´ˆ")
        
        # ìºì‹œ í†µê³„ ì¶œë ¥ (Phase 8.7)
        if self._cache_enabled and self._cache:
            cache_stats = self.get_cache_stats()
            if cache_stats['total_entries'] > 0 or cache_stats['hit_count'] > 0:
                print(f"\nìµœì¢… ìºì‹œ í†µê³„:")
                print(f"- í™œì„±í™” ì—¬ë¶€: {'ì˜ˆ' if cache_stats['enabled'] else 'ì•„ë‹ˆì˜¤'}")
                print(f"- ì´ í•­ëª© ìˆ˜: {cache_stats['total_entries']}")
                print(f"- ìºì‹œ ì ì¤‘: {cache_stats['hit_count']}")
                print(f"- ìºì‹œ ë¯¸ìŠ¤: {cache_stats['miss_count']}")
                print(f"- ì ì¤‘ë¥ : {cache_stats['hit_rate']:.1%}")
                print(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {cache_stats['memory_usage']:.1f}MB")
                print(f"- ë§Œë£Œëœ í•­ëª©: {cache_stats['expired_count']}")
                print(f"- ì œê±°ëœ í•­ëª©: {cache_stats['eviction_count']}")
        
        # ì—ëŸ¬ ë³µêµ¬ ì‹œìŠ¤í…œ í†µê³„ ì¶œë ¥
        recovery_system = get_error_recovery_system()
        error_summary = recovery_system.get_error_summary(hours=24)
        if error_summary['total_errors'] > 0:
            print(f"\nìµœì¢… ì—ëŸ¬ ë³µêµ¬ í†µê³„ (ìµœê·¼ 24ì‹œê°„):")
            print(f"- ì´ ì—ëŸ¬ ìˆ˜: {error_summary['total_errors']}")
            print(f"- ì‹¬ê°ë„ë³„ ë¶„í¬: {error_summary['by_severity']}")
            print(f"- ë³µêµ¬ ì„±ê³µë¥ : {error_summary['recovery_rate']:.1%}")
            print(f"- ê°€ì¥ ë¹ˆë²ˆí•œ ì—ëŸ¬:")
            for error_info in error_summary['most_common'][:3]:
                print(f"  - {error_info['error']}: {error_info['count']}íšŒ")
        
        # ì—ëŸ¬ í†µê³„ íŒŒì¼ë¡œ ì €ì¥
        recovery_system.save_stats()
        
        # í†µí•© í†µê³„ ì €ì¥ (Phase 5.1)
        print("\ní†µí•© í†µê³„ ì €ì¥ ì¤‘...")
        stats_manager = get_stats_manager()
        
        # DynamicBatchControllerê°€ ìˆë‹¤ë©´ í¬í•¨
        batch_controller = None
        if hasattr(self, '_dynamic_batch_controller'):
            batch_controller = self._dynamic_batch_controller
        
        # ëª¨ë“  ëª¨ë“ˆì˜ í†µê³„ ìˆ˜ì§‘
        all_stats = stats_manager.collect_all_stats(
            rate_limiter=self.rate_limiter if hasattr(self, 'rate_limiter') else None,
            backoff_strategy=backoff_strategy,
            error_recovery=recovery_system,
            batch_controller=batch_controller,
            cache=self._cache if self._cache_enabled and self._cache else None
        )
        
        # JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
        json_path = stats_manager.save_stats(all_stats, format='json', include_timestamp=True)
        print(f"- í†µí•© í†µê³„ ì €ì¥ë¨ (JSON): {json_path}")
        
        # CSV í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (ìš”ì•½ ì •ë³´)
        csv_path = stats_manager.save_stats(all_stats, format='csv', include_timestamp=True)
        print(f"- í†µí•© í†µê³„ ì €ì¥ë¨ (CSV): {csv_path}")
        
        # ì••ì¶•ëœ JSON Lines í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì¥ê¸° ë³´ê´€ìš©)
        jsonl_gz_path = stats_manager.save_stats(
            all_stats, 
            format='jsonl', 
            compress=True,
            filename='stats_history',
            include_timestamp=False
        )
        print(f"- í†µê³„ ì´ë ¥ ì¶”ê°€ë¨ (JSONL.GZ): {jsonl_gz_path}")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½ ì¶œë ¥
        summary = all_stats.get('summary', {})
        print(f"\nì‹œìŠ¤í…œ ìµœì¢… ìƒíƒœ: {summary.get('system_health', 'UNKNOWN')}")
        print(f"- ì „ì²´ API í˜¸ì¶œ: {summary.get('total_api_calls', 0):,}")
        print(f"- ì „ì²´ ì—ëŸ¬: {summary.get('total_errors', 0):,}")
        print(f"- ì „ì²´ ì—ëŸ¬ìœ¨: {summary.get('overall_error_rate', 0):.2%}")
        
        # ìºì‹œ ì •ë¦¬ (Phase 8.7)
        if self._cache_enabled and self._cache:
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì •ì§€
            if hasattr(self._cache, 'stop_cleanup_thread'):
                self._cache.stop_cleanup_thread()
            logger.info("ìºì‹œ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì •ë¦¬ ì™„ë£Œ")

    def set_base_url(self, mock: bool = True):
        """í…ŒìŠ¤íŠ¸(ëª¨ì˜íˆ¬ì) ì„œë²„ ì‚¬ìš© ì„¤ì •
        Args:
            mock(bool, optional): True: í…ŒìŠ¤íŠ¸ì„œë²„, False: ì‹¤ì„œë²„ Defaults to True.
        """
        if mock:
            self.base_url = "https://openapivts.koreainvestment.com:29443"
        else:
            self.base_url = "https://openapi.koreainvestment.com:9443"

    @retry_on_rate_limit(max_retries=3)  # í† í° ë°œê¸‰ì€ 3íšŒë§Œ ì¬ì‹œë„
    def issue_access_token(self):
        """OAuthì¸ì¦/ì ‘ê·¼í† í°ë°œê¸‰
        """
        path = "oauth2/tokenP"
        url = f"{self.base_url}/{path}"
        headers = {"content-type": "application/json"}
        data = {
            "grant_type": "client_credentials",
            "appkey": self.api_key,
            "appsecret": self.api_secret
        }

        resp = requests.post(url, headers=headers, json=data)
        resp_data = resp.json()
        self.access_token = f'Bearer {resp_data["access_token"]}'

        # 'expires_in' has no reference time and causes trouble:
        # The server thinks I'm expired but my token.dat looks still valid!
        # Hence, we use 'access_token_token_expired' here.
        # This error is quite big. I've seen 4000 seconds.
        timezone = ZoneInfo('Asia/Seoul')
        dt = datetime.strptime(resp_data['access_token_token_expired'], '%Y-%m-%d %H:%M:%S').replace(
            tzinfo=timezone)
        resp_data['timestamp'] = int(dt.timestamp())
        resp_data['api_key'] = self.api_key
        resp_data['api_secret'] = self.api_secret

        # dump access token
        self.token_file.parent.mkdir(parents=True, exist_ok=True)
        with self.token_file.open("wb") as f:
            pickle.dump(resp_data, f)

    def check_access_token(self) -> bool:
        """check access token

        Returns:
            Bool: True: token is valid, False: token is not valid
        """

        if not self.token_file.exists():
            return False

        with self.token_file.open("rb") as f:
            data = pickle.load(f)

        expire_epoch = data['timestamp']
        now_epoch = int(datetime.now().timestamp())
        status = False

        if (data['api_key'] != self.api_key) or (data['api_secret'] != self.api_secret):
            return False

        good_until = data['timestamp']
        ts_now = int(datetime.now().timestamp())
        return ts_now < good_until

    def load_access_token(self):
        """load access token
        """
        with self.token_file.open("rb") as f:
            data = pickle.load(f)
        self.access_token = f'Bearer {data["access_token"]}'

    def issue_hashkey(self, data: dict):
        """í•´ì‰¬í‚¤ ë°œê¸‰
        Args:
            data (dict): POST ìš”ì²­ ë°ì´í„°
        Returns:
            _type_: _description_
        """
        path = "uapi/hashkey"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "User-Agent": "Mozilla/5.0"
        }
        resp = requests.post(url, headers=headers, data=json.dumps(data))
        haskkey = resp.json()["HASH"]
        return haskkey

    def fetch_search_stock_info_list(self, stock_market_list):
        return self.__execute_concurrent_requests_with_cache(self.__fetch_search_stock_info, stock_market_list)

    def fetch_price_list(self, stock_list):
        return self.__execute_concurrent_requests_with_cache(self.__fetch_price, stock_list)

    def fetch_price_list_with_batch(self, stock_list, batch_size=50, batch_delay=1.0, progress_interval=10):
        """ê°€ê²© ëª©ë¡ ì¡°íšŒ (ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›)
        
        Args:
            stock_list: (symbol, market) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 50)
            batch_delay: ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 1.0)
            progress_interval: ì§„í–‰ ìƒí™© ì¶œë ¥ ê°„ê²© (ê¸°ë³¸ê°’: 10)
        
        Returns:
            list: ì¡°íšŒ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        return self.__execute_concurrent_requests(
            self.__fetch_price, 
            stock_list,
            batch_size=batch_size,
            batch_delay=batch_delay,
            progress_interval=progress_interval
        )
    
    def fetch_price_list_with_dynamic_batch(self, stock_list, dynamic_batch_controller=None):
        """ê°€ê²© ëª©ë¡ ì¡°íšŒ (ë™ì  ë°°ì¹˜ ì¡°ì •)
        
        Args:
            stock_list: (symbol, market) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            dynamic_batch_controller: DynamicBatchController ì¸ìŠ¤í„´ìŠ¤
                                     (Noneì´ë©´ ìë™ ìƒì„±)
        
        Returns:
            list: ì¡°íšŒ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if dynamic_batch_controller is None:
            from .batch_processing.dynamic_batch_controller import DynamicBatchController
            dynamic_batch_controller = DynamicBatchController(
                initial_batch_size=50,
                initial_batch_delay=1.0,
                target_error_rate=0.01
            )
        
        return self.__execute_concurrent_requests(
            self.__fetch_price,
            stock_list,
            dynamic_batch_controller=dynamic_batch_controller
        )

    def __fetch_price(self, symbol: str, market: str = "KR") -> dict:
        """êµ­ë‚´ì£¼ì‹ì‹œì„¸/ì£¼ì‹í˜„ì¬ê°€ ì‹œì„¸
           í•´ì™¸ì£¼ì‹í˜„ì¬ê°€/í•´ì™¸ì£¼ì‹ í˜„ì¬ì²´ê²°ê°€

        Args:
            symbol (str): ì¢…ëª©ì½”ë“œ

        Returns:
            dict: _description_
        """

        if market == "KR" or market == "KRX":
            stock_info = self.__fetch_stock_info(symbol, market)
            symbol_type = self.__get_symbol_type(stock_info)
            if symbol_type == "ETF":
                resp_json = self.fetch_etf_domestic_price("J", symbol)
            else:
                resp_json = self.fetch_domestic_price("J", symbol)
        elif market == "US":
            resp_json = self.fetch_oversea_price(symbol)
        else:
            raise ValueError("Unsupported market type")

        return resp_json

    def __get_symbol_type(self, symbol_info):
        symbol_type = symbol_info['output']['prdt_clsf_name']
        if symbol_type == 'ì£¼ê¶Œ' or symbol_type == 'ìƒì¥REITS' or symbol_type == 'ì‚¬íšŒê°„ì ‘ìë³¸íˆ¬ìœµìíšŒì‚¬':
            return 'Stock'
        elif symbol_type == 'ETF':
            return 'ETF'

        return "Unknown"

    @cacheable(
        ttl=300,  # 5ë¶„
        key_generator=lambda self, market_code, symbol: f"fetch_etf_domestic_price:{market_code}:{symbol}"
    )
    @retry_on_rate_limit()
    def fetch_etf_domestic_price(self, market_code: str, symbol: str) -> dict:
        """ì£¼ì‹í˜„ì¬ê°€ì‹œì„¸
        Args:
            market_code (str): ì‹œì¥ ë¶„ë¥˜ì½”ë“œ
            symbol (str): ì¢…ëª©ì½”ë“œ
        Returns:
            dict: API ê°œë°œ ê°€ì´ë“œ ì°¸ì¡°
        """
        path = "uapi/domestic-stock/v1/quotations/inquire-price"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "FHPST02400000"
        }
        params = {
            "fid_cond_mrkt_div_code": market_code,
            "fid_input_iscd": symbol
        }
        resp = requests.get(url, headers=headers, params=params)
        return resp.json()

    @cacheable(
        ttl=300,  # 5ë¶„
        key_generator=lambda self, market_code, symbol: f"fetch_domestic_price:{market_code}:{symbol}"
    )
    @retry_on_rate_limit()
    def fetch_domestic_price(self, market_code: str, symbol: str) -> dict:
        """ì£¼ì‹í˜„ì¬ê°€ì‹œì„¸
        Args:
            market_code (str): ì‹œì¥ ë¶„ë¥˜ì½”ë“œ
            symbol (str): ì¢…ëª©ì½”ë“œ
        Returns:
            dict: API ê°œë°œ ê°€ì´ë“œ ì°¸ì¡°
        """
        path = "uapi/domestic-stock/v1/quotations/inquire-price"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "FHKST01010100"
        }
        params = {
            "fid_cond_mrkt_div_code": market_code,
            "fid_input_iscd": symbol
        }
        resp = requests.get(url, headers=headers, params=params)
        return resp.json()

    @cacheable(
        ttl=259200,  # 3ì¼
        key_generator=lambda self: "fetch_kospi_symbols"
    )
    def fetch_kospi_symbols(self):
        """ì½”ìŠ¤í”¼ ì¢…ëª© ì½”ë“œ

        ì‹¤ì œ í•„ìš”í•œ ì¢…ëª©: ST, RT, EF, IF

        ST	ì£¼ê¶Œ
        MF	ì¦ê¶Œíˆ¬ìíšŒì‚¬
        RT	ë¶€ë™ì‚°íˆ¬ìíšŒì‚¬
        SC	ì„ ë°•íˆ¬ìíšŒì‚¬
        IF	ì‚¬íšŒê°„ì ‘ìë³¸íˆ¬ìœµìíšŒì‚¬
        DR	ì£¼ì‹ì˜ˆíƒì¦ì„œ
        EW	ELW
        EF	ETF
        SW	ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ê¶Œ
        SR	ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œ
        BC	ìˆ˜ìµì¦ê¶Œ
        FE	í•´ì™¸ETF
        FS	ì™¸êµ­ì£¼ê¶Œ


        Returns:
            DataFrame:
        """
        base_dir = os.getcwd()
        file_name = "kospi_code.mst.zip"
        url = "https://new.real.download.dws.co.kr/common/master/" + file_name
        self.download_master_file(base_dir, file_name, url)
        df = self.parse_kospi_master(base_dir)
        return df

    @cacheable(
        ttl=259200,  # 3ì¼
        key_generator=lambda self: "fetch_kosdaq_symbols"
    )
    def fetch_kosdaq_symbols(self):
        """ì½”ìŠ¤ë‹¥ ì¢…ëª© ì½”ë“œ

        Returns:
            DataFrame:
        """
        base_dir = os.getcwd()
        file_name = "kosdaq_code.mst.zip"
        url = "https://new.real.download.dws.co.kr/common/master/" + file_name
        self.download_master_file(base_dir, file_name, url)
        df = self.parse_kosdaq_master(base_dir)
        return df

    def fetch_symbols(self):
        """fetch symbols from the exchange

        Returns:
            pd.DataFrame: pandas dataframe
        """
        if self.exchange == "ì„œìš¸":  # todo: exchangeëŠ” ì œê±° ì˜ˆì •
            df = self.fetch_kospi_symbols()
            kospi_df = df[['ë‹¨ì¶•ì½”ë“œ', 'í•œê¸€ëª…', 'ê·¸ë£¹ì½”ë“œ']].copy()
            kospi_df['ì‹œì¥'] = 'ì½”ìŠ¤í”¼'

            df = self.fetch_kosdaq_symbols()
            kosdaq_df = df[['ë‹¨ì¶•ì½”ë“œ', 'í•œê¸€ëª…', 'ê·¸ë£¹ì½”ë“œ']].copy()
            kosdaq_df['ì‹œì¥'] = 'ì½”ìŠ¤ë‹¥'

            df = pd.concat([kospi_df, kosdaq_df], axis=0)

        return df

    def download_master_file(self, base_dir: str, file_name: str, url: str):
        """download master file

        Args:
            base_dir (str): download directory
            file_name (str: filename
            url (str): url
        """
        os.chdir(base_dir)

        # delete legacy master file
        if os.path.exists(file_name):
            os.remove(file_name)

        # download master file
        resp = requests.get(url)
        with open(file_name, "wb") as f:
            f.write(resp.content)

        # unzip
        kospi_zip = zipfile.ZipFile(file_name)
        kospi_zip.extractall()
        kospi_zip.close()

    def parse_kospi_master(self, base_dir: str):
        """parse kospi master file

        Args:
            base_dir (str): directory where kospi code exists

        Returns:
            _type_: _description_
        """
        file_name = base_dir + "/kospi_code.mst"
        tmp_fil1 = base_dir + "/kospi_code_part1.tmp"
        tmp_fil2 = base_dir + "/kospi_code_part2.tmp"

        wf1 = open(tmp_fil1, mode="w", encoding="cp949")
        wf2 = open(tmp_fil2, mode="w")

        with open(file_name, mode="r", encoding="cp949") as f:
            for row in f:
                rf1 = row[0:len(row) - 228]
                rf1_1 = rf1[0:9].rstrip()
                rf1_2 = rf1[9:21].rstrip()
                rf1_3 = rf1[21:].strip()
                wf1.write(rf1_1 + ',' + rf1_2 + ',' + rf1_3 + '\n')
                rf2 = row[-228:]
                wf2.write(rf2)

        wf1.close()
        wf2.close()

        part1_columns = ['ë‹¨ì¶•ì½”ë“œ', 'í‘œì¤€ì½”ë“œ', 'í•œê¸€ëª…']
        df1 = pd.read_csv(tmp_fil1, header=None, encoding='cp949', names=part1_columns)

        field_specs = [
            2, 1, 4, 4, 4,
            1, 1, 1, 1, 1,
            1, 1, 1, 1, 1,
            1, 1, 1, 1, 1,
            1, 1, 1, 1, 1,
            1, 1, 1, 1, 1,
            1, 9, 5, 5, 1,
            1, 1, 2, 1, 1,
            1, 2, 2, 2, 3,
            1, 3, 12, 12, 8,
            15, 21, 2, 7, 1,
            1, 1, 1, 1, 9,
            9, 9, 5, 9, 8,
            9, 3, 1, 1, 1
        ]

        part2_columns = [
            'ê·¸ë£¹ì½”ë“œ', 'ì‹œê°€ì´ì•¡ê·œëª¨', 'ì§€ìˆ˜ì—…ì¢…ëŒ€ë¶„ë¥˜', 'ì§€ìˆ˜ì—…ì¢…ì¤‘ë¶„ë¥˜', 'ì§€ìˆ˜ì—…ì¢…ì†Œë¶„ë¥˜',
            'ì œì¡°ì—…', 'ì €ìœ ë™ì„±', 'ì§€ë°°êµ¬ì¡°ì§€ìˆ˜ì¢…ëª©', 'KOSPI200ì„¹í„°ì—…ì¢…', 'KOSPI100',
            'KOSPI50', 'KRX', 'ETP', 'ELWë°œí–‰', 'KRX100',
            'KRXìë™ì°¨', 'KRXë°˜ë„ì²´', 'KRXë°”ì´ì˜¤', 'KRXì€í–‰', 'SPAC',
            'KRXì—ë„ˆì§€í™”í•™', 'KRXì² ê°•', 'ë‹¨ê¸°ê³¼ì—´', 'KRXë¯¸ë””ì–´í†µì‹ ', 'KRXê±´ì„¤',
            'Non1', 'KRXì¦ê¶Œ', 'KRXì„ ë°•', 'KRXì„¹í„°_ë³´í—˜', 'KRXì„¹í„°_ìš´ì†¡',
            'SRI', 'ê¸°ì¤€ê°€', 'ë§¤ë§¤ìˆ˜ëŸ‰ë‹¨ìœ„', 'ì‹œê°„ì™¸ìˆ˜ëŸ‰ë‹¨ìœ„', 'ê±°ë˜ì •ì§€',
            'ì •ë¦¬ë§¤ë§¤', 'ê´€ë¦¬ì¢…ëª©', 'ì‹œì¥ê²½ê³ ', 'ê²½ê³ ì˜ˆê³ ', 'ë¶ˆì„±ì‹¤ê³µì‹œ',
            'ìš°íšŒìƒì¥', 'ë½êµ¬ë¶„', 'ì•¡ë©´ë³€ê²½', 'ì¦ìêµ¬ë¶„', 'ì¦ê±°ê¸ˆë¹„ìœ¨',
            'ì‹ ìš©ê°€ëŠ¥', 'ì‹ ìš©ê¸°ê°„', 'ì „ì¼ê±°ë˜ëŸ‰', 'ì•¡ë©´ê°€', 'ìƒì¥ì¼ì',
            'ìƒì¥ì£¼ìˆ˜', 'ìë³¸ê¸ˆ', 'ê²°ì‚°ì›”', 'ê³µëª¨ê°€', 'ìš°ì„ ì£¼',
            'ê³µë§¤ë„ê³¼ì—´', 'ì´ìƒê¸‰ë“±', 'KRX300', 'KOSPI', 'ë§¤ì¶œì•¡',
            'ì˜ì—…ì´ìµ', 'ê²½ìƒì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ', 'ROE', 'ê¸°ì¤€ë…„ì›”',
            'ì‹œê°€ì´ì•¡', 'ê·¸ë£¹ì‚¬ì½”ë“œ', 'íšŒì‚¬ì‹ ìš©í•œë„ì´ˆê³¼', 'ë‹´ë³´ëŒ€ì¶œê°€ëŠ¥', 'ëŒ€ì£¼ê°€ëŠ¥'
        ]

        df2 = pd.read_fwf(tmp_fil2, widths=field_specs, names=part2_columns)
        df = pd.merge(df1, df2, how='outer', left_index=True, right_index=True)

        # clean temporary file and dataframe
        del (df1)
        del (df2)
        os.remove(tmp_fil1)
        os.remove(tmp_fil2)
        return df

    def parse_kosdaq_master(self, base_dir: str):
        """parse kosdaq master file

        Args:
            base_dir (str): directory where kosdaq code exists

        Returns:
            _type_: _description_
        """
        file_name = base_dir + "/kosdaq_code.mst"
        tmp_fil1 = base_dir + "/kosdaq_code_part1.tmp"
        tmp_fil2 = base_dir + "/kosdaq_code_part2.tmp"

        wf1 = open(tmp_fil1, mode="w", encoding="cp949")
        wf2 = open(tmp_fil2, mode="w")
        with open(file_name, mode="r", encoding="cp949") as f:
            for row in f:
                rf1 = row[0:len(row) - 222]
                rf1_1 = rf1[0:9].rstrip()
                rf1_2 = rf1[9:21].rstrip()
                rf1_3 = rf1[21:].strip()
                wf1.write(rf1_1 + ',' + rf1_2 + ',' + rf1_3 + '\n')

                rf2 = row[-222:]
                wf2.write(rf2)

        wf1.close()
        wf2.close()

        part1_columns = ['ë‹¨ì¶•ì½”ë“œ', 'í‘œì¤€ì½”ë“œ', 'í•œê¸€ëª…']
        df1 = pd.read_csv(tmp_fil1, header=None, encoding="cp949", names=part1_columns)

        field_specs = [
            2, 1, 4, 4, 4,  # line 20
            1, 1, 1, 1, 1,  # line 27
            1, 1, 1, 1, 1,  # line 32
            1, 1, 1, 1, 1,  # line 38
            1, 1, 1, 1, 1,  # line 43
            1, 9, 5, 5, 1,  # line 48
            1, 1, 2, 1, 1,  # line 54
            1, 2, 2, 2, 3,  # line 64
            1, 3, 12, 12, 8,  # line 69
            15, 21, 2, 7, 1,  # line 75
            1, 1, 1, 9, 9,  # line 80
            9, 5, 9, 8, 9,  # line 85
            3, 1, 1, 1
        ]

        part2_columns = [
            'ê·¸ë£¹ì½”ë“œ', 'ì‹œê°€ì´ì•¡ê·œëª¨', 'ì§€ìˆ˜ì—…ì¢…ëŒ€ë¶„ë¥˜', 'ì§€ìˆ˜ì—…ì¢…ì¤‘ë¶„ë¥˜', 'ì§€ìˆ˜ì—…ì¢…ì†Œë¶„ë¥˜',  # line 20
            'ë²¤ì²˜ê¸°ì—…', 'ì €ìœ ë™ì„±', 'KRX', 'ETP', 'KRX100',  # line 27
            'KRXìë™ì°¨', 'KRXë°˜ë„ì²´', 'KRXë°”ì´ì˜¤', 'KRXì€í–‰', 'SPAC',  # line 32
            'KRXì—ë„ˆì§€í™”í•™', 'KRXì² ê°•', 'ë‹¨ê¸°ê³¼ì—´', 'KRXë¯¸ë””ì–´í†µì‹ ', 'KRXê±´ì„¤',  # line 38
            'íˆ¬ìì£¼ì˜', 'KRXì¦ê¶Œ', 'KRXì„ ë°•', 'KRXì„¹í„°_ë³´í—˜', 'KRXì„¹í„°_ìš´ì†¡',  # line 43
            'KOSDAQ150', 'ê¸°ì¤€ê°€', 'ë§¤ë§¤ìˆ˜ëŸ‰ë‹¨ìœ„', 'ì‹œê°„ì™¸ìˆ˜ëŸ‰ë‹¨ìœ„', 'ê±°ë˜ì •ì§€',  # line 48
            'ì •ë¦¬ë§¤ë§¤', 'ê´€ë¦¬ì¢…ëª©', 'ì‹œì¥ê²½ê³ ', 'ê²½ê³ ì˜ˆê³ ', 'ë¶ˆì„±ì‹¤ê³µì‹œ',  # line 54
            'ìš°íšŒìƒì¥', 'ë½êµ¬ë¶„', 'ì•¡ë©´ë³€ê²½', 'ì¦ìêµ¬ë¶„', 'ì¦ê±°ê¸ˆë¹„ìœ¨',  # line 64
            'ì‹ ìš©ê°€ëŠ¥', 'ì‹ ìš©ê¸°ê°„', 'ì „ì¼ê±°ë˜ëŸ‰', 'ì•¡ë©´ê°€', 'ìƒì¥ì¼ì',  # line 69
            'ìƒì¥ì£¼ìˆ˜', 'ìë³¸ê¸ˆ', 'ê²°ì‚°ì›”', 'ê³µëª¨ê°€', 'ìš°ì„ ì£¼',  # line 75
            'ê³µë§¤ë„ê³¼ì—´', 'ì´ìƒê¸‰ë“±', 'KRX300', 'ë§¤ì¶œì•¡', 'ì˜ì—…ì´ìµ',  # line 80
            'ê²½ìƒì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ', 'ROE', 'ê¸°ì¤€ë…„ì›”', 'ì‹œê°€ì´ì•¡',  # line 85
            'ê·¸ë£¹ì‚¬ì½”ë“œ', 'íšŒì‚¬ì‹ ìš©í•œë„ì´ˆê³¼', 'ë‹´ë³´ëŒ€ì¶œê°€ëŠ¥', 'ëŒ€ì£¼ê°€ëŠ¥'
        ]

        df2 = pd.read_fwf(tmp_fil2, widths=field_specs, names=part2_columns)
        df = pd.merge(df1, df2, how='outer', left_index=True, right_index=True)

        # clean temporary file and dataframe
        del (df1)
        del (df2)
        os.remove(tmp_fil1)
        os.remove(tmp_fil2)
        return df

    def fetch_price_detail_oversea_list(self, stock_market_list):
        return self.__execute_concurrent_requests_with_cache(self.__fetch_price_detail_oversea, stock_market_list)

    @cacheable(
        ttl=300,  # 5ë¶„ 
        key_generator=lambda self, symbol, market: f"fetch_price_detail_oversea:{market}:{symbol}"
    )
    @retry_on_rate_limit()
    def __fetch_price_detail_oversea(self, symbol: str, market: str = "KR"):
        """í•´ì™¸ì£¼ì‹ í˜„ì¬ê°€ìƒì„¸

        Args:
            symbol (str): symbol
        """
        self.rate_limiter.acquire()

        path = "/uapi/overseas-price/v1/quotations/price-detail"
        url = f"{self.base_url}/{path}"

        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "HHDFS76200200"
        }

        if market == "KR" or market == "KRX":
            # API í˜¸ì¶œí•´ì„œ ì‹¤ì œë¡œ í™•ì¸ì€ ëª»í•´ë´„, overasea ì´ë¼ì„œ ì•ˆë  ê²ƒìœ¼ë¡œ íŒë‹¨í•´ì„œ ì¡°ê±´ë¬¸ ì¶”ê°€í•¨
            raise ValueError("Market cannot be either 'KR' or 'KRX'.")

        for market_code in MARKET_TYPE_MAP[market]:
            print("market_code", market_code)
            market_type = MARKET_CODE_MAP[market_code]
            exchange_code = EXCHANGE_CODE_MAP[market_type]
            params = {
                "AUTH": "",
                "EXCD": exchange_code,
                "SYMB": symbol
            }
            resp = requests.get(url, headers=headers, params=params)
            resp_json = resp.json()
            if resp_json['rt_cd'] != API_RETURN_CODE["SUCCESS"] or resp_json['output']['rsym'] == '':
                continue

            return resp_json

    def fetch_stock_info_list(self, stock_market_list):
        return self.__execute_concurrent_requests_with_cache(self.__fetch_stock_info, stock_market_list)

    @cacheable(
        ttl=18000,  # 5ì‹œê°„
        key_generator=lambda self, symbol, market: f"fetch_stock_info:{market}:{symbol}"
    )
    @retry_on_rate_limit()
    def __fetch_stock_info(self, symbol: str, market: str = "KR"):
        self.rate_limiter.acquire()

        path = "uapi/domestic-stock/v1/quotations/search-info"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "CTPF1604R"
        }

        for market_code in MARKET_TYPE_MAP[market]:
            try:
                params = {
                    "PDNO": symbol,
                    "PRDT_TYPE_CD": market_code
                }
                resp = requests.get(url, headers=headers, params=params)
                resp_json = resp.json()

                if resp_json['rt_cd'] == API_RETURN_CODE['NO_DATA']:
                    continue
                return resp_json

            except Exception as e:
                print(e)
                if resp_json['rt_cd'] != API_RETURN_CODE['SUCCESS']:
                    continue
                raise e

    def fetch_search_stock_info_list(self, stock_market_list):
        return self.__execute_concurrent_requests_with_cache(self.__fetch_search_stock_info, stock_market_list)

    @cacheable(
        ttl=18000,  # 5ì‹œê°„
        key_generator=lambda self, symbol, market: f"fetch_search_stock_info:{market}:{symbol}"
    )
    @retry_on_rate_limit()
    def __fetch_search_stock_info(self, symbol: str, market: str = "KR"):
        """
        êµ­ë‚´ ì£¼ì‹ë§Œ ì œê³µí•˜ëŠ” APIì´ë‹¤
        """

        self.rate_limiter.acquire()

        path = "uapi/domestic-stock/v1/quotations/search-stock-info"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "CTPF1002R"
        }

        if market != "KR" and market != "KRX":
            raise ValueError("Market must be either 'KR' or 'KRX'.")

        for market_ in MARKET_TYPE_MAP[market]:
            try:
                params = {
                    "PDNO": symbol,
                    "PRDT_TYPE_CD": market_
                }
                resp = requests.get(url, headers=headers, params=params)
                resp_json = resp.json()

                if resp_json['rt_cd'] == API_RETURN_CODE['NO_DATA']:
                    continue
                return resp_json

            except Exception as e:
                print(e)
                if resp_json['rt_cd'] != API_RETURN_CODE['SUCCESS']:
                    continue
                raise e

    def __execute_concurrent_requests_with_cache(self, method, stock_list,
                                                  batch_size: Optional[int] = None,
                                                  batch_delay: float = 0.0,
                                                  progress_interval: int = 10,
                                                  use_cache: bool = True):
        """ìºì‹œë¥¼ í™œìš©í•œ ë³‘ë ¬ ìš”ì²­ ì‹¤í–‰
        
        Phase 8.4: ë¦¬ìŠ¤íŠ¸ ë©”ì„œë“œ ìºì‹œ ì²˜ë¦¬
        
        Args:
            method: ì‹¤í–‰í•  ë©”ì„œë“œ
            stock_list: (symbol, market) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ì „ì²´ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬)
            batch_delay: ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            progress_interval: ì§„í–‰ ìƒí™© ì¶œë ¥ ê°„ê²©
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            list: ì¡°íšŒ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not use_cache or not hasattr(self, '_cache') or not self._cache_enabled:
            # ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ê¸°ì¡´ ë©”ì„œë“œ ì‚¬ìš©
            return self.__execute_concurrent_requests(
                method, stock_list, batch_size, batch_delay, progress_interval
            )
        
        # ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        cached_results = {}
        uncached_items = []
        result_order = []  # ì›ë˜ ìˆœì„œ ìœ ì§€ìš©
        
        # 1. ìºì‹œì—ì„œ ë¨¼ì € ì¡°íšŒ
        cache_hits = 0
        cache_misses = 0
        
        for idx, (symbol, market) in enumerate(stock_list):
            result_order.append((idx, symbol, market))
            
            # ìºì‹œ í‚¤ ìƒì„±
            method_name = getattr(method, '__name__', str(method)).replace('_KoreaInvestment__', '')
            cache_key = f"{method_name}:{market}:{symbol}"
            
            # ìºì‹œì—ì„œ ì¡°íšŒ
            cached_value = self._cache.get(cache_key)
            if cached_value is not None:
                cached_results[idx] = cached_value
                cache_hits += 1
            else:
                uncached_items.append((symbol, market))
                cache_misses += 1
        
        if cache_hits > 0:
            print(f"ğŸ’¾ ìºì‹œ ì ì¤‘: {cache_hits}ê°œ ({cache_hits/len(stock_list)*100:.1f}%), "
                  f"ìºì‹œ ë¯¸ìŠ¤: {cache_misses}ê°œ ({cache_misses/len(stock_list)*100:.1f}%)")
        
        # 2. ìºì‹œë˜ì§€ ì•Šì€ í•­ëª©ë§Œ API í˜¸ì¶œ
        api_results = []
        if uncached_items:
            print(f"ğŸ”„ API í˜¸ì¶œ í•„ìš”: {len(uncached_items)}ê°œ í•­ëª©")
            api_results = self.__execute_concurrent_requests(
                method, uncached_items, batch_size, batch_delay, progress_interval
            )
            
            # 3. API í˜¸ì¶œ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            method_name = getattr(method, '__name__', str(method)).replace('_KoreaInvestment__', '')
            for (symbol, market), result in zip(uncached_items, api_results):
                # ì„±ê³µí•œ ê²½ìš°ë§Œ ìºì‹±
                if result.get('rt_cd') == '0':
                    cache_key = f"{method_name}:{market}:{symbol}"
                    # ë©”ì„œë“œë³„ TTLì€ cacheable ë°ì½”ë ˆì´í„°ì—ì„œ ì²˜ë¦¬
                    # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ TTL ì‚¬ìš©
                    self._cache.set(cache_key, result)
        
        # 4. ì „ì²´ ê²°ê³¼ ì¡°í•© (ì›ë˜ ìˆœì„œëŒ€ë¡œ)
        final_results = []
        api_result_idx = 0
        
        for idx, symbol, market in result_order:
            if idx in cached_results:
                # ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©
                final_results.append(cached_results[idx])
            else:
                # API í˜¸ì¶œ ê²°ê³¼ ì‚¬ìš©
                if api_result_idx < len(api_results):
                    final_results.append(api_results[api_result_idx])
                    api_result_idx += 1
                else:
                    # ì˜ˆì™¸ì ì¸ ê²½ìš° (API í˜¸ì¶œ ì‹¤íŒ¨ ë“±)
                    final_results.append({
                        'rt_cd': '9',
                        'error': True,
                        'symbol': symbol,
                        'market': market,
                        'msg1': 'Failed to fetch data'
                    })
        
        # ìºì‹œ í†µê³„ ì—…ë°ì´íŠ¸
        if hasattr(self._cache, 'print_stats') and cache_hits > 0:
            print(f"ğŸ“Š API í˜¸ì¶œ ì ˆê°: {cache_hits}íšŒ")
        
        return final_results
    
    # Phase 8.6: ìºì‹œ ê´€ë¦¬ ë©”ì„œë“œ
    def clear_cache(self, pattern: Optional[str] = None):
        """ìºì‹œ ì‚­ì œ
        
        Args:
            pattern: ì‚­ì œí•  ìºì‹œ í‚¤ íŒ¨í„´ (Noneì´ë©´ ì „ì²´ ì‚­ì œ)
                    ì˜ˆ: "fetch_domestic_price:J:005930"
        """
        if not self._cache_enabled or not self._cache:
            return
        
        if pattern is None:
            # ì „ì²´ ìºì‹œ ì‚­ì œ
            self._cache.clear()
            logger.info("ì „ì²´ ìºì‹œ ì‚­ì œ ì™„ë£Œ")
        else:
            # íŒ¨í„´ì— ë§ëŠ” ìºì‹œ ì‚­ì œ
            deleted_count = self._cache.delete_pattern(pattern)
            logger.info(f"{pattern} íŒ¨í„´ì˜ ìºì‹œ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ")
    
    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì¡°íšŒ
        
        Returns:
            dict: ìºì‹œ í†µê³„ ì •ë³´
        """
        if not self._cache_enabled or not self._cache:
            return {
                'enabled': False,
                'hit_rate': 0.0,
                'total_entries': 0,
                'memory_usage': 0,
                'expired_count': 0
            }
        
        stats = self._cache.get_stats()
        return {
            'enabled': True,
            'hit_rate': stats.get('hit_rate', 0.0),
            'total_entries': stats.get('size', 0),
            'memory_usage': stats.get('memory_usage_mb', 0),
            'expired_count': stats.get('expired_count', 0),
            'hit_count': stats.get('hit_count', 0),
            'miss_count': stats.get('miss_count', 0),
            'eviction_count': stats.get('eviction_count', 0)
        }
    
    def set_cache_enabled(self, enabled: bool):
        """ìºì‹œ ê¸°ëŠ¥ on/off
        
        Args:
            enabled: Trueë©´ ìºì‹œ í™œì„±í™”, Falseë©´ ë¹„í™œì„±í™”
        """
        self._cache_enabled = enabled
        logger.info(f"ìºì‹œ {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
    
    def preload_cache(self, symbols: List[str], market: str = "KR"):
        """ìì£¼ ì‚¬ìš©í•˜ëŠ” ì¢…ëª© ë¯¸ë¦¬ ìºì‹±
        
        Args:
            symbols: ì¢…ëª© ì½”ë“œ ë¦¬ìŠ¤íŠ¸
            market: ì‹œì¥ ì½”ë“œ (ê¸°ë³¸ê°’: "KR")
        """
        if not self._cache_enabled or not self._cache:
            logger.warning("ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ preloadë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        print(f"ğŸ”„ {len(symbols)}ê°œ ì¢…ëª© ìºì‹œ ì‚¬ì „ ë¡œë“œ ì‹œì‘...")
        
        # ì¢…ëª© ì •ë³´ ë¡œë“œ
        stock_info_list = [(symbol, market) for symbol in symbols]
        self.fetch_stock_info_list(stock_info_list)
        
        # í˜„ì¬ê°€ ì •ë³´ ë¡œë“œ
        price_list = [(symbol, market) for symbol in symbols]
        self.fetch_price_list(price_list)
        
        print(f"âœ… {len(symbols)}ê°œ ì¢…ëª© ìºì‹œ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ")
        
        # ìºì‹œ í†µê³„ ì¶œë ¥
        stats = self.get_cache_stats()
        print(f"ğŸ“Š ìºì‹œ ìƒíƒœ: {stats['total_entries']}ê°œ í•­ëª©, "
              f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats['memory_usage']:.1f}MB")
    
    # Visualization ë©”ì„œë“œë“¤
    def create_monitoring_dashboard(self, 
                                  stats_dir: str = "logs/integrated_stats",
                                  update_interval: int = 5000) -> Optional[Any]:
        """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±
        
        Args:
            stats_dir: í†µê³„ íŒŒì¼ ë””ë ‰í† ë¦¬
            update_interval: ì—…ë°ì´íŠ¸ ê°„ê²© (ë°€ë¦¬ì´ˆ)
            
        Returns:
            ëŒ€ì‹œë³´ë“œ Figure ê°ì²´ ë˜ëŠ” None
        """
        if not self.dashboard_manager:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # ë°ì´í„° ë¡œë“œ
            self.visualizer.stats_dir = Path(stats_dir)
            self.visualizer.load_history_data()
            self.visualizer.load_latest_stats()
            
            # ëŒ€ì‹œë³´ë“œ ìƒì„±
            dashboard = self.dashboard_manager.create_realtime_dashboard(update_interval)
            
            if dashboard:
                logger.info("ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„± ì™„ë£Œ")
            
            return dashboard
            
        except Exception as e:
            logger.error(f"ëŒ€ì‹œë³´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def save_monitoring_dashboard(self, 
                                filename: str = "api_monitoring_dashboard.html") -> bool:
        """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            filename: ì €ì¥í•  íŒŒì¼ëª…
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.dashboard_manager:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            path = self.dashboard_manager.save_dashboard(filename)
            return bool(path)
        except Exception as e:
            logger.error(f"ëŒ€ì‹œë³´ë“œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def create_stats_report(self, save_as: str = "monitoring_report") -> Dict[str, str]:
        """í†µê³„ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            save_as: ì €ì¥í•  íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
            
        Returns:
            ìƒì„±ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        if not self.dashboard_manager:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            paths = self.dashboard_manager.create_report(save_as)
            logger.info(f"í†µê³„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {len(paths)}ê°œ íŒŒì¼")
            return paths
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def get_system_health_chart(self) -> Optional[Any]:
        """ì‹œìŠ¤í…œ í—¬ìŠ¤ ì°¨íŠ¸ ìƒì„±
        
        Returns:
            í—¬ìŠ¤ ì¸ë””ì¼€ì´í„° Figure ë˜ëŠ” None
        """
        if not self.visualizer:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # ìµœì‹  í†µê³„ ë¡œë“œ
            if not self.visualizer.latest_stats:
                self.visualizer.load_latest_stats()
            
            # í—¬ìŠ¤ ì°¨íŠ¸ ìƒì„±
            chart = self.visualizer.create_system_health_indicator()
            return chart
        except Exception as e:
            logger.error(f"í—¬ìŠ¤ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def get_api_usage_chart(self, hours: int = 24) -> Optional[Any]:
        """API ì‚¬ìš©ëŸ‰ ì°¨íŠ¸ ìƒì„±
        
        Args:
            hours: í‘œì‹œí•  ì‹œê°„ ë²”ìœ„
            
        Returns:
            API ì‚¬ìš©ëŸ‰ ì°¨íŠ¸ Figure ë˜ëŠ” None
        """
        if not self.visualizer:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # íˆìŠ¤í† ë¦¬ ë°ì´í„° ë¡œë“œ
            if not self.visualizer.history_data:
                self.visualizer.load_history_data()
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = self.visualizer.prepare_dataframe()
            
            # ì‹œê°„ í•„í„°ë§
            if not df.empty and 'timestamp' in df.columns:
                from datetime import datetime, timedelta
                cutoff_time = datetime.now() - timedelta(hours=hours)
                df = df[df['timestamp'] >= cutoff_time]
            
            # API í˜¸ì¶œ ì°¨íŠ¸ ìƒì„±
            chart = self.visualizer.create_api_calls_chart(df)
            return chart
        except Exception as e:
            logger.error(f"API ì‚¬ìš©ëŸ‰ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def show_monitoring_dashboard(self):
        """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ í‘œì‹œ (ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°)"""
        if not self.dashboard_manager:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ëŒ€ì‹œë³´ë“œê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not self.dashboard_manager.dashboard:
                self.create_monitoring_dashboard()
            
            # ëŒ€ì‹œë³´ë“œ í‘œì‹œ
            self.dashboard_manager.show_dashboard()
        except Exception as e:
            logger.error(f"ëŒ€ì‹œë³´ë“œ í‘œì‹œ ì‹¤íŒ¨: {e}")

    # IPO ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ë“¤
    def _validate_date_format(self, date_str: str) -> bool:
        """ë‚ ì§œ í˜•ì‹ ê²€ì¦ (YYYYMMDD)"""
        if len(date_str) != 8:
            return False
        try:
            datetime.strptime(date_str, "%Y%m%d")
            return True
        except ValueError:
            return False

    def _validate_date_range(self, from_date: str, to_date: str) -> bool:
        """ë‚ ì§œ ë²”ìœ„ ìœ íš¨ì„± ê²€ì¦"""
        try:
            start = datetime.strptime(from_date, "%Y%m%d")
            end = datetime.strptime(to_date, "%Y%m%d")
            return start <= end
        except ValueError:
            return False

    @staticmethod
    def parse_ipo_date_range(date_range_str: str) -> tuple:
        """ì²­ì•½ê¸°ê°„ ë¬¸ìì—´ íŒŒì‹±
        
        Args:
            date_range_str: "2024.01.15~2024.01.16" í˜•ì‹ì˜ ë¬¸ìì—´
            
        Returns:
            tuple: (ì‹œì‘ì¼ datetime, ì¢…ë£Œì¼ datetime) ë˜ëŠ” (None, None)
        """
        if not date_range_str:
            return (None, None)
        
        # "2024.01.15~2024.01.16" í˜•ì‹ íŒŒì‹±
        pattern = r'(\d{4}\.\d{2}\.\d{2})~(\d{4}\.\d{2}\.\d{2})'
        match = re.match(pattern, date_range_str)
        
        if match:
            try:
                start_str = match.group(1).replace('.', '')
                end_str = match.group(2).replace('.', '')
                start_date = datetime.strptime(start_str, "%Y%m%d")
                end_date = datetime.strptime(end_str, "%Y%m%d")
                return (start_date, end_date)
            except ValueError:
                pass
        
        return (None, None)

    @staticmethod
    def format_ipo_date(date_str: str) -> str:
        """ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYYMMDD -> YYYY-MM-DD)"""
        if len(date_str) == 8:
            return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        elif '.' in date_str:
            return date_str.replace('.', '-')
        return date_str

    @staticmethod
    def calculate_ipo_d_day(ipo_date_str: str) -> int:
        """ì²­ì•½ì¼ê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ ê³„ì‚°"""
        if '~' in ipo_date_str:
            start_date, _ = KoreaInvestment.parse_ipo_date_range(ipo_date_str)
            if start_date:
                today = datetime.now()
                return (start_date - today).days
        return -999

    @staticmethod
    def get_ipo_status(subscr_dt: str) -> str:
        """ì²­ì•½ ìƒíƒœ íŒë‹¨
        
        Returns:
            str: "ì˜ˆì •", "ì§„í–‰ì¤‘", "ë§ˆê°", "ì•Œìˆ˜ì—†ìŒ"
        """
        start_date, end_date = KoreaInvestment.parse_ipo_date_range(subscr_dt)
        if not start_date or not end_date:
            return "ì•Œìˆ˜ì—†ìŒ"
        
        today = datetime.now()
        if today < start_date:
            return "ì˜ˆì •"
        elif start_date <= today <= end_date:
            return "ì§„í–‰ì¤‘"
        else:
            return "ë§ˆê°"

    @staticmethod
    def format_number(num_str: str) -> str:
        """ìˆ«ì ë¬¸ìì—´ì— ì²œë‹¨ìœ„ ì½¤ë§ˆ ì¶”ê°€"""
        try:
            return f"{int(num_str):,}"
        except (ValueError, TypeError):
            return num_str

    # IPO Schedule API
    @cacheable(
        ttl=3600,  # 1ì‹œê°„
        key_generator=lambda self, from_date=None, to_date=None, symbol="": f"fetch_ipo_schedule:{from_date or 'DEFAULT'}:{to_date or 'DEFAULT'}:{symbol or 'ALL'}"
    )
    @retry_on_rate_limit()
    def fetch_ipo_schedule(self, from_date: str = None, to_date: str = None, symbol: str = "") -> dict:
        """ê³µëª¨ì£¼ ì²­ì•½ ì¼ì • ì¡°íšŒ
        
        ì˜ˆíƒì›ì •ë³´(ê³µëª¨ì£¼ì²­ì•½ì¼ì •) APIë¥¼ í†µí•´ ê³µëª¨ì£¼ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        í•œêµ­íˆ¬ì HTS(eFriend Plus) > [0667] ê³µëª¨ì£¼ì²­ì•½ í™”ë©´ê³¼ ë™ì¼í•œ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
        
        Args:
            from_date: ì¡°íšŒ ì‹œì‘ì¼ (YYYYMMDD, ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
            to_date: ì¡°íšŒ ì¢…ë£Œì¼ (YYYYMMDD, ê¸°ë³¸ê°’: 30ì¼ í›„)
            symbol: ì¢…ëª©ì½”ë“œ (ì„ íƒ, ê³µë°±ì‹œ ì „ì²´ ì¡°íšŒ)
            
        Returns:
            dict: ê³µëª¨ì£¼ ì²­ì•½ ì¼ì • ì •ë³´
                {
                    "rt_cd": "0",  # ì„±ê³µì—¬ë¶€
                    "msg_cd": "ì‘ë‹µì½”ë“œ",
                    "msg1": "ì‘ë‹µë©”ì‹œì§€",
                    "output1": [
                        {
                            "record_date": "ê¸°ì¤€ì¼",
                            "sht_cd": "ì¢…ëª©ì½”ë“œ",
                            "isin_name": "ì¢…ëª©ëª…",
                            "fix_subscr_pri": "ê³µëª¨ê°€",
                            "face_value": "ì•¡ë©´ê°€",
                            "subscr_dt": "ì²­ì•½ê¸°ê°„",  # "2024.01.15~2024.01.16"
                            "pay_dt": "ë‚©ì…ì¼",
                            "refund_dt": "í™˜ë¶ˆì¼",
                            "list_dt": "ìƒì¥/ë“±ë¡ì¼",
                            "lead_mgr": "ì£¼ê°„ì‚¬",
                            "pub_bf_cap": "ê³µëª¨ì „ìë³¸ê¸ˆ",
                            "pub_af_cap": "ê³µëª¨í›„ìë³¸ê¸ˆ",
                            "assign_stk_qty": "ë‹¹ì‚¬ë°°ì •ë¬¼ëŸ‰"
                        }
                    ]
                }
                
        Raises:
            ValueError: ëª¨ì˜íˆ¬ì ì‚¬ìš©ì‹œ ë˜ëŠ” ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜ì‹œ
            
        Note:
            - ëª¨ì˜íˆ¬ìëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            - ì˜ˆíƒì›ì—ì„œ ì œê³µí•œ ìë£Œì´ë¯€ë¡œ ì •ë³´ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
            - ì‹¤ì œ ì²­ì•½ì‹œì—ëŠ” ë°˜ë“œì‹œ ê³µì‹ ê³µëª¨ì£¼ ì²­ì•½ ê³µê³ ë¬¸ì„ í™•ì¸í•˜ì„¸ìš”.
            
        Examples:
            >>> # ì „ì²´ ê³µëª¨ì£¼ ì¡°íšŒ (ì˜¤ëŠ˜ë¶€í„° 30ì¼)
            >>> ipos = broker.fetch_ipo_schedule()
            
            >>> # íŠ¹ì • ê¸°ê°„ ì¡°íšŒ
            >>> ipos = broker.fetch_ipo_schedule(
            ...     from_date="20240101",
            ...     to_date="20240131"
            ... )
            
            >>> # íŠ¹ì • ì¢…ëª© ì¡°íšŒ
            >>> ipo = broker.fetch_ipo_schedule(symbol="123456")
        """
        # ëª¨ì˜íˆ¬ì ì²´í¬
        if self.mock:
            raise ValueError("ê³µëª¨ì£¼ì²­ì•½ì¼ì • ì¡°íšŒëŠ” ëª¨ì˜íˆ¬ìë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
        self.rate_limiter.acquire()
        
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì •
        if not from_date:
            from_date = datetime.now().strftime("%Y%m%d")
        if not to_date:
            to_date = (datetime.now() + timedelta(days=30)).strftime("%Y%m%d")
        
        # ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦
        if not self._validate_date_format(from_date) or not self._validate_date_format(to_date):
            raise ValueError("ë‚ ì§œ í˜•ì‹ì€ YYYYMMDD ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        if not self._validate_date_range(from_date, to_date):
            raise ValueError("ì‹œì‘ì¼ì€ ì¢…ë£Œì¼ë³´ë‹¤ ì´ì „ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        path = "uapi/domestic-stock/v1/ksdinfo/pub-offer"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "HHKDB669108C0",
            "custtype": "P"  # ê°œì¸
        }
        
        params = {
            "SHT_CD": symbol,
            "CTS": "",
            "F_DT": from_date,
            "T_DT": to_date
        }
        
        resp = requests.get(url, headers=headers, params=params)
        resp_json = resp.json()
        
        # ì—ëŸ¬ ì²˜ë¦¬
        if resp_json.get('rt_cd') != '0':
            logger.error(f"ê³µëª¨ì£¼ ì¡°íšŒ ì‹¤íŒ¨: {resp_json.get('msg1', 'Unknown error')}")
            return resp_json
        
        return resp_json


# RateLimiter í´ë˜ìŠ¤ëŠ” enhanced_rate_limiter.pyë¡œ ì´ë™ë¨


if __name__ == "__main__":
    with open("../koreainvestment.key", encoding='utf-8') as key_file:
        lines = key_file.readlines()

    key = lines[0].strip()
    secret = lines[1].strip()
    acc_no = lines[2].strip()

    broker = KoreaInvestment(
        api_key=key,
        api_secret=secret,
        acc_no=acc_no,
        # exchange="ë‚˜ìŠ¤ë‹¥" # todo: exchangeëŠ” ì œê±° ì˜ˆì •
    )

    balance = broker.fetch_present_balance()
    print(balance)

    # result = broker.fetch_oversea_day_night()
    # pprint.pprint(result)

    # minute1_ohlcv = broker.fetch_today_1m_ohlcv("005930")
    # pprint.pprint(minute1_ohlcv)

    # broker = KoreaInvestment(key, secret, exchange="ë‚˜ìŠ¤ë‹¥")
    # import pprint
    # resp = broker.fetch_price("005930")
    # pprint.pprint(resp)
    #
    # b = broker.fetch_balance("63398082")
    # pprint.pprint(b)
    #
    # resp = broker.create_market_buy_order("63398082", "005930", 10)
    # pprint.pprint(resp)
    #
    # resp = broker.cancel_order("63398082", "91252", "0000117057", "00", 60000, 5, "Y")
    # print(resp)
    #
    # resp = broker.create_limit_buy_order("63398082", "TQQQ", 35, 1)
    # print(resp)



    # import pprint
    # broker = KoreaInvestment(key, secret, exchange="ë‚˜ìŠ¤ë‹¥")
    # resp_ohlcv = broker.fetch_ohlcv("TSLA", '1d', to="")
    # print(len(resp_ohlcv['output2']))
    # pprint.pprint(resp_ohlcv['output2'][0])
    # pprint.pprint(resp_ohlcv['output2'][-1])
