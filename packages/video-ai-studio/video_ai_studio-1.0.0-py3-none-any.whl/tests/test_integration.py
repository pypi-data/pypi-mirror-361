#!/usr/bin/env python3
"""
Comprehensive integration tests for AI Content Pipeline package.
Tests all features including YAML loading, parallel execution, console scripts, and more.
"""
import subprocess
import sys
import os
import json
import tempfile
from pathlib import Path
from dotenv import load_dotenv

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Load environment variables
load_dotenv()

def test_package_installation():
    """Test that the package is properly installed"""
    print("ðŸ§ª Testing Package Installation...")
    
    try:
        # Test import using installed package
        from packages.core.ai_content_pipeline.ai_content_pipeline.pipeline.manager import AIPipelineManager
        print("âœ… Package imports successful")
        
        # Test basic functionality
        manager = AIPipelineManager()
        models = manager.get_available_models()
        total_models = sum(len(model_list) for model_list in models.values())
        print(f"âœ… Pipeline manager initialized with {total_models} models")
        
        # Show model distribution
        print("ðŸ“Š Model distribution:")
        for step_type, model_list in models.items():
            if model_list:
                print(f"   {step_type}: {len(model_list)} models")
        
        return True
    except Exception as e:
        print(f"âŒ Package installation test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_console_scripts():
    """Test that console scripts are properly installed and working"""
    print("\nðŸ–¥ï¸  Testing Console Scripts...")
    
    scripts_tested = 0
    scripts_passed = 0
    
    # Test main command
    try:
        result = subprocess.run(['ai-content-pipeline', '--help'], 
                              capture_output=True, text=True, timeout=10)
        scripts_tested += 1
        if result.returncode == 0 and 'AI Content Pipeline' in result.stdout:
            print("âœ… 'ai-content-pipeline' command working")
            scripts_passed += 1
        else:
            print(f"âŒ 'ai-content-pipeline' command failed")
            if result.stderr:
                print(f"   Error: {result.stderr[:200]}")
    except Exception as e:
        print(f"âš ï¸  'ai-content-pipeline' test skipped: {e}")
    
    # Test alias command
    try:
        result = subprocess.run(['aicp', '--help'], 
                              capture_output=True, text=True, timeout=10)
        scripts_tested += 1
        if result.returncode == 0 and 'AI Content Pipeline' in result.stdout:
            print("âœ… 'aicp' alias working")
            scripts_passed += 1
        else:
            print(f"âŒ 'aicp' alias failed")
    except Exception as e:
        print(f"âš ï¸  'aicp' test skipped: {e}")
    
    # Test list-models command
    try:
        result = subprocess.run(['ai-content-pipeline', 'list-models'], 
                              capture_output=True, text=True, timeout=10)
        scripts_tested += 1
        if result.returncode == 0 and ('Available' in result.stdout or 'Supported Models' in result.stdout):
            print("âœ… 'list-models' command working")
            scripts_passed += 1
        else:
            print(f"âŒ 'list-models' command failed")
    except Exception as e:
        print(f"âš ï¸  'list-models' test skipped: {e}")
    
    return scripts_passed >= 2  # Pass if at least 2 out of 3 work

def test_yaml_configuration():
    """Test YAML configuration loading"""
    print("\nðŸ“„ Testing YAML Configuration Loading...")
    
    try:
        from packages.core.ai_content_pipeline.ai_content_pipeline.pipeline.manager import AIPipelineManager
        
        manager = AIPipelineManager()
        
        # Look for YAML files in different possible locations
        yaml_paths = [
            "input/pipelines/simple_test.yaml",
            "input/pipelines/analysis_detailed_gemini.yaml",
            "input/tts_simple_test.yaml",
            "input/tts_parallel_test.yaml"
        ]
        
        yaml_found = False
        for yaml_path in yaml_paths:
            if os.path.exists(yaml_path):
                yaml_found = True
                print(f"ðŸ“‹ Testing with: {yaml_path}")
                
                try:
                    chain = manager.create_chain_from_config(yaml_path)
                    print(f"âœ… Config loaded: {chain.name}")
                    print(f"   Steps: {len(chain.steps)}")
                    
                    # Show first few steps
                    for i, step in enumerate(chain.steps[:3], 1):
                        print(f"   Step {i}: {step.step_type.value} ({step.model})")
                    if len(chain.steps) > 3:
                        print(f"   ... and {len(chain.steps) - 3} more steps")
                    
                    # Test validation
                    try:
                        errors = chain.validate()
                        if errors:
                            print(f"âš ï¸  Validation warnings: {len(errors)} issues")
                        else:
                            print("âœ… Chain validation passed")
                    except Exception as e:
                        print(f"âš ï¸  Validation test skipped: {e}")
                    
                    # Test cost estimation
                    cost_info = manager.estimate_chain_cost(chain)
                    print(f"ðŸ’° Estimated cost: ${cost_info['total_cost']:.4f}")
                    
                    break
                except Exception as e:
                    print(f"âš ï¸  Error loading {yaml_path}: {e}")
                    continue
        
        if not yaml_found:
            print("âš ï¸  No YAML test files found, creating test config...")
            # Create a test YAML file
            test_config = """
name: integration_test
description: Integration test pipeline
steps:
  - type: text_to_speech
    model: elevenlabs
    params:
      voice: Rachel
      text: "Hello from integration test"
  
  - type: text_to_image
    model: flux_schnell
    params:
      prompt: "A beautiful sunset"
      num_images: 1
"""
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                f.write(test_config)
                temp_yaml = f.name
            
            chain = manager.create_chain_from_config(temp_yaml)
            print(f"âœ… Test YAML loaded: {chain.name}")
            os.unlink(temp_yaml)
        
        return True
        
    except Exception as e:
        print(f"âŒ YAML configuration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_parallel_execution():
    """Test parallel execution feature"""
    print("\nâš¡ Testing Parallel Execution Feature...")
    
    try:
        # Set environment variable for parallel execution
        original_value = os.environ.get('PIPELINE_PARALLEL_ENABLED')
        os.environ['PIPELINE_PARALLEL_ENABLED'] = 'true'
        
        from packages.core.ai_content_pipeline.ai_content_pipeline.pipeline.manager import AIPipelineManager
        
        manager = AIPipelineManager()
        print("âœ… Parallel execution enabled")
        
        # Create a test config with simple steps (parallel execution is controlled by environment variable)
        config = {
            "name": "parallel_test",
            "description": "Test parallel execution",
            "steps": [
                {
                    "type": "text_to_image",
                    "model": "flux_schnell",
                    "params": {"prompt": "Test 1"}
                },
                {
                    "type": "text_to_image", 
                    "model": "flux_schnell",
                    "params": {"prompt": "Test 2"}
                }
            ]
        }
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(config, f)
            temp_path = f.name
        
        chain = manager.create_chain_from_config(temp_path)
        print(f"âœ… Parallel chain created: {chain.name}")
        
        # Validate chain structure
        if chain.steps:
            print(f"âœ… Chain contains {len(chain.steps)} steps")
            print(f"âœ… Parallel execution environment variable is set")
        else:
            print("âš ï¸  Chain has no steps")
        
        os.unlink(temp_path)
        
        # Reset environment variable
        if original_value is None:
            if 'PIPELINE_PARALLEL_ENABLED' in os.environ:
                del os.environ['PIPELINE_PARALLEL_ENABLED']
        else:
            os.environ['PIPELINE_PARALLEL_ENABLED'] = original_value
        
        return True
    except Exception as e:
        print(f"âŒ Parallel execution test failed: {e}")
        return False

def test_output_management():
    """Test output directory management"""
    print("\nðŸ“ Testing Output Management...")
    
    try:
        from packages.core.ai_content_pipeline.ai_content_pipeline.pipeline.manager import AIPipelineManager
        
        manager = AIPipelineManager()
        
        # Check output directories
        output_dir = manager.output_dir
        temp_dir = manager.temp_dir
        
        print(f"ðŸ“‚ Output directory: {output_dir}")
        print(f"ðŸ“‚ Temp directory: {temp_dir}")
        
        # Verify they are Path objects
        if isinstance(output_dir, Path) and isinstance(temp_dir, Path):
            print("âœ… Directory paths are valid Path objects")
            
            # Check if parent directories exist (directories themselves might not exist yet)
            if output_dir.parent.exists() or str(output_dir).startswith('output'):
                print("âœ… Output directory structure is valid")
            else:
                print("âš ï¸  Output directory will be created on first use")
            
            return True
        else:
            print("âŒ Invalid directory path types")
            return False
            
    except Exception as e:
        print(f"âŒ Output management test failed: {e}")
        return False

def test_model_availability():
    """Test model availability and categorization"""
    print("\nðŸŽ¯ Testing Model Availability...")
    
    try:
        from packages.core.ai_content_pipeline.ai_content_pipeline.pipeline.manager import AIPipelineManager
        
        manager = AIPipelineManager()
        models = manager.get_available_models()
        
        # Expected categories
        expected_categories = [
            'text_to_speech', 'text_to_image', 'text_to_video',
            'image_to_image', 'image_to_video', 'video_to_video',
            'prompt_generation'
        ]
        
        print("ðŸ“Š Model Categories:")
        for category in expected_categories:
            if category in models:
                count = len(models[category])
                if count > 0:
                    print(f"âœ… {category}: {count} models")
                    # Show first 2 models as examples
                    for model in list(models[category])[:2]:
                        print(f"   - {model}")
                    if count > 2:
                        print(f"   ... and {count - 2} more")
                else:
                    print(f"âš ï¸  {category}: No models configured")
            else:
                print(f"âŒ {category}: Category missing")
        
        total_models = sum(len(model_list) for model_list in models.values())
        print(f"\nðŸ“ˆ Total: {total_models} models available")
        
        return total_models > 0
        
    except Exception as e:
        print(f"âŒ Model availability test failed: {e}")
        return False

def main():
    """Run comprehensive integration tests"""
    print("ðŸš€ AI Content Pipeline - Integration Test Suite")
    print("="*60)
    
    tests = [
        ("Package Installation", test_package_installation),
        ("Console Scripts", test_console_scripts),
        ("YAML Configuration", test_yaml_configuration),
        ("Parallel Execution", test_parallel_execution),
        ("Output Management", test_output_management),
        ("Model Availability", test_model_availability),
    ]
    
    passed = 0
    failed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"âŒ {test_name} test crashed: {e}")
            failed += 1
    
    # Final summary
    print("\n" + "="*60)
    print("ðŸ“Š INTEGRATION TEST RESULTS")
    print("="*60)
    
    if passed == total:
        print("ðŸŽ‰ ALL TESTS PASSED!")
        print(f"âœ… {passed}/{total} tests successful")
        print("\nðŸ† The AI Content Pipeline package is fully functional!")
        print("\nðŸ”§ Available Commands:")
        print("   â€¢ ai-content-pipeline --help")
        print("   â€¢ ai-content-pipeline list-models")
        print("   â€¢ ai-content-pipeline run-chain --config config.yaml")
        print("   â€¢ ai-content-pipeline generate-image --text 'prompt'")
        print("   â€¢ aicp (shortened alias)")
        print("\nðŸ“¦ Key Features Verified:")
        print("   â€¢ Multi-model support across 7 categories")
        print("   â€¢ YAML-based pipeline configuration")
        print("   â€¢ Parallel execution capability")
        print("   â€¢ Cost estimation and tracking")
        print("   â€¢ Organized output management")
    else:
        print(f"âš ï¸  {passed}/{total} tests passed, {failed} failed")
        print("ðŸ”§ Some features may need configuration")
        print("ðŸ“ Check the output above for specific issues")
        
        if passed >= total * 0.6:  # If at least 60% passed
            print("\nâœ… Core functionality is working")
            print("ðŸ’¡ The package can still be used for most tasks")
    
    return 0 if passed == total else 1

if __name__ == "__main__":
    sys.exit(main())