# This file was generated by Nuitka

# Stubs included by default
from __future__ import annotations
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import approx_count_distinct, col, concat_ws, expr, floor, hash, pmod, rand, when
from pyspark.sql.types import BooleanType, DoubleType, IntegerType, StringType, StructField, StructType, TimestampType
from typing import Any, Dict, List, Optional, Tuple
from typing_extensions import Self
import json
import logging
import os
import re

class Branch:
    def __init__(self: Self, name: str, predicate: str) -> None: ...
    def to_dict(self: Self) -> dict[str, str]: ...

class Root:
    def __init__(self: Self, name: str) -> None: ...
    def add_branch(self: Self, b: Branch) -> Any: ...
    def to_dict(self: Self) -> dict[str, Any]: ...

class DRTree:
    def __init__(self: Self) -> None: ...
    def add_root(self: Self, root: Root) -> Any: ...
    def to_dict(self: Self, concise: bool) -> dict[str, Any]: ...

class QuantileSkewDetector:
    def __init__(self: Self, threshold: float, top_n: int) -> None: ...
    def detect(self: Self, df: DataFrame) -> List[str]: ...

class KeyFeatureDetector:
    def __init__(self: Self, threshold: float, max_combo: int) -> None: ...
    def detect(self: Self, df: DataFrame, dr_tree: Optional[DRTree]) -> List[str]: ...

class AutoParameterAdvisor:
    def __init__(self: Self, df: DataFrame, skew_top_n: int, cat_top_n: int, sample_frac: float, max_sample: int, seed: int) -> None: ...
    def recommend(self: Self) -> Tuple[List[str], List[str], DataFrame]: ...

class HexaDruid:
    def __init__(self: Self, df: DataFrame, output_dir: str) -> None: ...
    def schemaVisor(self: Self, sample_frac: float, max_sample: int) -> tuple[DataFrame, StructType, DRTree]: ...
    def detect_skew(self: Self, threshold: float, top_n: int) -> List[str]: ...
    def apply_smart_salting(self: Self, col_name: Optional[str], salt_count: Optional[int]) -> DataFrame: ...
    def detect_keys(self: Self, threshold: float, max_combo: int) -> List[str]: ...

balance_skew = HexaDruid.apply_smart_salting

__name__ = ...



# Modules used internally, to allow implicit dependencies to be seen:
import logging
import os
import re
import json
import typing
import pyspark
import pyspark.sql
import pyspark.sql.DataFrame
import pyspark.sql.SparkSession
import pyspark.sql.functions
import pyspark.sql.functions.col
import pyspark.sql.functions.when
import pyspark.sql.functions.concat_ws
import pyspark.sql.functions.expr
import pyspark.sql.functions.approx_count_distinct
import pyspark.sql.functions.floor
import pyspark.sql.functions.rand
import pyspark.sql.functions.hash
import pyspark.sql.functions.pmod
import pyspark.sql.types
import itertools
import pyspark.sql.functions.sum