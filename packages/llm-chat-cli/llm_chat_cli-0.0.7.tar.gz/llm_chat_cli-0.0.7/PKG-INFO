Metadata-Version: 2.4
Name: llm-chat-cli
Version: 0.0.7
Summary: A CLI tool for chatting with multiple LLM providers
Author-email: Varadan Kalkunte <varadan.vk@gmail.com>
Project-URL: Homepage, https://github.com/yourusername/llm_chat
Project-URL: Bug Tracker, https://github.com/yourusername/llm_chat/issues
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Environment :: Console
Classifier: Topic :: Communications :: Chat
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: groq
Requires-Dist: openai
Requires-Dist: anthropic
Requires-Dist: tiktoken
Requires-Dist: prompt_toolkit
Requires-Dist: termcolor
Requires-Dist: rich
Requires-Dist: python-dotenv
Requires-Dist: cerebras_cloud_sdk

# llm-chat-cli

A CLI tool for chatting with multiple LLM providers in one place.

## Install

```bash
pip install llm-chat-cli
```

## Setup

First time setup:

```bash
lmci setup
```

This will prompt you for your API keys and create a config file.

## Usage

Start chatting:

```bash
lmci
```

## Supported Providers

- Groq
- OpenAI
- Anthropic
- Cerebras

## Commands

- `change model` - Switch models
- `token count` - Show tokens used
- `clear history` - Clear chat history
- `quit`/`exit` - Exit
