{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Transformer\n",
    "> Rederiving the Transformer as an energy-based Associative Memory.\n",
    "\n",
    "<style>\n",
    "    .red { color:rgb(247, 109, 104); }\n",
    "    .blue { color:rgb(64, 130, 200); }\n",
    "    .green { color:rgb(89, 203, 78); }\n",
    "    .yellow { color:rgb(252, 211, 28); }\n",
    "</style>\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bhoov/amtutorial/blob/main/tutorial_ipynbs/01_energy_transformer.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "> Squint, and the Transformer looks like a dynamical system. \n",
    "\n",
    "At its core, the transformer is a stack of $L$ transformer blocks that takes a length $N$ sequence of input tokens $\\{\\mathbf{x}^{(0)}_1, \\ldots, \\mathbf{x}^{(0)}_N\\}$ and outputs a length $N$ sequence of output tokens $\\{\\mathbf{x}^{(L)}_1, \\ldots, \\mathbf{x}^{(L)}_N\\}$. Each token $\\mathbf{x}^{(l)}_i \\in \\mathbb{R}^D$ is a vector of dimension $D$. \n",
    "\n",
    "When blocks are stacked, the residual connections form a \"residual highway\" that consists entirely of normalizations and additions from `Attention` and `MLP` operations.\n",
    "\n",
    "![A vanilla Transformer Block consisting of 4 main operations: [**(multi-headed) attention**]{.red}, [**MLP**]{.blue}, [**(pre-)layernorms**]{.green}, and [**residual connections**]{.yellow}. The Transformer is a stack of these blocks, which we show depicted as a \"residual highway\" design. The residual highway showcases how each block \"perturbs\" its input, and the mathematical operation looks like a dynamical system. If the system can be described such that the operation of each block is a gradient descent, the system becomes an energy-based model.](./assets/figs/standard-transformer.png){#fig-standard-transformer}\n",
    "\n",
    "**Associative Memory** (AM) requires a global energy function, where each computation minimizes the total energy of the system. Our goal is to derive an energy function whose gradient looks as much like the Transformer block as possible.\n",
    "\n",
    "![**The Energy Transformer block, shown as the derivative of its energy.** Attention and Hopfield Network (symmetric MLP) updates are computed in parallel. Updates are added to the input via a residual connection that is a byproduct of ET describing a dynamical system.](./assets/figs/et-block.png){width=300}\n",
    "\n",
    "## Introducing Energy into the Transformer\n",
    "\n",
    "We will now build a kind of associative memory called the \"Energy Transformer\" [@hoover2024energy] that turns the familiar transformer operation into an energy minimization. Energy Transformer (ET) defines a single energy on an $\\mathbf{x} \\in \\mathbb{R}^{N \\times D}$ collection of tokens, where we can think of each token $\\mathbf{x}_B$ as a \"particle\" that knows some information about itself and needs to figure out what it should become. Some particles (unmasked tokens) already know their identity, while others (masked tokens) only know their position and must discover their identity by interacting with their neighbors.\n",
    "\n",
    "Minimizing the energy of the Energy Transformer (ET) is a recurrent process. The entire transformer consists of a single Transformer block, and each \"layer\" of the transformer becomes a gradient descent step down the energy. This gradient descent step looks remarkably like a standard transformer block, complete with attention, MLP-like operations, layer normalizations, and residual connections.\n",
    "\n",
    "The global energy combines two intuitive ideas: (1) **attention energy** that encourages masked tokens to align with relevant unmasked tokens, and (2) **memory energy** that ensures all tokens look like realistic patterns the model has learned. The gradient of each of these energies look like a self-attention and MLP, respectively, with some shared weight constraints.\n",
    "\n",
    "This is one of those situations where the code ends up being significantly simpler than the equations. We write the equations for completeness, but feel free to skip to [@sec-ET-implementation] for succinct code.\n",
    "\n",
    "### Attention Energy\n",
    "\n",
    "We describe the energy of a multi-headed attention with $H$ heads, where the $h$-th head of attention is parameterized by $\\mathbf{W}_h^Q, \\mathbf{W}_h^K \\in \\mathbb{R}^{D \\times Y}$, where $Y$ is the \"head dimension\". The input to the attention is the normalized token vectors $\\hat{\\mathbf{x}} \\in \\mathbb{R}^{N \\times D}$. In the math that follows, we index the heads by $h=1\\ldots H$, the head dimension by $\\alpha=1\\ldots Y$, tokens by $A,B,C=1 \\ldots N$, and each token vector by $i,j=1\\ldots D$.\n",
    "\n",
    ":::{.callout-note}\n",
    "## Einstein notation\n",
    "We find it convenient to use Einstein notation for the math, since it maps 1:1 to the einops operations we'll use in the code. If you aren't familiar with the notation, check out [this awesome tutorial](https://einops.rocks/1-einops-basics/). But fair warning, the equations at first look pretty complicated with all the indices.\n",
    "\n",
    "One tip for reading equations with lots of indices: *you don't need to remember the shape or order of tensors*, just remember the meaning of the indices. The number of subscripts is the number of dimensions of the tensor, and the meaning of each dimension is captured in the index name. For example, let $B=1\\ldots N$ index the token position in a sequence, and let $i=1\\ldots D$ index into each token vector. $x_{Bi}$ is an element of a 2-dimensional tensor capturing the sequence length $N$ and token dimension $D$. Transposes don't have meaning since things are named, so $x_{Bi} = x_{iB}$. So long as you know the index semantics, you can read always read the equation. Everything is just scalar multiplication and addition.\n",
    ":::\n",
    "\n",
    "The familiar queries and keys are computed as normal linear transformations:\n",
    "\n",
    "$$ \n",
    "   \\begin{split}\n",
    "        K_{h \\alpha B} &= \\sum\\limits_j W^K_{h \\alpha j}\\; \\hat{x}_{Bj}, \\qquad \\mathbf{K} \\in \\mathbb{R}^{H \\times Y \\times N} \\\\\n",
    "        Q_{h \\alpha C} &= \\sum\\limits_j W^Q_{h \\alpha j}\\; \\hat{x}_{Cj}, \\qquad \\mathbf{Q} \\in \\mathbb{R}^{H \\times Y \\times N}\n",
    "    \\end{split}\n",
    "$$\n",
    "\n",
    "Our familiar \"raw attention scores\" (pre-softmax) are still the dot-product correlations between each query and key:\n",
    "\n",
    "$$\n",
    "A_{hBC} = \\sum_{\\alpha} K_{h\\alpha B} Q_{h\\alpha C} \n",
    "$$\n",
    "\n",
    "Now for the different part: we describe the energy of the attention as the negative log-sum-exp of the attention scores. We will use the $\\beta$ as an inverse-temperature hyperparameter to scale the attention scores.\n",
    "\n",
    "$$\n",
    "E^\\text{ATT} = -\\frac{1}{\\beta} \\sum_{h=1}^H \\sum_{C=1}^N \\log \\left( \\sum_{B \\neq C} \\exp(\\beta A_{hBC}) \\right)\n",
    "$${#eq-attention-energy}\n",
    "\n",
    "As we saw in [a previous notebook](./00_dense_storage.ipynb), the negative log-sum-exp is an exponential variation of the Dense Associative Memory. The cool thing is that the gradient of the negative log-sum-exp is the softmax, which is what we'd like to see in the attention update rule.\n",
    "\n",
    ":::{.callout-note}\n",
    "## Where are our values?\n",
    "You may recall that traditional attention also has a value matrix. When we take the gradient of @eq-attention-energy, we lose the flexibility to include an independently parameterized values: the values **must** be a function of the queries and the keys.\n",
    ":::\n",
    "\n",
    "### Memory Energy\n",
    "\n",
    "In traditional transformers, the MLP (without biases) can be written as a two-layer feedforward network with a ReLU on the hidden activations. The MLP is parameterized by two weight matrices $\\mathbf{V}, \\mathbf{W} \\in \\mathbb{R}^{M \\times D}$ where $M$ is the size of the hidden layer ($M=4D$ is often viewed as the default expansion factor atop token dimension $D$). Let's again use Einstein notation, where $\\mu=1\\ldots M$ indexes the hidden units, $i,j=1\\ldots D$ index the token dimensions, and $B=1\\ldots N$ indexes each token. \n",
    "\n",
    "$$\n",
    "\\text{MLP}(\\hat{\\mathbf{x}})_{Bi} = \\sum_\\mu W_{\\mu i} \\; \\text{ReLU}\\left(\\sum_j V_{\\mu j} \\hat{\\mathbf{x}}_{Bj}\\right)\n",
    "$${#eq-mlp-update}\n",
    "\n",
    "If we assume weight sharing between $\\mathbf{V} = \\mathbf{W} = \\boldsymbol{\\xi}$, this is a gradient descent step down the energy of a Hopfield Network \n",
    "\n",
    "$$\n",
    "E^{\\text{HN}}(\\hat{\\mathbf{x}}) = - \\sum_{B, \\mu} F\\left(\\sum_j \\xi_{\\mu j} \\hat{\\mathbf{x}}_{Bj}\\right)\n",
    "$$\n",
    "\n",
    "with rectified quadratic energy $F(\\cdot) := \\frac12 \\text{ReLU}(\\cdot)^2$. If we say $f(\\cdot) := F'(\\cdot) = \\text{ReLU}(\\cdot)$, the negative gradient of the energy is\n",
    "\n",
    "$$\n",
    "-\\frac{\\partial E^{\\text{HN}}(\\mathbf{\\hat{x}})}{\\partial \\hat{x}_{Bi}} \n",
    "= \\sum_\\mu \\xi_{\\mu i} \\; f\\left(\\sum_j \\xi_{\\mu j} \\hat{\\mathbf{x}}_{Bj}\\right),\n",
    "$$\n",
    "\n",
    "which is identical to the MLP operation in @eq-mlp-update with a weight sharing constraint.\n",
    "\n",
    ":::{.callout-note}\n",
    "It is perfectly reasonable to consider other convex functions $F$ for use in the energy. Polynomials of higher degree $n$ or exponential functions are both valid and will yield [Dense Associative Memory](./00_dense_storage.ipynb). However, because traditional Transformers use a ReLU activation, we use a rectified quadratic energy.\n",
    ":::\n",
    "\n",
    "\n",
    "### ET in code {#sec-ET-implementation}\n",
    "\n",
    "Let's implement the attention energy in code. We will use [`jax`](https://github.com/jax-ml/jax) and [`equinox`](https://github.com/patrick-kidger/equinox) for our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment for colab users\n",
    "# !pip install amtutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Necessary imports\"\n",
    "import jax, jax.numpy as jnp, jax.random as jr, jax.tree_util as jtu, jax.lax as lax\n",
    "import equinox as eqx\n",
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "from glob import glob\n",
    "from fastcore.basics import *\n",
    "from fastcore.meta import *\n",
    "import matplotlib.pyplot as plt\n",
    "from jaxtyping import Float, Array\n",
    "import functools as ft\n",
    "from einops import rearrange\n",
    "from amtutorial.data_utils import get_et_imgs, get_et_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `EnergyTransformer` class captures all the token processing in the entire transformer.** There are maybe 7 lines of code that perform the actual energy computation. This single energy function, when paired with a layer-norm, is analogous to the full computation across all layers of a traditional transformer. The only things missing are some some token and position embedding matrices to make it work on real data, but we will do that in the following section.\n",
    "\n",
    "First, let's describe the configuration for ET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETConfig(eqx.Module):\n",
    "  D: int = 768 # token dimension\n",
    "  H: int = 12 # number of heads\n",
    "  Y: int = 64 # head dimension\n",
    "  M: int = 3072 # MLP size\n",
    "  beta: Optional[float] = None # Inverse temperature for attention, defaults to 1/sqrt(Y)\n",
    "  prevent_self_attention: bool = True # Prevent explicit self-attention\n",
    "  def get_beta(self): return self.beta or 1/jnp.sqrt(self.Y)\n",
    "\n",
    "smallETConfig = ETConfig(D=12, H=2, Y=6, M=24)\n",
    "mediumETConfig = ETConfig(D=128, H=4, Y=32, M=256)\n",
    "fullETConfig = ETConfig(D=768, H=12, Y=64, M=3072, beta=1/jnp.sqrt(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ETConfig` class captures all the dimensions and default hyperparameters for ET. The only thing left to do is implement the energies of Energy Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyTransformer(eqx.Module):\n",
    "  config: ETConfig\n",
    "  Wq: Float[Array, \"H D Y\"] # Query projection\n",
    "  Wk: Float[Array, \"H D Y\"] # Key projection\n",
    "  Xi: Float[Array, \"M D\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EnergyTransformer` is parameterized by **only** three matrices: $\\mathbf{W}^Q, \\mathbf{W}^K$ and $\\mathbf{Xi}$ (we did not choose to introduce any biases, though we could have).\n",
    "\n",
    "We use these parameters to define both the **attention energy** and the **memory energy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def attn_energy(self: EnergyTransformer, xhat: Float[Array, \"N D\"]):\n",
    "  beta = self.config.get_beta()\n",
    "  K = jnp.einsum(\"kd,hdy->khy\", xhat, self.Wk)\n",
    "  Q = jnp.einsum(\"qd,hdy->qhy\", xhat, self.Wq)\n",
    "  N = K.shape[0]\n",
    "  if self.config.prevent_self_attention:\n",
    "    bmask = jnp.ones((N, N)) - jnp.eye(N) # Prevent self-attention\n",
    "  else:\n",
    "    bmask = jnp.ones((N, N))\n",
    "  A = jax.nn.logsumexp(beta * jnp.einsum(\"khy,qhy->hqk\", K, Q), b=bmask, axis=-1)\n",
    "  return -1/beta * A.sum()\n",
    "\n",
    "@patch\n",
    "def hn_energy(self: EnergyTransformer, xhat: Float[Array, \"N D\"]):\n",
    "  \"\"\"ReLU-based \"memory energy\" using a Hopfield Network\"\"\"\n",
    "  hid = jnp.einsum(\"nd,md->nm\", xhat, self.Xi)\n",
    "  return -0.5 * (hid.clip(0) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total energy is just the sum of the attention and memory energies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def energy(self: EnergyTransformer, xhat: Float[Array, \"N D\"]):\n",
    "  \"Total energy of the Energy Transformer\"\n",
    "  return self.attn_energy(xhat) + self.hn_energy(xhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's make a `classmethod` to easily initialize the module with random parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(cls_method=True)\n",
    "def rand_init(cls: EnergyTransformer, key, config: ETConfig):\n",
    "  key1, key2, key3 = jr.split(key, 3)\n",
    "  return cls(config,\n",
    "    Wq=jr.normal(key1, (config.H, config.D, config.Y)) / jnp.sqrt(config.Y),\n",
    "    Wk=jr.normal(key2, (config.H, config.D, config.Y)) / jnp.sqrt(config.Y),\n",
    "    Xi=jr.normal(key3, (config.M, config.D)) / jnp.sqrt(config.D)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "## Special Layer Normalization\n",
    "Note that the `xhat` inputs above are all layer-normalized tokens. However, like other AMs, we restrict ourselves to using non-linearities that are gradients of a convex Lagrangian function. Our \"special layernorm\"  is the same as the standard layer normalization *except* that we need our learnable `gamma` parameter to be a scalar instead of a vector of shape `D`. We will just show this in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyLayerNorm(eqx.Module):\n",
    "  \"\"\"Define our primary activation function (modified LayerNorm) as a lagrangian with energy\"\"\"\n",
    "  gamma: Float[Array, \"\"]  # Scaling scalar\n",
    "  delta: Float[Array, \"D\"] # Bias per token\n",
    "  use_bias: bool = False\n",
    "  eps: float = 1e-5\n",
    "    \n",
    "  def lagrangian(self, x):\n",
    "    \"\"\"Integral of the standard LayerNorm\"\"\"\n",
    "    D = x.shape[-1]\n",
    "    xmeaned = x - x.mean(-1, keepdims=True)\n",
    "    t1 = D * self.gamma * jnp.sqrt((1 / D * xmeaned**2).sum() + self.eps)\n",
    "    if not self.use_bias: return t1\n",
    "    t2 = (self.delta * x).sum()\n",
    "    return t1 + t2\n",
    "\n",
    "  def __call__(self, x):\n",
    "    \"\"\"LayerNorm. The derivative of the Lagrangian\"\"\"\n",
    "    xmeaned = x - x.mean(-1, keepdims=True)\n",
    "    v = self.gamma * (xmeaned) / jnp.sqrt((xmeaned**2).mean(-1, keepdims=True)+ self.eps)\n",
    "    if self.use_bias: return v + self.delta\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "That's it! We rely on autograd to do the energy minimization, or the \"inference\" pass through the entire transformer.\n",
    "\n",
    "Let's check that the energy both monotonically decreases and is bounded from below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(11)\n",
    "et = EnergyTransformer.rand_init(key, config=smallETConfig)\n",
    "lnorm = EnergyLayerNorm(gamma=1., delta=jnp.zeros(et.config.D))\n",
    "\n",
    "def energy_recall(Efn, x_init, nsteps, step_size):\n",
    "  \"Simple gradient descent to recall a memory\"\n",
    "  @jax.jit\n",
    "  def gd_step(x, i):\n",
    "      energy, grad = jax.value_and_grad(Efn)(lnorm(x))\n",
    "      x_next = x - step_size * grad\n",
    "      return x_next, energy\n",
    "\n",
    "  xhat_init = lnorm(x_init)\n",
    "  final_x, energy_history = jax.lax.scan(\n",
    "      gd_step,\n",
    "      xhat_init,\n",
    "      jnp.arange(nsteps)\n",
    "  )\n",
    "  return final_x, energy_history\n",
    "\n",
    "x_init = jr.normal(key, (100, et.config.D)) # Layer normalized tokens\n",
    "final_x, energy_history = energy_recall(et.energy, x_init, nsteps=3000, step_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| label: fig-energy-descent-combined\n",
    "#| fig-cap: Energy descent for the Energy Transformer.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "# Plot attention energy descent\n",
    "ax.plot(energy_history, linewidth=2, color='blue')\n",
    "ax.set_xlabel('Gradient Descent Steps')\n",
    "ax.set_ylabel('Energy')\n",
    "ax.set_title('Energy Transformer')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with an Energy Transformer\n",
    "\n",
    "To make the Energy Transformer described above work on real data, we need to add some necessary addendums to work with image data: the token and position embedding matrices, and some data processing code.\n",
    "\n",
    "### Loading data\n",
    "\n",
    "Energy Transformer was originally trained on [ImageNet](https://image-net.org/). We will load some example images (unseen during training) to demonstrate ET's ability to remember images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare unseen images\n",
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406]) * 255 # C, H, W\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225]) * 255 # C, H, W\n",
    "\n",
    "def normalize_img(im):\n",
    "  \"\"\"Put into channel first format, normalize\"\"\"\n",
    "  x = (im - IMAGENET_MEAN) / IMAGENET_STD\n",
    "  x = rearrange(x, \"h w c-> c h w\")\n",
    "  return x\n",
    "\n",
    "def unnormalize_img(x):\n",
    "  \"\"\"Put back into channel last format, denormalize\"\"\"\n",
    "  x = rearrange(x, \"c h w -> h w c\")\n",
    "  im = (x * IMAGENET_STD) + IMAGENET_MEAN\n",
    "  return im.astype(jnp.uint8)\n",
    "\n",
    "@ft.lru_cache\n",
    "def get_normalized_imgs():\n",
    "  imgs = jnp.array(get_et_imgs())\n",
    "  imgs = jax.vmap(normalize_img)(imgs)\n",
    "  return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "nh, nw = 2, 5\n",
    "imgs = get_normalized_imgs()\n",
    "unnormalized_imgs = jax.vmap(unnormalize_img)(imgs[:nh*nw])\n",
    "xshow = rearrange(unnormalized_imgs, \"(nh nw) h w c -> (nh h) (nw w) c\", nh=nh, nw=nw)\n",
    "plt.imshow(xshow)\n",
    "plt.imshow(xshow)\n",
    "plt.axis('off')\n",
    "plt.title(\"Example validation images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching images\n",
    "\n",
    "We build a `Patcher` class to patchify and unpatchify images, which is mostly a simple wrapper around the `rearrange` function from `einops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Patcher class\"\n",
    "class Patcher(eqx.Module):\n",
    "  \"Patchify and unpatchify an image.\"\n",
    "  image_shape: Iterable[int] # (C, H, W) Image shape\n",
    "  patch_size: int # Square patch size\n",
    "  kh: int # Number of patches in the height direction\n",
    "  kw: int # Number of patches in the width direction\n",
    "\n",
    "  @property\n",
    "  def patch_shape(self): return (self.image_shape[0], self.patch_size, self.patch_size)\n",
    "\n",
    "  @property\n",
    "  def num_patch_elements(self): return ft.reduce(lambda a, b=1: a * b, self.patch_shape)\n",
    "\n",
    "  @property\n",
    "  def num_patches(self): return self.kh * self.kw\n",
    "\n",
    "  def patchify(self, img):\n",
    "    \"Turn an image (possibly batched) into a collection of patches.\"\n",
    "    return rearrange(\n",
    "      img,\n",
    "      \"... c (kh h) (kw w)-> ... (kh kw) c h w\",\n",
    "      h=self.patch_size,\n",
    "      w=self.patch_size,\n",
    "    )\n",
    "\n",
    "  def unpatchify(self, patches):\n",
    "    \"Turn a collection of patches (possibly batched) back into an image.\"\n",
    "    return rearrange(\n",
    "      patches, \"... (kh kw) c h w -> ... c (kh h) (kw w)\", kh=self.kh, kw=self.kw\n",
    "    )\n",
    "\n",
    "  def rasterize(self, patches):\n",
    "    \"Rasterize patches into tokens\"\n",
    "    return rearrange(patches, \"... c h w -> ... (c h w)\")\n",
    "\n",
    "  def unrasterize(self, tokens):\n",
    "    \"Unrasterize tokens into patches\"\n",
    "    c,h,w = self.patch_shape\n",
    "    return rearrange(tokens, \"... (c h w) -> ... c h w\", c=c, h=h, w=w)\n",
    "\n",
    "  def tokenify(self, img):\n",
    "    \"Turn img into rasterized patches\"\n",
    "    return self.rasterize(self.patchify(img))\n",
    "\n",
    "  def untokenify(self, tokens):\n",
    "    \"Untokenify tokens into original image\"\n",
    "    return self.unpatchify(self.unrasterize(tokens))\n",
    "\n",
    "  def patchified_shape(self):\n",
    "    \"The expected shape of a patchified image\"\n",
    "    return (self.num_patches, *self.patch_shape)\n",
    "\n",
    "  @classmethod\n",
    "  def from_img(cls, img, patch_size):\n",
    "    \"Create a Patcher from an example image.\"\n",
    "    return cls.from_img_shape(img.shape, patch_size)\n",
    "\n",
    "  @classmethod\n",
    "  def from_img_shape(cls, img_shape, patch_size):\n",
    "    \"Create a patcher from a specified image shape.\"\n",
    "    height, width = img_shape[-2:]\n",
    "    assert (height % patch_size) == 0\n",
    "    assert (width % patch_size) == 0\n",
    "    kh = int(height / patch_size)\n",
    "    kw = int(width / patch_size)\n",
    "    return cls(img_shape, patch_size, kh, kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It lets us do things like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher = Patcher.from_img_shape(imgs[0].shape, patch_size=16)\n",
    "patched_img = patcher.patchify(imgs[0])\n",
    "print(patched_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "pad_width = 1\n",
    "padded_patches = np.pad(patched_img, ((0,0), (0,0), (pad_width,pad_width+1), (pad_width,pad_width+1)), mode='constant', constant_values=-np.inf)\n",
    "plt.imshow(unnormalize_img(patcher.unpatchify(padded_patches)))\n",
    "plt.axis('off')\n",
    "plt.title(\"Visualized patched image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Patcher.unpatchify` gets us back to the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jnp.all(patcher.unpatchify(patched_img) == imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also process an images and batches of imags into tokens and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenified_img = patcher.tokenify(imgs[0])\n",
    "print(\"Token pre-embedding shape: \", tokenified_img.shape)\n",
    "\n",
    "untokenified_img = patcher.untokenify(tokenified_img)\n",
    "assert jnp.all(untokenified_img == imgs[0])\n",
    "\n",
    "batch_tokenified_imgs = patcher.tokenify(imgs)\n",
    "print(\"Batch token pre-embedding shape: \", batch_tokenified_imgs.shape)\n",
    "\n",
    "batch_untokenified_imgs = patcher.untokenify(batch_tokenified_imgs)\n",
    "assert jnp.all(batch_untokenified_imgs == imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image-compatible ET\n",
    "\n",
    "Let's create a full ET, complete with embeddings, model that can be used for masked-image inpainting. We say that each image has $N$ total patches/tokens, where each patch as $Z = c \\times h \\times w$ pixels when rasterized. We will use linear embeddings (with biases) to embed and unembed rasterized image patches to tokens.\n",
    "\n",
    "First, let's describe the data and ET we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageETConfig(eqx.Module):\n",
    "  image_shape: Tuple[int, int, int] = (3, 224, 224) # (C, H, W) Image shape\n",
    "  patch_size: int = 16 # Square patch size\n",
    "  et_conf: ETConfig = fullETConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with data, we add a few extra matrices: embedding/unembedding matrices (let's use a bias for each), position embeddings, and CLS/MASK tokens. The position embeddings are used to encode the position of each token in the sequence, and the CLS/MASK tokens are used for interop with the original ViT. [@dosovitskiy2020vit] Additionally, the `layernorm` is external to the computation of the ET so we'll insert those parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnergyTransformer(eqx.Module):\n",
    "  patcher: Patcher\n",
    "  W_emb: Float[Array, \"Z D\"]\n",
    "  b_emb: Float[Array, \"D\"]\n",
    "  W_unemb: Float[Array, \"D Z\"]\n",
    "  b_unemb: Float[Array, \"Z\"]\n",
    "\n",
    "  pos_embed: Float[Array, \"(N+1) D\"] # Don't forget the CLS token!\n",
    "  cls_token: jax.Array\n",
    "  mask_token: jax.Array\n",
    "  et: EnergyTransformer\n",
    "  lnorm: EnergyLayerNorm\n",
    "\n",
    "  config: ImageETConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some functions for converting image patches to/from tokens. These are a.k.a. \"embedding\" and \"unembedding\" operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def encode(\n",
    "  self: ImageEnergyTransformer, \n",
    "  x: Float[Array, \"N Z\"]\n",
    "):\n",
    "  \"Embed rasterized patches to tokens\"\n",
    "  out = x @ self.W_emb + self.b_emb # (..., N, D)\n",
    "  return out\n",
    "\n",
    "@patch\n",
    "def decode(\n",
    "  self: ImageEnergyTransformer, \n",
    "  x: Float[Array, \"N D\"]):\n",
    "  \"Turn x from tokens to rasterized img patches\"\n",
    "  return x @ self.W_unemb + self.b_unemb # (..., N, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking tokens is also a part of this data connection. Let's corrupt and add the CLS register:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def corrupt_tokens(\n",
    "  self: ImageEnergyTransformer, \n",
    "  x: Float[Array, \"N D\"],\n",
    "  mask: Float[Array, \"N\"], \n",
    "  max_n_masked: int=100):\n",
    "  \"\"\"Corrupt tokens with MASK tokens wherever `mask` is 1.\n",
    "\n",
    "  `max_n_masked` needs to be known in advance for JAX JIT to work properly\n",
    "  \"\"\"\n",
    "  maskmask = jnp.nonzero(mask == 1, size=max_n_masked, fill_value=0)\n",
    "  return x.at[maskmask].set(self.mask_token) # (..., N, D)\n",
    "\n",
    "@patch\n",
    "def prep_tokens(\n",
    "  self: ImageEnergyTransformer, \n",
    "  x: Float[Array, \"N D\"], \n",
    "  mask: Float[Array, \"N\"]):\n",
    "  \"Add CLS+MASK tokens and POS embeddings\"\n",
    "  x = self.corrupt_tokens(x, mask)\n",
    "  x = jnp.concatenate([self.cls_token[None], x]) # (..., N+1, D)\n",
    "  return x + self.pos_embed # (..., N+1, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference process is *gradient descent* down the energy, and turns a full image whose patches are masked according to `mask` and returns predictions for the whole image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def __call__(\n",
    "  self: ImageEnergyTransformer, \n",
    "  img: Float[Array, \"C H W\"], \n",
    "  mask: Float[Array, \"N\"], \n",
    "  nsteps=12, \n",
    "  step_size=0.1):\n",
    "  \"A complete pipeline for masked image modeling in ET using gradient descent\"\n",
    "  x = self.patcher.tokenify(img) # (..., N, Z)\n",
    "  x = self.encode(x)\n",
    "  x = self.prep_tokens(x, mask)  # (..., N+1, D)\n",
    "\n",
    "  get_energy_info = jax.value_and_grad(self.et.energy)\n",
    "  \n",
    "  def gd_step(x, i):\n",
    "      xhat = self.lnorm(x)\n",
    "      E, dEdg = get_energy_info(xhat)\n",
    "      x_next = x - step_size * dEdg\n",
    "      return x_next, {\"energy\": E, \"xhat\": xhat}\n",
    "\n",
    "  x, traj_outputs = jax.lax.scan(gd_step, x, jnp.arange(nsteps))\n",
    "\n",
    "  xhat_final = self.lnorm(x)\n",
    "  E_final = self.et.energy(xhat_final)\n",
    "  traj_outputs['xhat'] = jnp.concatenate([traj_outputs['xhat'], xhat_final[None]], axis=0)\n",
    "  traj_outputs['energy'] = jnp.concatenate([traj_outputs['energy'], E_final[None]], axis=0)\n",
    "\n",
    "  xhat_final = xhat_final[1:]  # Discard CLS token for masked inpainting\n",
    "  x_decoded = self.decode(xhat_final)\n",
    "  return self.patcher.untokenify(x_decoded), traj_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note collapse=\"true\"}\n",
    "\n",
    "## Random initialization helper\n",
    "\n",
    "For completeness, let's add a helper function to initialize the model with random parameters. We won't use it in this tutorial, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(cls_method=True)\n",
    "def rand_init(cls: ImageEnergyTransformer, key, config=ImageETConfig()):\n",
    "  key1, key2, key3, key4, key5, key6, key7, key8 = jr.split(key, 8)\n",
    "  patcher = Patcher.from_img_shape(config.image_shape, config.patch_size)\n",
    "  W_emb = jr.normal(key1, (patcher.num_patch_elements, config.et_conf.D)) / config.et_conf.D\n",
    "  b_emb = jr.normal(key2, (config.et_conf.D,))\n",
    "  W_unemb = jr.normal(key3, (config.et_conf.D, patcher.num_patch_elements)) / patcher.num_patch_elements\n",
    "  b_unemb = jr.normal(key4, (patcher.num_patch_elements,))\n",
    "  pos_embed = jr.normal(key5, (patcher.num_patches, config.et_conf.D)) / config.et_conf.D\n",
    "  cls_token = 0.002 * jr.normal(key6, (config.et_conf.D,))\n",
    "  mask_token = 0.002 * jr.normal(key7, (config.et_conf.D,))\n",
    "  pos_embed = 0.002 * jr.normal(key8, (1 + patcher.num_patches, config.et_conf.D)) / config.et_conf.D\n",
    "\n",
    "  return cls(\n",
    "    patcher=patcher,\n",
    "    W_emb=W_emb,\n",
    "    b_emb=b_emb,\n",
    "    W_unemb=W_unemb,\n",
    "    b_unemb=b_unemb,\n",
    "    pos_embed=pos_embed,\n",
    "    cls_token=cls_token,\n",
    "    mask_token=mask_token,\n",
    "    et=EnergyTransformer.rand_init(key7, config.et_conf),\n",
    "    lnorm=EnergyLayerNorm(gamma=1., delta=jnp.zeros(config.et_conf.D)),\n",
    "    config=config\n",
    "  )\n",
    "\n",
    "imageET = ImageEnergyTransformer.rand_init(key, ImageETConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "### Loading pretrained weights\n",
    "\n",
    "ET has publicly available pretrained weights that can be used for masked-image inpainting. The model itself is pretty small ~20MB, with no compression tricks on the weights (everything is `np.float32`). We load the state dict from a saved `.npz` file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache\n",
    "def get_pretrained_et():\n",
    "  load_dict = {k: jnp.array(v) for k,v in get_et_checkpoint().items()}\n",
    "\n",
    "  # config from state_dict\n",
    "  H, Y, D = load_dict[\"Wk\"].shape\n",
    "  D, M = load_dict[\"Xi\"].shape\n",
    "\n",
    "  et_config = ETConfig(D=D, H=H, Y=Y, M=M, prevent_self_attention=False) # These weights were trained allowing self attention. But the arch works equally well both ways.\n",
    "  et = EnergyTransformer(\n",
    "    Wk = rearrange(load_dict[\"Wk\"], \"h y d -> h d y\"),\n",
    "    Wq = rearrange(load_dict[\"Wq\"], \"h y d -> h d y\"),\n",
    "    Xi = rearrange(load_dict[\"Xi\"], \"d m -> m d\"),\n",
    "    config = et_config\n",
    "  )\n",
    "\n",
    "  image_config = ImageETConfig(image_shape=(3, 224, 224), patch_size=16, et_conf=et_config)\n",
    "  patcher = Patcher.from_img_shape(image_config.image_shape, image_config.patch_size)\n",
    "  iet = ImageEnergyTransformer(\n",
    "    patcher = patcher,\n",
    "    W_emb = load_dict[\"Wenc\"],\n",
    "    b_emb = load_dict[\"Benc\"],\n",
    "    W_unemb = load_dict[\"Wdec\"],\n",
    "    b_unemb = load_dict[\"Bdec\"],\n",
    "    pos_embed = load_dict[\"POS_embed\"],\n",
    "    cls_token = load_dict[\"CLS_token\"],\n",
    "    mask_token = load_dict[\"MASK_token\"],\n",
    "    et = et,\n",
    "    lnorm = EnergyLayerNorm(gamma=load_dict[\"LNORM_gamma\"], delta=load_dict[\"LNORM_bias\"]),\n",
    "    config = image_config\n",
    "  )\n",
    "\n",
    "  return iet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inpaint images with ET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint_image(\n",
    "  iet: ImageEnergyTransformer, \n",
    "  img: Float[Array, \"C H W\"], \n",
    "  n_mask: int, \n",
    "  key: jax.random.PRNGKey, \n",
    "  nsteps: int=12, \n",
    "  step_size: float=0.1):\n",
    "    \" Perform masked image inpainting with Energy Transformer\"\n",
    "    # Create random mask\n",
    "    mask_idxs = jr.choice(\n",
    "        key, np.arange(iet.patcher.num_patches), shape=(n_mask,), replace=False\n",
    "    )\n",
    "    mask = jnp.zeros(iet.patcher.num_patches).at[mask_idxs].set(1)\n",
    "    \n",
    "    x = iet.patcher.tokenify(img)\n",
    "    x = iet.encode(x)  # Img to embedded tokens\n",
    "    x = iet.prep_tokens(x, mask)[1:]  # N,D (remove CLS token)\n",
    "    masked_img = iet.decode(iet.lnorm(x))\n",
    "    masked_img = iet.patcher.untokenify(masked_img)\n",
    "    \n",
    "    # Reconstruct image using Energy Transformer\n",
    "    recons_img, traj_outputs = iet(img, mask, nsteps=nsteps, step_size=step_size)\n",
    "    \n",
    "    return masked_img, recons_img, traj_outputs\n",
    "\n",
    "iet = get_pretrained_et()\n",
    "nh, nw = 2, 5\n",
    "N = nh*nw\n",
    "og_imgs = get_normalized_imgs()[:N]\n",
    "\n",
    "keys = jr.split(jr.PRNGKey(0), len(og_imgs))\n",
    "masked_imgs, recons_imgs, traj_outputs = jax.vmap(inpaint_image, in_axes=(None, 0, None, 0))(iet, og_imgs, 100, keys)\n",
    "\n",
    "vunnormalize_img = jax.vmap(unnormalize_img)\n",
    "og_imgs_show, masked_imgs_show, recons_imgs_show = [vunnormalize_img(im) for im in (og_imgs, masked_imgs, recons_imgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "# Rearrange for plotting\n",
    "stacked_imgs = jnp.stack([masked_imgs_show, recons_imgs_show, og_imgs_show], axis=0)\n",
    "pw = 4\n",
    "stacked_imgs_padded = jnp.pad(stacked_imgs, \n",
    "                              ((0,0), (0,0), (pw,pw), (0,0), (0,0)), \n",
    "                              mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "# Rearrange into a single grid: 3 rows (image types) × nh rows × nw columns\n",
    "combined_grid = rearrange(stacked_imgs_padded, \n",
    "                         \"t n h w c -> (t h) (n w) c\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.imshow(combined_grid / 255.)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Add row labels (horizontally readable, positioned to the left)\n",
    "h_per_row = combined_grid.shape[0] // 3\n",
    "for i, name in enumerate(['Masked Input', 'Reconstruction', 'Original Image']):\n",
    "  ax.text(-50, h_per_row // 2 + i * h_per_row, name, va='center', ha='right', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also animate the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Animation dependencies\"\n",
    "from pathlib import Path\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import Video, Markdown\n",
    "from moviepy.editor import ipython_display\n",
    "import os\n",
    "\n",
    "CACHE_DIR = Path(\"./cache\") / \"01_energy_transformer\"\n",
    "CACHE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CACHE_VIDEOS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| fig-label: Minimizing the energy of ET inpaints the masked tokens. All tokens are allowed to evolve during inference, and energy monotonically decreases each iteration.\n",
    "\n",
    "def show_et_recall_animation(iet, traj_outputs, cache_name, steps_per_sample=1, force_remake=False, fps=2):\n",
    "    \"Create animated video showing both image reconstruction evolution and energy descent\"\n",
    "    video_fname = CACHE_DIR / (cache_name + \".mp4\")\n",
    "\n",
    "    def decode_to_show(xhat): \n",
    "        decoded = iet.decode(xhat[1:]) # Drop CLS\n",
    "        decoded = iet.patcher.untokenify(decoded)\n",
    "        return unnormalize_img(decoded) \n",
    "\n",
    "    xhats = jax.vmap(jax.vmap(decode_to_show))(traj_outputs['xhat']) # Map over steps and img\n",
    "    xhats = rearrange(xhats, \"n t ...-> t n ...\")\n",
    "    energies = rearrange(traj_outputs['energy'], \"n t -> t n\")\n",
    "    \n",
    "    if not CACHE_VIDEOS or not video_fname.exists():\n",
    "        # Downsample frames and energies\n",
    "        sampled_xhats = xhats[::steps_per_sample]  # (t_sampled, n, h, w, c)\n",
    "        sampled_energies = energies[::steps_per_sample]  # (t_sampled, n)\n",
    "        sampled_steps = np.arange(0, len(energies), steps_per_sample)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Initialize image grid plot (left side)\n",
    "        # Create initial grid from first timestep\n",
    "        first_grid = rearrange(sampled_xhats[0], \"(nh nw) h w c -> (nh h) (nw w) c\", nh=nh, nw=nw)\n",
    "        im = ax1.imshow(first_grid / 255.)\n",
    "        ax1.set_title(\"Step 0\")\n",
    "        ax1.axis(\"off\")\n",
    "        \n",
    "        # Initialize energy plot (right side)  \n",
    "        n_images = sampled_energies.shape[1]\n",
    "        lines = []\n",
    "        balls = []\n",
    "        for i in range(n_images):\n",
    "            line, = ax2.plot([], [], alpha=0.8, linewidth=3)  # Made thicker\n",
    "            lines.append(line)\n",
    "            # Add a ball at the end of each line\n",
    "            ball = ax2.scatter([], [], s=50, alpha=0.9, zorder=5)\n",
    "            balls.append(ball)\n",
    "        \n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Energy')\n",
    "        ax2.set_title('Energy During Reconstruction')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xlim(0, sampled_steps[-1])\n",
    "        ax2.set_ylim(sampled_energies.min() * 1.1, sampled_energies.max() * 1.1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        def update(i):\n",
    "            # Update left subplot (image grid evolution)\n",
    "            current_grid = rearrange(sampled_xhats[i], \"(nh nw) h w c -> (nh h) (nw w) c\", nh=nh, nw=nw)\n",
    "            im.set_data(current_grid / 255.)\n",
    "            ax1.set_title(f\"Step {sampled_steps[i]}\")\n",
    "            \n",
    "            # Update right subplot (energy curves and balls)\n",
    "            current_steps = sampled_steps[:i+1]\n",
    "            for j, (line, ball) in enumerate(zip(lines, balls)):\n",
    "                current_energies = sampled_energies[:i+1, j]\n",
    "                line.set_data(current_steps, current_energies)\n",
    "                # Update ball position to current end point\n",
    "                if len(current_steps) > 0:\n",
    "                    ball.set_offsets([[sampled_steps[i], sampled_energies[i, j]]])\n",
    "                else:\n",
    "                    ball.set_offsets([])\n",
    "            \n",
    "            return [im] + lines + balls\n",
    "\n",
    "        anim = animation.FuncAnimation(fig, update, frames=len(sampled_xhats), \n",
    "                                     interval=100, blit=True)\n",
    "        \n",
    "        # Save as MP4\n",
    "        print(f\"Saving ET reconstruction animation to {video_fname}\")\n",
    "        anim.save(video_fname, writer='ffmpeg', fps=fps) \n",
    "        plt.close(fig)\n",
    "    \n",
    "    return Video(video_fname, width=800), video_fname\n",
    "\n",
    "# Create the animation\n",
    "video, video_fname = show_et_recall_animation(iet, traj_outputs, \"et_reconstruction\", \n",
    "                                             steps_per_sample=1, force_remake=True)\n",
    "Markdown(f\"![]({video_fname})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# For google colab only\n",
    "ipython_display(str(video_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are fully reconstructed using autograd down the parameterized energy function. You may notice the reconstructions are not perfect, e.g., the right eye of the white dog is missing.\n",
    "\n",
    ":::{.callout-warning collapse=\"true\"}\n",
    "\n",
    "## The energy is still decreasing! Shouldn't the images get better if we run longer?\n",
    "\n",
    "Unfortunately, these weights were only trained to 12 steps at a fixed step size. Running longer will still cause the energy to decrease, but our image reconstruction quality will not improve. This reflects that our model has learned a kind of 'metastable state' at which nice reconstructions are retrieved, but these reconstructions are not \"memories\" in the formal definition of the term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_imgs, recons_imgs, traj_outputs = jax.vmap(ft.partial(inpaint_image, nsteps=40), in_axes=(None, 0, None, 0))(iet, og_imgs, 100, keys)\n",
    "video, video_fname = show_et_recall_animation(iet, traj_outputs, \"et_reconstruction_long\", \n",
    "                                             steps_per_sample=1, force_remake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| fig-label: Because of training limitations, running for longer doesn't improve the reconstruction quality\n",
    "Markdown(f\"![]({video_fname})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# For google colab only\n",
    "ipython_display(str(video_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "## Interpreting ET\n",
    "\n",
    "The representations learned by ET are attractors of the dynamics. That is, the weights of the Hofield Network in ET are not arbitrary linear transformations --- they are actual stored data patterns. Visualizing the weights reveals what the model has actually learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_stored_pattern(iet, xi):\n",
    "  c,h,w = iet.patcher.patch_shape\n",
    "  decoded = iet.decode(iet.lnorm(xi))\n",
    "  patches = rearrange(decoded, '... (c h w) -> ... c h w', c=c, h=h, w=w)\n",
    "  return unnormalize_img(patches) \n",
    "\n",
    "Xi_show = jax.vmap(ft.partial(decode_stored_pattern, iet))(iet.et.Xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| fig-cap: Sampling the stored patterns in the Hopfield Network, sorted by frequency content\n",
    "\n",
    "def compute_frequency_score(patch):\n",
    "    \"Compute a score based on frequency content\"\n",
    "    gray = jnp.mean(patch, axis=-1)  # (h, w)\n",
    "    \n",
    "    # Compute 2D FFT\n",
    "    fft = jnp.fft.fft2(gray)\n",
    "    fft_magnitude = jnp.abs(fft)\n",
    "    \n",
    "    # Create frequency coordinates\n",
    "    h, w = gray.shape\n",
    "    freqs_h = jnp.fft.fftfreq(h)\n",
    "    freqs_w = jnp.fft.fftfreq(w)\n",
    "    fh, fw = jnp.meshgrid(freqs_h, freqs_w, indexing='ij')\n",
    "    \n",
    "    # Compute radial frequency (distance from DC component)\n",
    "    radial_freq = jnp.sqrt(fh**2 + fw**2)\n",
    "    \n",
    "    # Weight higher frequencies more to find patterns with edges/textures\n",
    "    # Also compute directional content (x and y frequency components)\n",
    "    high_freq_weight = (radial_freq > 0.1).astype(float)\n",
    "    x_directional = jnp.abs(fw) * fft_magnitude\n",
    "    y_directional = jnp.abs(fh) * fft_magnitude\n",
    "    \n",
    "    # Combine different frequency measures\n",
    "    high_freq_content = jnp.sum(fft_magnitude * high_freq_weight)\n",
    "    directional_content = jnp.sum(x_directional) + jnp.sum(y_directional)\n",
    "    total_energy = jnp.sum(fft_magnitude)\n",
    "    \n",
    "    # Avoid division by zero and normalize\n",
    "    score = (high_freq_content + 0.5 * directional_content) / (total_energy + 1e-8)\n",
    "    \n",
    "    # Also add a penalty for too-uniform patterns (low variance)\n",
    "    variance_penalty = jnp.var(gray)\n",
    "    \n",
    "    return score * (1 + 0.1 * variance_penalty)\n",
    "\n",
    "frequency_scores = jax.vmap(compute_frequency_score)(Xi_show)\n",
    "sorted_indices = jnp.argsort(frequency_scores)  # Ascending order (low to high)\n",
    "sorted_patterns = Xi_show[sorted_indices]\n",
    "\n",
    "# Select \"middle\" 2500 patterns (remove lowest and highest freq info)\n",
    "nh = nw = 50\n",
    "Nshow = nh * nw\n",
    "offset =300 # Show predominantly high freq\n",
    "selected_patterns = sorted_patterns[offset:Nshow+offset]\n",
    "\n",
    "# Create the visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "pattern_grid = rearrange(selected_patterns, \"(nh nw) h w c -> (nh h) (nw w) c\", nh=nh, nw=nw)\n",
    "ax.imshow(pattern_grid / 255.)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Sample of stored patterns\", fontsize=14, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip} \n",
    "\n",
    "## Interpretability by design \n",
    "\n",
    "You can think of the Hopfield Network like an SAE that is integrated into the core computation of the model. Interpretability is a natural byproduct of good architecture design.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "path": "nbs/tutorial/01_energy_transformer.qmd"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
