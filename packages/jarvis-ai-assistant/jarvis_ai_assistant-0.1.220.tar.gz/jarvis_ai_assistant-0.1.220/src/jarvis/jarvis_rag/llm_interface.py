from abc import ABC, abstractmethod
import os
import os
from abc import ABC, abstractmethod

from jarvis.jarvis_agent import Agent as JarvisAgent
from jarvis.jarvis_platform.base import BasePlatform
from jarvis.jarvis_platform.registry import PlatformRegistry


class LLMInterface(ABC):
    """
    Abstract Base Class for Large Language Model interfaces.

    This class defines the standard interface for interacting with a remote LLM.
    Any LLM provider (OpenAI, Anthropic, etc.) should be implemented as a
    subclass of this interface.
    """

    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """
        Generates a response from the LLM based on a given prompt.

        Args:
            prompt: The input prompt to send to the LLM.
            **kwargs: Additional keyword arguments for the LLM API call
                      (e.g., temperature, max_tokens).

        Returns:
            The text response generated by the LLM.
        """
        pass


class ToolAgent_LLM(LLMInterface):
    """
    An implementation of the LLMInterface that uses a tool-wielding JarvisAgent
    to generate the final response.
    """

    def __init__(self):
        """
        Initializes the Tool-Agent LLM wrapper.
        """
        print("🤖 已初始化工具 Agent 作为最终应答者。")
        self.allowed_tools = ["read_code", "execute_script"]
        # A generic system prompt for the agent
        self.system_prompt = "You are a helpful assistant. Please answer the user's question based on the provided context. You can use tools to find more information if needed."
        self.summary_prompt = """
<report>
请为本次问答任务生成一个总结报告，包含以下内容：

1. **原始问题**: 重述用户最开始提出的问题。
2. **关键信息来源**: 总结你是基于哪些关键信息或文件得出的结论。
3. **最终答案**: 给出最终的、精炼的回答。
</report>
"""

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Runs the JarvisAgent with a restricted toolset to generate an answer.

        Args:
            prompt: The full prompt, including context, to be sent to the agent.
            **kwargs: Ignored, kept for interface compatibility.

        Returns:
            The final answer generated by the agent.
        """
        try:
            # Initialize the agent with specific settings for RAG context
            agent = JarvisAgent(
                system_prompt=self.system_prompt,
                use_tools=self.allowed_tools,
                auto_complete=True,
                use_methodology=False,
                use_analysis=False,
                need_summary=True,
                summary_prompt=self.summary_prompt,
            )

            # The agent's run method expects the 'user_input' parameter
            final_answer = agent.run(user_input=prompt)
            return str(final_answer)

        except Exception as e:
            print(f"❌ Agent 在执行过程中发生错误: {e}")
            return "错误: Agent 未能成功生成回答。"


class JarvisPlatform_LLM(LLMInterface):
    """
    An implementation of the LLMInterface for the project's internal platform.

    This class uses the PlatformRegistry to get the configured "normal" model.
    """

    def __init__(self):
        """
        Initializes the Jarvis Platform LLM client.
        """
        try:
            self.registry = PlatformRegistry.get_global_platform_registry()
            self.platform: BasePlatform = self.registry.get_normal_platform()
            self.platform.set_suppress_output(
                False
            )  # Ensure no console output from the model
            print(f"🚀 已初始化 Jarvis 平台 LLM，模型: {self.platform.name()}")
        except Exception as e:
            print(f"❌ 初始化 Jarvis 平台 LLM 失败: {e}")
            raise

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Sends a prompt to the local platform model and returns the response.

        Args:
            prompt: The user's prompt.
            **kwargs: Ignored, kept for interface compatibility.

        Returns:
            The response generated by the platform model.
        """
        try:
            # Use the robust chat_until_success method
            return self.platform.chat_until_success(prompt)
        except Exception as e:
            print(f"❌ 调用 Jarvis 平台模型时发生错误: {e}")
            return "错误: 无法从本地LLM获取响应。"
