{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview: Synthetic Data Generation for Document-Grounded Reasoning\n",
    "\n",
    "This notebook demonstrates how to build a **synthetic dataset specifically designed to train reasoning-capable language models**, using the `sdg_hub` pipeline. It focuses on helping a student model learn *how to reason* over complex documents by mimicking the structured thinking patterns of a large teacher LLM.\n",
    "\n",
    "### Key Highlights:\n",
    "\n",
    "* **Reasoning-Centric Pipeline**: The entire flow is optimized to generate examples that reflect *step-by-step reasoning*, rather than simple factual recall or shallow Q\\&A.\n",
    "* **Hands-On, Step-by-Step Construction**: We walk through building a reasoning-focused synthetic data pipeline—from swapping the teacher model to progressively modifying the knowledge flow for generating \"thinking data.\"\n",
    "* **Pipeline Extension with New Blocks**: The notebook shows how to extend an existing flow by injecting new functional blocks—such as post-processing to clean \"think\" prompts or regex parsers to structure model outputs.\n",
    "* **Grounded and Faithful Outputs**: Reasoning outputs remain aligned with the source documents, minimizing hallucinations and ensuring factual consistency.\n",
    "\n",
    "This notebook is ideal for researchers and engineers using `sdg_hub` to train or fine-tune models that reason—rather than simply respond—when tackling document-grounded tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let first instantiate teacher model using OpenAI client. We assume the teacher model is already deployed on vllm server.\n",
    "# In this case we will be using a Nemotron Super model.\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "\n",
    "endpoint = f\"http://localhost:8000/v1\"\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = endpoint\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "teacher_model = client.models.list().data[0].id\n",
    "\n",
    "teacher_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by first importing all components we will need for running SDG: the Flow, and the SDG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdg_hub.flow import Flow\n",
    "from sdg_hub.sdg import SDG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this notebook is to perform **knowledge tuning using a reasoning model**. In the previous knowledge tuning notebook, we saw how to fine-tune a model on a user’s document to improve its factual recall. Here, we extend that approach by training a *reasoning model* on the same document—so the model doesn't just remember facts, but also learns to **reason through the content**.\n",
    "\n",
    "We begin by **swapping the teacher model** to `Nemotron Super`, which will be used to generate high-quality reasoning data.\n",
    "\n",
    "If this model isn't already registered in `sdg_hub`, we’ll need to add it to the **Prompt Registry**. You can check the currently supported templates in `src/sdg_hub/prompts.py`.\n",
    "\n",
    "To register a new prompt template:\n",
    "\n",
    "* Import the `PromptRegistry` class.\n",
    "* Create a function that returns the chat template for your model (obtained via `tokenizer.chat_template`).\n",
    "* Use the `@PromptRegistry.register(...)` decorator to register it, as shown below.\n",
    "\n",
    "In this example, we instantiate the tokenizer for Nemotron Super and extract its chat formatting logic for integration into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdg_hub.prompts import PromptRegistry\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama-3_3-Nemotron-Super-49B-v1\")\n",
    "nemotron_chat_template = tokenizer.chat_template\n",
    "nemotron_chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```jinja\n",
    "'{{- bos_token }}{%- if messages[0][\\'role\\'] == \\'system\\' %}{%- set system_message = messages[0][\\'content\\']|trim %}{%- set messages = messages[1:] %}{%- else %}{%- set system_message = \"\" %}{%- endif %}{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}{{- system_message }}{{- \"<|eot_id|>\" }}{%- for message in messages %}{%- if message[\\'role\\'] == \\'assistant\\' and \\'</think>\\' in message[\\'content\\'] %}{%- set content = message[\\'content\\'].split(\\'</think>\\')[-1].lstrip() %}{%- else %}{%- set content = message[\\'content\\'] %}{%- endif %}{{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\' + content | trim + \\'<|eot_id|>\\' }}{%- endfor %}{%- if add_generation_prompt %}{{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}{%- endif %}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Nemotron Super` model supports two reasoning modes controlled via the system message:\n",
    "\n",
    "* **Detailed Thinking ON**: The model is instructed to *think step-by-step before answering* (i.e., \"think before you answer\").\n",
    "* **Detailed Thinking OFF**: The model is prompted to *respond directly without internal reasoning*.\n",
    "\n",
    "For our use case, we want to **always enable detailed thinking** to ensure that the teacher model generates rich, multi-step reasoning traces. To achieve this, we will **modify the chat template’s Jinja structure** so that the system message consistently sets `detailed thinking: on`.\n",
    "\n",
    "This updated template will then be returned in our registered chat template function and integrated into the prompt registry, ensuring every prompt sent to Nemotron Super reflects our reasoning-first configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@PromptRegistry.register(\"nvidia/Llama-3_3-Nemotron-Super-49B-v1\")\n",
    "def nemotron_chat_template():\n",
    "    return \"\"\"{{- bos_token }}\n",
    "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}detailed thinking on{{- \"<|eot_id|>\" }}\n",
    "{%- for message in messages %}\n",
    "  {%- if message['role'] == 'assistant' and '</think>' in message['content'] %}\n",
    "    {%- set content = message['content'].split('</think>')[-1].lstrip() %}\n",
    "  {%- else %}\n",
    "    {%- set content = message['content'] %}\n",
    "  {%- endif %}\n",
    "  {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + content | trim + '<|eot_id|>' }}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "  {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
    "{%- endif %}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the corpus and combine it with the seed examples to construct the **final seed dataset**.\n",
    "\n",
    "In `sdg_hub`, the seed data must adhere to a **strict column naming convention**:\n",
    "Each column name must **exactly match** the variable names expected in the prompt template of the first `LLMBlock` in the flow.\n",
    "\n",
    "This alignment is crucial—every subsequent block in the pipeline relies on the output columns of the previous block to match its own input variable names. This ensures that prompts are populated correctly and that the flow remains **fully compatible and composable** from block to block.\n",
    "\n",
    "Maintaining this column-to-variable consistency is essential for smooth data propagation and accurate prompt construction across the entire synthetic data generation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document_outline', 'document', 'icl_document', 'icl_query_1', 'icl_response_1', 'icl_query_2', 'icl_response_2', 'icl_query_3', 'icl_response_3'],\n",
       "    num_rows: 263\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple dataset\n",
    "from datasets import load_dataset\n",
    "quality_corpus = load_dataset(\"zitongyang/entigraph-quality-corpus\", split='train').remove_columns(['entity', 'entigraph']).rename_columns({'raw': 'document', 'uid': 'document_outline'})\n",
    "\n",
    "# For knowledge tuning we need seed examples (teacher model will use this template to generate more data) to generate synthetic data. Write example QA/seed_examples\n",
    "seed_examples = {\n",
    "    \"icl_document\": (\n",
    "      \"The coastal town of Willow Creek, once renowned for its pristine beaches, now struggles with rampant pollution. Plastic debris and oil spills have devastated marine life, prompting a decline in tourism and fishing industries. Residents have organized weekly clean-up initiatives, but the scale of the problem overwhelms their efforts.\",\n",
    "      \"Technologists at the local university have developed an AI-powered buoy system to combat this. The buoys, equipped with solar panels and filtration technology, can identify and absorb oil spills while collecting microplastics. Data from the buoys is shared publicly, raising awareness and pressuring corporations to adopt sustainable practices. Though costly, the project has sparked hope for revitalizing the ecosystem and economy.\"\n",
    "    ),\n",
    "    \"icl_query_1\": \"How does the technological solution address the economic *and* environmental challenges highlighted in the document?\",\n",
    "    \"icl_response_1\": \"The buoys directly mitigate environmental harm by absorbing spills and collecting plastics, which could restore marine life. Economically, this restoration might revive tourism and fishing, reversing their decline. Public data-sharing also pressures corporations, potentially reducing pollution and creating a sustainable economic cycle aligned with environmental health.\",\n",
    "    \n",
    "    \"icl_query_2\": \"What implicit values or priorities do the community’s actions (clean-up initiatives) and the technologists’ project reflect, and how do these align or contrast?\",\n",
    "    \"icl_response_2\": \"Both reflect a prioritization of environmental stewardship and community engagement. The clean-ups emphasize collective responsibility, while the buoy project highlights innovation and systemic accountability (targeting corporations). They align in seeking solutions but contrast in scale and approach—manual vs. technological, local vs. systemic.\",\n",
    "    \n",
    "    \"icl_query_3\": \"Imagine the buoy project succeeds. What unintended consequences might arise from its impact, considering document's themes?\",\n",
    "    \"icl_response_3\": \"Success could lead to reduced community participation in clean-up efforts if residents perceive the technology as a sufficient solution. Economically, revitalization might exacerbate existing resource strains (e.g., overwhelming local infrastructure) or create dependency on the buoy system, undermining long-term sustainability if the technology falters.\"\n",
    "\n",
    "}\n",
    "\n",
    "# Add seed examples to the our corpus\n",
    "quality_corpus = quality_corpus.map(lambda x: seed_examples)\n",
    "quality_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will take our knowledge flow and update the model-id to work with new registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# Load the exiting knowledge flow\n",
    "with open(\"../../src/sdg_hub/flows/generation/knowledge/synth_knowledge1.5.yaml\", \"r\") as f:\n",
    "    flow_config = yaml.safe_load(f)\n",
    "\n",
    "# Pretty print the flow\n",
    "print(yaml.dump(flow_config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "-   block_config:\n",
    "        block_name: duplicate_document_col\n",
    "        columns_map:\n",
    "            document: base_document\n",
    "    block_type: DuplicateColumns\n",
    "-   block_config:\n",
    "        block_name: gen_detailed_summary\n",
    "        config_path: configs/knowledge/detailed_summary.yaml\n",
    "        model_id: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "        output_cols:\n",
    "        - summary_detailed\n",
    "    block_type: LLMBlock\n",
    "    gen_kwargs:\n",
    "        max_tokens: 4096\n",
    "        n: 50\n",
    "        seed: 7452\n",
    "        temperature: 0.7\n",
    "-   block_config:\n",
    "        block_name: gen_atomic_facts\n",
    "        config_path: configs/knowledge/atomic_facts.yaml\n",
    "        model_id: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "        output_cols:\n",
    "        - summary_atomic_facts\n",
    "    block_type: LLMBlock\n",
    "    gen_kwargs:\n",
    "        max_tokens: 4096\n",
    "        seed: 7452\n",
    "        temperature: 0.7\n",
    "-   block_config:\n",
    "        block_name: gen_extractive_summary\n",
    "        config_path: configs/knowledge/extractive_summary.yaml\n",
    "        model_id: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "        output_cols:\n",
    "        - summary_extractive\n",
    "    block_type: LLMBlock\n",
    "    gen_kwargs:\n",
    "        max_tokens: 4096\n",
    "        seed: 7452\n",
    "        temperature: 0.7\n",
    "-   block_config:\n",
    "        block_name: flatten_summary_columns\n",
    "        value_name: summary\n",
    "        var_cols:\n",
    "        - summary_detailed\n",
    "        - summary_extractive\n",
    "        - summary_atomic_facts\n",
    "        - base_document\n",
    "        var_name: dataset_type\n",
    "    block_type: FlattenColumnsBlock\n",
    "-   block_config:\n",
    "        block_name: rename_to_document_column\n",
    "        columns_map:\n",
    "            document: raw_document\n",
    "            summary: document\n",
    "    block_type: RenameColumns\n",
    "-   block_config:\n",
    "        block_name: knowledge generation\n",
    "        config_path: configs/knowledge/generate_questions.yaml\n",
    "        model_id: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "        output_cols:\n",
    "        - question\n",
    "        parser_kwargs:\n",
    "            parser_name: custom\n",
    "            parsing_pattern: \\[(?:Question|QUESTION)\\]\\s*(.*?)\\s*(?=\\[(?:Question|QUESTION)\\]|$)\n",
    "    block_type: LLMBlock\n",
    "    gen_kwargs:\n",
    "        max_tokens: 100\n",
    "        seed: 7452\n",
    "        temperature: 0.7\n",
    "-   block_config:\n",
    "        block_name: knowledge generation\n",
    "        config_path: configs/knowledge/generate_responses.yaml\n",
    "        model_id: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "        output_cols:\n",
    "        - response\n",
    "    block_type: LLMBlock\n",
    "    gen_kwargs:\n",
    "        max_tokens: 2048\n",
    "        seed: 7452\n",
    "        temperature: 0.7\n",
    "-   block_config:\n",
    "        block_name: eval_faithfulness_qa_pair\n",
    "        config_path: configs/knowledge/evaluate_faithfulness.yaml\n",
    "        model_id: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "        output_cols:\n",
    "        - explanation\n",
    "        - judgment\n",
    "    block_type: LLMBlock\n",
    "    gen_kwargs:\n",
    "        max_tokens: 2048\n",
    "-   block_config:\n",
    "        block_name: filter_faithfulness\n",
    "        filter_column: judgment\n",
    "        filter_value: 'YES'\n",
    "        operation: operator.eq\n",
    "    block_type: FilterByValueBlock\n",
    "    drop_columns:\n",
    "    - judgment\n",
    "    - explanation\n",
    "-   block_config:\n",
    "        block_name: eval_relevancy_qa_pair\n",
    "        config_path: configs/knowledge/evaluate_relevancy.yaml\n",
    "        model_id: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "        output_cols:\n",
    "        - feedback\n",
    "        - score\n",
    "    block_type: LLMBlock\n",
    "    gen_kwargs:\n",
    "        max_tokens: 2048\n",
    "-   block_config:\n",
    "        block_name: filter_relevancy\n",
    "        convert_dtype: float\n",
    "        filter_column: score\n",
    "        filter_value: 2.0\n",
    "        operation: operator.eq\n",
    "    block_type: FilterByValueBlock\n",
    "    drop_columns:\n",
    "    - feedback\n",
    "    - score\n",
    "-   block_config:\n",
    "        block_name: eval_verify_question\n",
    "        config_path: configs/knowledge/evaluate_question.yaml\n",
    "        model_id: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "        output_cols:\n",
    "        - explanation\n",
    "        - rating\n",
    "    block_type: LLMBlock\n",
    "    gen_kwargs:\n",
    "        max_tokens: 2048\n",
    "-   block_config:\n",
    "        block_name: filter_verify_question\n",
    "        convert_dtype: float\n",
    "        filter_column: rating\n",
    "        filter_value: 1.0\n",
    "        operation: operator.eq\n",
    "    block_type: FilterByValueBlock\n",
    "    drop_columns:\n",
    "    - explanation\n",
    "    - rating\n",
    "    - __index_level_0__\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model ID to be used across blocks\n",
    "MODEL_ID = \"nvidia/Llama-3_3-Nemotron-Super-49B-v1\"\n",
    "\n",
    "# Print the first block\n",
    "print(\"Original model-id:\",flow_config[1])\n",
    "# Update the model-id\n",
    "for block in flow_config:\n",
    "    if block['block_type'] == 'LLMBlock':\n",
    "        block['block_config']['model_id'] = MODEL_ID\n",
    "\n",
    "# Print the first block\n",
    "print(\"Updated model-id:\",flow_config[1])\n",
    "\n",
    "# Save the updated flow\n",
    "with open(\"flows/synth_knowledge1.5_updated.yaml\", \"w\") as f:\n",
    "    yaml.dump(flow_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the flow\n",
    "knowledge_1_5_flow = Flow(client).get_flow_from_file(\"flows/synth_knowledge1.5_updated.yaml\")\n",
    "knowledge_1_5_sdg = SDG(\n",
    "    flows=[knowledge_1_5_flow],\n",
    "    num_workers=1,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "# Generate summaries\n",
    "generated_summaries = knowledge_1_5_sdg.generate(quality_corpus.select([1]), checkpoint_dir=\"simple_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated Summary:\", generated_summaries[0]['document'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Generated Summary:\n",
    "<think>\n",
    "Okay, let's start by reading through the document carefully. The title is \"Fight Clubbed\" by David Plotz, from 1999. The main point seems to be comparing the movie Fight Club with the real Ultimate Fighting Championship (UFC) and how the ...</think>\n",
    "\n",
    "**Detailed Summary of \"Fight Clubbed\" by David Plotz (1999)**\n",
    "...\n",
    "- Contrast with *Fight Club*’s cultural impact.  \n",
    "- Underground survival of the sport.  \n",
    "\n",
    "**No New Information Added; All Document Points Retained.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding PostProcessThinkingBlock\n",
    "\n",
    "The PostProcessThinkingBlock is a custom block that helps process the output from models that use thinking/reasoning tags in their responses. This block is particularly useful when working with models that show their reasoning process using special tags like `<think>` and `</think>`. This the text between the thinking tags\n",
    "\n",
    "Here's how to create and use a PostProcessThinkingBlock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.conda/envs/research_sdg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sdg_hub.blocks import BlockRegistry, Block\n",
    "from datasets import Dataset\n",
    "\n",
    "@BlockRegistry.register(\"PostProcessThinkingBlock\")\n",
    "class PostProcessThinkingBlock(Block):\n",
    "    def __init__(self, block_name: str, column_name: str) -> None:\n",
    "        super().__init__(block_name=block_name)  \n",
    "        self.column_name = column_name\n",
    "    \n",
    "    def generate(self, samples: Dataset):\n",
    "        def post_process_thinking(x):\n",
    "            if '</think>' in x[self.column_name]:\n",
    "                x[self.column_name] = x[self.column_name].split('</think>')[-1].lstrip()\n",
    "            return x\n",
    "        samples = samples.map(post_process_thinking)\n",
    "        return samples\n",
    "\n",
    "# Helper function to create a post-processing block configuration\n",
    "def create_postprocess_block(block_name, column_name):\n",
    "    return {\n",
    "        \"block_type\": \"PostProcessThinkingBlock\",\n",
    "        \"block_config\": {\n",
    "            \"block_name\": block_name,\n",
    "            \"column_name\": column_name\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this block in a flow, you would add it after an LLMBlock that generates the thinking tags (<think>...</think>) output:\n",
    "\n",
    "```yaml\n",
    "- block_config:\n",
    "    block_name: duplicate_document_col\n",
    "    columns_map:\n",
    "      document: base_document\n",
    "  block_type: DuplicateColumns\n",
    "- block_config:\n",
    "    block_name: gen_detailed_summary\n",
    "    config_path: configs/knowledge/detailed_summary.yaml\n",
    "    model_id: nvidia/Llama-3_3-Nemotron-Super-49B-v1\n",
    "    output_cols:\n",
    "    - summary_detailed\n",
    "  block_type: LLMBlock\n",
    "  gen_kwargs:\n",
    "    max_tokens: 4096\n",
    "    n: 50\n",
    "    seed: 7452\n",
    "    temperature: 0.7\n",
    "\n",
    "- block_type: PostProcessThinkingBlock\n",
    "  block_config:\n",
    "    block_name: process_thinking\n",
    "    column_name: summary_detailed\n",
    "```\n",
    "\n",
    "The PostProcessThinkingBlock:\n",
    "1. Takes a column name as input that contains the thinking output\n",
    "2. Looks for the `</think>` tag in the text\n",
    "3. If found, removes everything before and including the `</think>` tag\n",
    "4. Returns the cleaned output\n",
    "\n",
    "This is particularly useful when you want to:\n",
    "- Remove the model's thinking process from the final output\n",
    "- Clean up responses that include reasoning steps\n",
    "- Extract only the final answer from a chain-of-thought response\n",
    "\n",
    "Next lets update the flow yaml to use the post-process thinking Block. \n",
    "\n",
    "We want to remove all the thinking block from all responses except the LLMBlock that generates the responses for the generated question. This is important as we want to train a student reasoning model to always output in this format:\n",
    "```text\n",
    "Question:\n",
    "...question...\n",
    "Model Response:\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "...final answer\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the flow to use the PostProcessThinkingBlock\n",
    "with open(\"flows/synth_knowledge1.5_updated.yaml\", \"r\") as f:\n",
    "    flow_config = yaml.safe_load(f)\n",
    "\n",
    "# Add the PostProcessThinkingBlock after all LLMBlocks except the LLMBlock that generates the response\n",
    "new_flow_config = []\n",
    "for block in flow_config:\n",
    "    \n",
    "    # Add post-process thinking block after the LLMBlock except for the generate_response block\n",
    "    new_flow_config.append(block)\n",
    "    if block['block_type'] != 'LLMBlock' :\n",
    "        continue\n",
    "    if 'generate_questions.yaml' in block['block_config']['config_path']:\n",
    "        new_flow_config[-1]['gen_kwargs']['max_tokens'] = 1024\n",
    "\n",
    "    # Add post-process thinking block after the LLMBlock except for the generate_response block\n",
    "    if 'generate_responses.yaml' not in block['block_config']['config_path']:\n",
    "            new_flow_config.append(\n",
    "                create_postprocess_block(\n",
    "                    block_name=\"process_thinking\",\n",
    "                    column_name=block['block_config']['output_cols'][0]\n",
    "                    )\n",
    "                    )\n",
    "\n",
    "# Save the updated flow\n",
    "with open(\"flows/synth_knowledge1.5_updated_with_post_process_thinking.yaml\", \"w\") as f:\n",
    "    yaml.dump(new_flow_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the flow\n",
    "knowledge_1_5_flow = Flow(client).get_flow_from_file(\"flows/synth_knowledge1.5_updated_with_post_process_thinking.yaml\")\n",
    "knowledge_1_5_sdg = SDG(\n",
    "    flows=[knowledge_1_5_flow],\n",
    "    num_workers=1,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "# Generate summaries\n",
    "generated_summaries = knowledge_1_5_sdg.generate(quality_corpus.select([1]), checkpoint_dir=\"simple_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to update  out pipeline to fix the parsing order of the model response.\n",
    "\n",
    "### Current Setup\n",
    "\n",
    "Right now, the typical definition of our **LLMBlock** for generating questions looks like this:\n",
    "\n",
    "```yaml\n",
    "- block_config:\n",
    "    block_name: knowledge generation\n",
    "    config_path: configs/knowledge/generate_questions.yaml\n",
    "    model_id: nvidia/Llama-3_3-Nemotron-Super-49B-v1\n",
    "    output_cols:\n",
    "      - question\n",
    "    parser_kwargs:\n",
    "      parser_name: custom\n",
    "      parsing_pattern: \\[(?:Question|QUESTION)\\]\\s*(.*?)\\s*(?=\\[(?:Question|QUESTION)\\]|$)\n",
    "  block_type: LLMBlock\n",
    "  gen_kwargs:\n",
    "    max_tokens: 100\n",
    "    seed: 7452\n",
    "    temperature: 0.7\n",
    "- block_type: PostProcessThinkingBlock\n",
    "  block_config:\n",
    "    block_name: process_thinking\n",
    "    column_name: question\n",
    "```\n",
    "\n",
    "This block is currently followed by a **post-processing block**, which comes *after* regex parsing.\n",
    "\n",
    "### Current Flow\n",
    "\n",
    "```\n",
    "LLM Output (e.g., \"<think>...</think>Q1 ... Q2 ... Q3 ...\") \n",
    "→ Regex Parse (extract [Q1, Q2, Q3, ...]) \n",
    "→ Post-Process Thinking (e.g., clean or enrich Q1, Q2, Q3)\n",
    "```\n",
    "\n",
    "### The Problem\n",
    "\n",
    "This ordering introduces ambiguity in the downstream output. The regex may extract incomplete or malformed questions, especially if the `<think>` section or question formatting isn't perfectly consistent. Additionally, applying post-processing *after* parsing can cause inconsistencies in the final output quality.\n",
    "\n",
    "### Proposed Change\n",
    "\n",
    "We want to **remove the post-processing block after regex parsing** and instead **apply post-processing directly on the raw LLM output before parsing**. This ensures better control and guarantees the integrity of the final parsed questions.\n",
    "\n",
    "### Updated Flow\n",
    "\n",
    "```\n",
    "LLM Output (e.g., \"<think>...</think>Q1 ... Q2 ... Q3 ...\") \n",
    "→ Post-Process Thinking (clean/improve Q1, Q2, Q3 format in raw text) \n",
    "→ Regex Parse (extract [Q1, Q2, Q3, ...])\n",
    "```\n",
    "\n",
    "This adjustment improves reliability and consistency across generated data samples.\n",
    "\n",
    "\n",
    "How do we achieve this:\n",
    "* We will first remove all the regex parsing from the LLM Block by removing the `parser_kwargs` from the corresponding LLM Blocks\n",
    "* We will introduce new Block for regular expression and add it after the LLMBlock, PostProcessThinkingBlock pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "@BlockRegistry.register(\"RegexParserBlock\")\n",
    "class RegexParserBlock(Block):\n",
    "    \"\"\"Block for parsing text using regular expressions and cleaning up tags.\n",
    "    \n",
    "    This block takes text input, applies regex pattern matching to extract structured data,\n",
    "    and optionally cleans up specified tags from the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, block_name: str, column_name: str, parsing_pattern: str=\"\", parser_cleanup_tags: List[str]=[], output_cols: List[str]=[]) -> None:\n",
    "        \"\"\"Initialize the RegexParserBlock.\n",
    "        \n",
    "        Args:\n",
    "            block_name: Name identifier for this block\n",
    "            column_name: Name of column containing text to parse\n",
    "            parsing_pattern: Regex pattern to use for parsing\n",
    "            parser_cleanup_tags: List of tags to remove from parsed output\n",
    "            output_cols: Names of columns to store parsed outputs\n",
    "        \"\"\"\n",
    "        super().__init__(block_name=block_name)\n",
    "        self.column_name = column_name\n",
    "        self.parsing_pattern = parsing_pattern\n",
    "        self.parser_cleanup_tags = parser_cleanup_tags\n",
    "        self.output_cols = output_cols\n",
    "\n",
    "    def generate(self, samples: Dataset):\n",
    "        \"\"\"Process samples by parsing text and cleaning tags.\n",
    "        \n",
    "        Args:\n",
    "            samples: Dataset containing samples to process\n",
    "            \n",
    "        Returns:\n",
    "            Dataset with parsed and cleaned outputs\n",
    "        \"\"\"\n",
    "        if self.parsing_pattern:\n",
    "            # Parse text using regex pattern\n",
    "            new_data = []\n",
    "            for sample in samples:\n",
    "                parsed_outputs = self._parse(sample[self.column_name])\n",
    "                \n",
    "                # Align all outputs to same length by taking max length\n",
    "                max_length = max(len(value) for value in parsed_outputs.values())\n",
    "                for values in zip(*(lst[:max_length] for lst in parsed_outputs.values())):\n",
    "                    new_data.append({**sample, **dict(zip(parsed_outputs.keys(), values))})\n",
    "            samples = Dataset.from_list(new_data)\n",
    "            \n",
    "        # Clean up any specified tags from output\n",
    "        if self.parser_cleanup_tags:\n",
    "            for clean_tag in self.parser_cleanup_tags:\n",
    "               samples = samples.map(lambda x: {column_name: x[column_name].replace(clean_tag, \"\") for column_name in self.output_cols})\n",
    "        return samples\n",
    "\n",
    "    def _parse(self, generated_string):\n",
    "        \"\"\"Parse text using regex pattern to extract structured data.\n",
    "        \n",
    "        Args:\n",
    "            generated_string: Text string to parse\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping output column names to lists of parsed values\n",
    "        \"\"\"\n",
    "        pattern = re.compile(self.parsing_pattern, re.DOTALL)\n",
    "        all_matches = pattern.findall(generated_string)\n",
    "        matches = {column_name: [] for column_name in self.output_cols}\n",
    "        \n",
    "        # Handle both single group and multiple capture group matches\n",
    "        if all_matches and isinstance(all_matches[0], tuple):\n",
    "            for match in all_matches:\n",
    "                for column_name, value in zip(self.output_cols, match):\n",
    "                    value = value.strip()\n",
    "                    matches[column_name].append(value)\n",
    "        else:\n",
    "            matches[self.output_cols[0]] = (\n",
    "                [match.strip() for match in all_matches] if all_matches else []\n",
    "            )\n",
    "        return matches\n",
    "\n",
    "# Helper function to create a regex parser block configuration\n",
    "def create_regex_parser_block(block_name, column_name, parsing_pattern, cleanup_tags, output_cols):\n",
    "    return {\n",
    "        \"block_type\": \"RegexParserBlock\",\n",
    "        \"block_config\": {\n",
    "            \"block_name\": block_name,\n",
    "            \"column_name\": column_name,\n",
    "            \"parsing_pattern\": parsing_pattern,\n",
    "            \"parser_cleanup_tags\": cleanup_tags,\n",
    "            \"output_cols\": copy.deepcopy(output_cols)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Update the flow to use the new RegexParserBlock\n",
    "with open(\"flows/synth_knowledge1.5_updated.yaml\", \"r\") as f:\n",
    "    flow_config = yaml.safe_load(f)\n",
    "\n",
    "regex_patterns = {\n",
    "    \"configs/knowledge/generate_questions.yaml\": (\"\\\\[(?:Question|QUESTION)\\\\]\\\\s*(.*?)\\\\s*(?=\\\\[(?:Question|QUESTION)\\\\]|$)\", [\"[END]\"]),\n",
    "    \"configs/knowledge/generate_answers.yaml\": (\"\", [\"[END]\", \"[ANSWER]\", \"assistant\"]),\n",
    "}\n",
    "new_flow_config = []\n",
    "# Update the flow to use the new RegexParserBlock\n",
    "for block in flow_config:\n",
    "    \n",
    "    # Add post-process thinking block after the LLMBlock except for the generate_response block\n",
    "    new_flow_config.append(block)\n",
    "    if block['block_type'] != 'LLMBlock' :\n",
    "        continue\n",
    "    if 'generate_questions.yaml' in block['block_config']['config_path']:\n",
    "        new_flow_config[-1]['gen_kwargs']['max_tokens'] = 1024\n",
    "    # Remove parser_kwargs for generate_answers.yaml and generate_questions.yaml\n",
    "    if 'generate_answers.yaml' in block['block_config']['config_path'] or 'generate_questions.yaml' in block['block_config']['config_path']:\n",
    "        del block['block_config']['parser_kwargs']\n",
    "\n",
    "    # Add post-process thinking block after the LLMBlock except for the generate_response block\n",
    "    if 'generate_responses.yaml' not in block['block_config']['config_path']:\n",
    "            # Create post-processing block for summaries\n",
    "            new_flow_config.append(\n",
    "                create_postprocess_block(\n",
    "                    block_name=\"process_thinking\",\n",
    "                    column_name=block['block_config']['output_cols'][0]\n",
    "                    )\n",
    "                    )\n",
    "    # Add regex parser block after the post-process thinking block\n",
    "    if 'generate_answers.yaml' in block['block_config']['config_path'] or 'generate_questions.yaml' in block['block_config']['config_path']:\n",
    "        new_flow_config.append(\n",
    "            create_regex_parser_block(\n",
    "                block_name=\"regex_parser\",\n",
    "                column_name=block['block_config']['output_cols'][0],\n",
    "                parsing_pattern=regex_patterns[block['block_config']['config_path']][0],\n",
    "                cleanup_tags=regex_patterns[block['block_config']['config_path']][1],\n",
    "                output_cols=block['block_config']['output_cols']\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Save the updated flow\n",
    "with open(\"flows/synth_knowledge1.5_updated_with_post_process_thinking_and_regex_parser.yaml\", \"w\") as f:\n",
    "    yaml.dump(new_flow_config, f, default_style='\"')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also notice that we added a `parser_cleanup_tags` parameter to our `RegexParserBlock`. This works similarly to the `parser_cleanup_tags` used in `LLMBlock`. It defines a list of tags or strings that may appear in the model’s response and should be **cleaned up before parsing**. Internally, this is equivalent to applying:\n",
    "\n",
    "```python\n",
    "response = response.replace(tag, \"\")\n",
    "```\n",
    "\n",
    "for each tag in the `parser_cleanup_tags` list. This ensures cleaner and more consistent outputs, especially when the model appends artifacts like `[END]` or `[THINK]`.\n",
    "\n",
    "Additionally, you might observe that the `parsing_pattern` for the `generate_answer` block is intentionally left **empty**. This is because we want to **retain the raw output** of the model without applying any regex-based extraction—useful when no structural parsing is required or when the output is already in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the flow\n",
    "knowledge_1_5_flow = Flow(client).get_flow_from_file(\"flows/synth_knowledge1.5_updated_with_post_process_thinking_and_regex_parser.yaml\")\n",
    "knowledge_1_5_sdg = SDG(\n",
    "    flows=[knowledge_1_5_flow],\n",
    "    num_workers=1,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "# Generate summaries\n",
    "generated_summaries = knowledge_1_5_sdg.generate(quality_corpus.select([1]), checkpoint_dir=\"simple_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What implicit double standards in American cultural and political discourse are highlighted by the comparison between the UFC’s treatment and the acceptance of boxing, despite the UFC’s claimed safety advantages?  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(generated_summaries['question'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Pipeline with Instruction-Driven Summaries\n",
    "\n",
    "Because a reasoning model can **brainstorm multiple viewpoints**, we’ll ask it to invent its own summarization *instructions* before it writes any summaries.\n",
    "For instance, given a document on EVs and their environmental impact, the model might propose:\n",
    "\n",
    "1. “Summarize as an academic essay.”\n",
    "2. “Highlight key environmental implications.”\n",
    "3. “Provide a comparative analysis.”\n",
    "4. “Rewrite as an investor pitch.”\n",
    "5. *…and so on.*\n",
    "\n",
    "We’ll implement this in two steps, each powered by its own prompt:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. `generate_summary_inst.yaml` – *Create 10 diverse instructions*\n",
    "\n",
    "```yaml\n",
    "system: You are an AI assistant that is expert at summarizing text.\n",
    "\n",
    "introduction: |\n",
    "  Given the document below, generate 10 short, distinct instructions for summarizing it.\n",
    "  Each instruction should vary in perspective, tone, or purpose.\n",
    "\n",
    "principles: \"\"\n",
    "\n",
    "examples: |\n",
    "  Example:\n",
    "  1. Summarize the article in simple terms for a 10-year-old.\n",
    "  2. Highlight the major arguments and counterarguments.\n",
    "  3. Provide a summary focusing on implications for future research.\n",
    "\n",
    "generation: |\n",
    "  Document:\n",
    "  {{document_outline}}\n",
    "\n",
    "  {{document}}\n",
    "\n",
    "  Now generate 10 diverse summarization instructions.\n",
    "\n",
    "start_tags: [\"\"]\n",
    "end_tags: [\"\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `generate_summary.yaml` – *Write a summary for each instruction*\n",
    "\n",
    "```yaml\n",
    "system: You are an AI assistant that is expert at summarizing text.\n",
    "\n",
    "introduction: |\n",
    "  Using the instruction below, summarize the document:\n",
    "  {{summary_instruction}}\n",
    "\n",
    "principles:\n",
    "  - Include as much of the document as possible to create a comprehensive summary.\n",
    "  - If tables are present, include all table data.\n",
    "\n",
    "examples: \"\"\n",
    "\n",
    "generation: |\n",
    "  Document:\n",
    "  {{document_outline}}\n",
    "  {{document}}\n",
    "\n",
    "start_tags: [\"\"]\n",
    "end_tags: [\"\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Flow Updates\n",
    "\n",
    "To support these prompts, we add two new processing stages and remove the old single-summary block:\n",
    "\n",
    "| # | Block                                     | Purpose                                                                                                                                  |\n",
    "| - | ----------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| 1 | **LLMBlock – `gen_summary_instructions`** | Generate the 10-item instruction list.                                                                                                   |\n",
    "| 2 | **PostProcessThinkingBlock**              | Capture the model’s internal “thinking” for transparency/debugging.                                                                      |\n",
    "| 3 | **RegexParserBlock**                      | Extract the numbered instructions into a clean `summary_instruction` column (`parser_cleanup_tags` handle stray tokens such as `[END]`). |\n",
    "| 4 | **LLMBlock – `gen_detailed_summary`**     | Produce one summary per instruction.                                                                                                     |\n",
    "| 5 | **PostProcessThinkingBlock**              | Record the reasoning steps used to craft each summary.                                                                                   |\n",
    "\n",
    "With these two additional blocks—and by removing the original one-size-fits-all summarization step—the pipeline now:\n",
    "\n",
    "1. **Brainstorms** multiple summarization angles.\n",
    "2. **Parses** and stores those angles cleanly.\n",
    "3. **Writes** a tailored summary for each angle, capturing the model’s reasoning throughout.\n",
    "\n",
    "This upgrade lets us generate richer, more varied training data that showcases the model’s reasoning abilities instead of a single static summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import yaml\n",
    "import copy\n",
    "\n",
    "# Helper function to create an LLM block configuration\n",
    "def create_llm_block(block_name, config_path, output_cols, model_id, gen_kwargs):\n",
    "    return {\n",
    "        \"block_type\": \"LLMBlock\",\n",
    "        \"block_config\": {\n",
    "            \"block_name\": block_name,\n",
    "            \"config_path\": config_path,\n",
    "            \"model_id\": model_id,\n",
    "            \"output_cols\": output_cols,\n",
    "            \"gen_kwargs\": gen_kwargs\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Create block for generating summary instructions\n",
    "summary_inst_block = create_llm_block(\n",
    "    block_name=\"gen_summary_instructions\",\n",
    "    config_path=\"prompts/generate_summary_inst.yaml\",\n",
    "    output_cols=[\"summary_instruction\"],\n",
    "    model_id=MODEL_ID,\n",
    "    gen_kwargs={\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.95,\n",
    "        \"n\": 2,\n",
    "        \"seed\": 43146\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create post-processing block for summary instructions\n",
    "thinking_block_inst = create_postprocess_block(\n",
    "    block_name=\"process_thinking\",\n",
    "    column_name=summary_inst_block[\"block_config\"][\"output_cols\"][0]\n",
    ")\n",
    "\n",
    "# Create regex parser block to extract numbered instructions\n",
    "regex_parser_block = create_regex_parser_block(\n",
    "    block_name=\"regex_parser\",\n",
    "    column_name=summary_inst_block[\"block_config\"][\"output_cols\"][0],\n",
    "    parsing_pattern=\"(?:^|\\\\n)\\\\s*\\\\d+[\\\\.\\\\)]\\\\s*([^\\\\n]+)\",\n",
    "    cleanup_tags=[\"[END]\"],\n",
    "    output_cols=summary_inst_block[\"block_config\"][\"output_cols\"]\n",
    ")\n",
    "\n",
    "# Create block for generating detailed summaries\n",
    "summary_block = create_llm_block(\n",
    "    block_name=\"gen_detailed_summary\",\n",
    "    config_path=\"prompts/generate_summary.yaml\",\n",
    "    output_cols=[\"summary\"],\n",
    "    model_id=MODEL_ID,\n",
    "    gen_kwargs={\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.95,\n",
    "        \"n\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create post-processing block for summaries\n",
    "thinking_block_summary = create_postprocess_block(\n",
    "    block_name=\"process_thinking\",\n",
    "    column_name=summary_block[\"block_config\"][\"output_cols\"][0]\n",
    ")\n",
    "\n",
    "# Define input and output YAML paths\n",
    "input_yaml_path = \"flows/synth_knowledge1.5_updated_with_post_process_thinking_and_regex_parser.yaml\"\n",
    "output_yaml_path = \"flows/synth_knowledge1.5_updated_with_post_process_thinking_and_regex_parser_with_summary_instruction.yaml\"\n",
    "\n",
    "# Load existing flow configuration\n",
    "with open(input_yaml_path, 'r') as f:\n",
    "    flow_config = yaml.safe_load(f)\n",
    "\n",
    "# Find the index of the flatten_summary_columns block\n",
    "for idx, block in enumerate(flow_config):\n",
    "    if 'block_config' in block and block['block_config'].get('block_name') == 'flatten_summary_columns':\n",
    "        break\n",
    "\n",
    "# Create list of new blocks to inject\n",
    "injected_blocks = [\n",
    "    summary_inst_block,\n",
    "    thinking_block_inst,\n",
    "    regex_parser_block,\n",
    "    summary_block,\n",
    "    thinking_block_summary\n",
    "]\n",
    "\n",
    "# Insert new blocks after the first block and before the flatten block\n",
    "flow_config = flow_config[:1] + injected_blocks + flow_config[idx+1:]\n",
    "\n",
    "# Save the modified flow configuration\n",
    "with open(output_yaml_path, 'w') as f:\n",
    "    yaml.dump(flow_config, f, default_style='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the flow\n",
    "knowledge_1_5_flow = Flow(client).get_flow_from_file(\"flows/synth_knowledge1.5_updated_with_post_process_thinking_and_regex_parser_with_summary_instruction.yaml\")\n",
    "knowledge_1_5_sdg = SDG(\n",
    "    flows=[knowledge_1_5_flow],\n",
    "    num_workers=1,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "# Generate summaries\n",
    "generated_summaries = knowledge_1_5_sdg.generate(quality_corpus.select([1]), checkpoint_dir=\"simple_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated Summary Instruction:\\n\", generated_summaries['summary_instruction'][0])\n",
    "print(\"Corresponding generated summary:\\n\", generated_summaries['document'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Generated Summary Instruction:\n",
    "**Summarize for a debate competition**: Focus on the ethical arguments comparing UFC’s safety record to boxing and counterarguments from critics like Sen. McCain.\n",
    "\n",
    "Corresponding generated summary:\n",
    "**Summarized Debate Summary: UFC vs. Boxing Safety & Ethics**  \n",
    "\n",
    "**Background**: The Ultimate Fighting Championship (UFC), founded in 1993, initially featured no-holds-barred cage fights, attracting millions with its brutality (\"the damage\"). However, beneath the \n",
    "...\n",
    "**Conclusion**: The UFC’s demise reflects not ethical failings but political and cultural bias. Its underground persistence raises questions about America’s tolerance for innovation in combat sports versus entrenched preferences.  \n",
    "\n",
    "*(No tables in the document; all key data and quotes incorporated.)*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question:\\n\", generated_summaries['question'][0])\n",
    "print(\"Answer:\\n\", generated_summaries['response'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Question:\n",
    "How do the ethical arguments for UFC’s safety (e.g., submission-focused endings, adaptive rules) contrast with critics’ moral objections, and what underlying values drive this divergence?  \n",
    "\n",
    "Answer:\n",
    "assistant<think>\n",
    "Okay, let's tackle this question. The user wants to know how the ethical arguments for UFC's safety contrast with critics' moral objections and what underlying values cause this divergence.\n",
    "...\n",
    "Check the examples provided to ensure the format and style match. The answer should directly address both parts of the question: contrast in arguments and underlying values.\n",
    "</think>\n",
    "\n",
    "The ethical arguments for UFC’s safety emphasize empirical safety records and adaptive governance, contrasting sharply with critics’ moral objections rooted in cultural perception and tradition. UFC advocates highlight **submission-focused endings** (reducing prolonged brain \n",
    "...\n",
    "whereas critics value **traditional moral norms** and societal comfort with established sports, even when safety records contradict these preferences.  \n",
    "[END]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Building Reasoning-Centric Synthetic Data with `sdg_hub`\n",
    "\n",
    "In this notebook, we extended the traditional knowledge tuning workflow to support **document-grounded reasoning tasks** using `sdg_hub`. By swapping in a reasoning-capable teacher model (Nemotron Super) and customizing its prompt template, we enabled richer, multi-step thinking in our synthetic data generation pipeline.\n",
    "\n",
    "Key contributions of this workflow include:\n",
    "\n",
    "* **Instruction-first summarization**: The model is prompted to brainstorm diverse ways to summarize a document, reflecting different tones, goals, and reasoning perspectives.\n",
    "* **Multi-block reasoning flow**: We composed the pipeline using `LLMBlock`, `PostProcessThinkingBlock`, and `RegexParserBlock` to simulate how a human might structure thoughts, write summaries, and clean up output.\n",
    "* **Faithful alignment between prompts and data**: Each block maintained strict variable alignment, enabling seamless chaining of tasks and clean YAML integration.\n",
    "\n",
    "This reasoning-aware seed dataset is now ready to be used for **fine-tuning student models**—training them not only to recall facts, but to reason through complex text in diverse, instruction-driven ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For reference: scripts for running SDG, starting student model and training\n",
    "\n",
    "### Start Teacher Model\n",
    "```shell\n",
    "export HUGGINGFACE_HUB_CACHE=\"/new_data/hf_cache\"   \n",
    "export HF_DATASETS_CACHE=\"/dev/shm/hf\"\n",
    "export HF_HOME=\"/new_data/hf_cache\"\n",
    "export HF_MODEL_CACHE=\"/new_data/hf_cache\"\n",
    "\n",
    "for i in $(seq 0 2 6); do\n",
    "    port=$((8000 + i/2))\n",
    "    CUDA_VISIBLE_DEVICES=$i,$((i+1)) python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model nvidia/Llama-3_3-Nemotron-Super-49B-v1 \\\n",
    "        --dtype float16 \\\n",
    "        --tensor-parallel-size 2 \\\n",
    "        --port $port \\\n",
    "        --trust-remote-code > log_$((i/2+1)).log 2>&1 &\n",
    "done\n",
    "```\n",
    "\n",
    "### Run SDG in parallel\n",
    "\n",
    "```shell\n",
    "# Get dataset size and save into variable\n",
    "dataset_size=$(wc -l seed_data.jsonl | awk '{print $1}')\n",
    "number_of_processes=4\n",
    "port=8000\n",
    "for i in {0..4}; do\n",
    "    # Continue until i=4\n",
    "    if [ $i -eq 4 ]; then\n",
    "        break\n",
    "    fi\n",
    "    dataset_start_index=$((i * dataset_size / number_of_processes))\n",
    "    dataset_end_index=$((dataset_start_index + dataset_size / number_of_processes))\n",
    "    python -m sdg_hub.flow_runner --ds_path  seed_data.jsonl \\\n",
    "        --bs 2 --num_workers 10 \\\n",
    "        --save_path data/knowledge/quality/synth_knowledge_reasoning/gen.jsonl \\\n",
    "        --flow flows/synth_knowledge1.5_updated_with_post_process_thinking_and_regex_parser_with_summary_instruction.yaml \\\n",
    "        --endpoint http://localhost:$port/v1 \\\n",
    "        --checkpoint_dir data/knowledge/quality/synth_knowledge_reasoning/data_checkpoints \\\n",
    "        --save_freq 1000 \\\n",
    "        --dataset_start_index $dataset_start_index \\\n",
    "        --dataset_end_index $dataset_end_index > run_sdg_$i.log 2>&1 &\n",
    "    echo \"Starting process $i with dataset from $dataset_start_index to $dataset_end_index on port $port\"\n",
    "    port=$((port + 1))\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_sdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
