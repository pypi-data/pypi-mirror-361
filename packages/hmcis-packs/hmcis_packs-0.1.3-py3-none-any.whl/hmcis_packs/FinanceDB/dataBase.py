# db_client.py

import logging
import os
import sys
import yaml

import pandas as pd
from pandas import PeriodDtype
import psycopg2
from psycopg2 import sql
from psycopg2.extras import execute_values

# ========== –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ==========
# –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤ ‚Äî —Å–æ–∑–¥–∞—ë–º, –µ—Å–ª–∏ –Ω–µ—Ç
LOG_DIR = os.path.join(os.path.dirname(__file__), 'logs')
os.makedirs(LOG_DIR, exist_ok=True)

LOG_FILE = os.path.join(LOG_DIR, 'database_client.log')

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π –ª–æ–≥–≥–µ—Ä –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥—É–ª—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s: %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # –≤ –∫–æ–Ω—Å–æ–ª—å (stdout)
        logging.FileHandler(LOG_FILE, encoding='utf-8')  # –≤ —Ñ–∞–π–ª
    ]
)

logger = logging.getLogger(__name__)


# ===============================================


class DatabaseClient:
    def __init__(self, config_path=None):
        if config_path is None:
            user_home = os.path.expanduser("~")
            config_path = os.path.join(user_home, "db_config.yaml")

        with open(config_path, 'r', encoding='utf-8') as f:
            self.db_config = yaml.safe_load(f)

    def _map_dtype(self, dtype):
        """
        –ú–∞–ø–ø–∏–Ω–≥ pandas dtype ‚Üí PostgreSQL —Ç–∏–ø.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç PeriodDtype, float, int, bool, datetime, object –∏ fallback BYTEA.
        """
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–∏–æ–¥–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å isinstance —Å PeriodDtype
        if isinstance(dtype, PeriodDtype):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–∏–æ–¥ –∫–∞–∫ –Ω–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ TIMESTAMP
            return 'TIMESTAMP'
        if pd.api.types.is_float_dtype(dtype):
            return 'NUMERIC'
        if pd.api.types.is_integer_dtype(dtype):
            return 'INTEGER'
        if pd.api.types.is_bool_dtype(dtype):
            return 'BOOLEAN'
        if pd.api.types.is_datetime64_any_dtype(dtype):
            return 'TIMESTAMP'
        if dtype == 'object':
            return 'VARCHAR'
        return 'BYTEA'

    def save_df_to_db(self,
                      df: pd.DataFrame,
                      table_name: str,
                      schema: str = 'IFRS Reports',
                      binary_columns: list[str] = None):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ PostgreSQL, —Å–æ–∑–¥–∞—ë—Ç —Å—Ö–µ–º—É/—Ç–∞–±–ª–∏—Ü—É –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏,
        –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –ø–∞–∫–µ—Ç–Ω—É—é –≤—Å—Ç–∞–≤–∫—É —á–µ—Ä–µ–∑ execute_values.

        :param df: DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
        :param table_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã (–±–µ–∑ —Å—Ö–µ–º—ã).
        :param schema: –ò–º—è —Å—Ö–µ–º—ã –≤ –ë–î.
        :param binary_columns: –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ BYTEA.
        """
        if binary_columns is None:
            binary_columns = []

        conn = psycopg2.connect(**self.db_config)
        cursor = conn.cursor()
        try:
            # 1) —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Å—Ö–µ–º–∞ –µ—Å—Ç—å; –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç, —Ç–æ —Å–æ–∑–¥–∞—ë–º —Å—Ö–µ–º—É
            cursor.execute(
                sql.SQL("CREATE SCHEMA IF NOT EXISTS {}")
                .format(sql.Identifier(schema))
            )

            # 2) DDL –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫
            cols_ddl = []
            for col, dtype in zip(df.columns, df.dtypes):
                if col in binary_columns:
                    pg_type = 'BYTEA'
                else:
                    pg_type = self._map_dtype(dtype)
                cols_ddl.append(f"{sql.Identifier(col).as_string(conn)} {pg_type}")

            create_sql = sql.SQL(
                "CREATE TABLE IF NOT EXISTS {schema}.{table} ({fields})"
            ).format(
                schema=sql.Identifier(schema),
                table=sql.Identifier(table_name),
                fields=sql.SQL(", ").join(sql.SQL(c) for c in cols_ddl),
            )
            cursor.execute(create_sql)
            # 3) –û—á–∏—â–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            truncate_sql = sql.SQL("TRUNCATE {schema}.{table}").format(
                schema=sql.Identifier(schema),
                table=sql.Identifier(table_name),
            )
            cursor.execute(truncate_sql)
            # 4) –≤—Å—Ç–∞–≤–∫–∞ –ø–∞–∫–µ—Ç–∞–º–∏
            insert_sql = sql.SQL(
                "INSERT INTO {schema}.{table} ({fields}) VALUES %s"
            ).format(
                schema=sql.Identifier(schema),
                table=sql.Identifier(table_name),
                fields=sql.SQL(', ').join(map(sql.Identifier, df.columns))
            )

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –≤—Å—Ç–∞–≤–∫–∏
            records = []
            for row in df.itertuples(index=False, name=None):
                rec = []
                for val, col in zip(row, df.columns):
                    if col in binary_columns:
                        rec.append(psycopg2.Binary(val))
                    else:
                        rec.append(val)
                records.append(tuple(rec))

            execute_values(cursor, insert_sql.as_string(conn), records, page_size=10000)
            conn.commit()

            msg = f"ü´°‚úÖüÜíüëå–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü—É '{schema}.{table_name}'"
            logger.info(msg)
            print(msg, flush=True)

        except Exception as e:
            conn.rollback()
            logger.error("üíÄü•∫–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –ë–î: %s", e)
            raise
        finally:
            cursor.close()
            conn.close()
