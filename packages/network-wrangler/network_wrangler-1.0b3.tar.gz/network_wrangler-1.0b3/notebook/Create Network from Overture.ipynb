{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Network from Overture\n",
    "\n",
    "Investigation into using Overture data to create a transportation network.\n",
    "\n",
    "## Download Data \n",
    "Uses [Overture Maps Python Command Line Tool](https://docs.overturemaps.org/getting-data/overturemaps-py/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install overturemaps\n",
    "! pip install libgdal-arrow-parquet\n",
    "! pip install ipyleaflet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Find bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, DrawControl, TileLayer\n",
    "m = Map(center=(37.7749, -122.4194), zoom=14, layers=[TileLayer(url='https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png')]) # Example: San Francisco\n",
    "draw_control = DrawControl()\n",
    "\n",
    "def handle_draw(self, action, geo_json):\n",
    "    if action == 'created':\n",
    "        # Get the coordinates of the bounding box\n",
    "        coordinates = geo_json['geometry']['coordinates'][0]\n",
    "        lons = [coord[0] for coord in coordinates]\n",
    "        lats = [coord[1] for coord in coordinates]\n",
    "        formatted_bbox = f\"{min(lons)},{min(lats)},{max(lons)},{max(lats)}\"\n",
    "        print(\"--bbox:\", formatted_bbox)\n",
    "\n",
    "draw_control.on_draw(handle_draw)\n",
    "\n",
    "m.add_control(draw_control)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data using the CLI and store on local file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "dl_path = Path.cwd()/'downloads'\n",
    "dl_path.mkdir(exist_ok=True,parents=True)\n",
    "bbox = \"-122.449322,37.762098,-122.434216,37.772682\"\n",
    "fmt: Literal[\"geojson\", \"geojsonseq\", \"geoparquet\"] = \"geoparquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_segment = f\"overturemaps download --bbox={bbox} -f {fmt} --type=segment -o {dl_path/'segments.parquet'}\"\n",
    "!{dl_segment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_connector = f\"overturemaps download --bbox={bbox} -f {fmt} --type=connector -o {dl_path/'connectors.parquet'}\"\n",
    "!{dl_connector}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Downloaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas import read_file\n",
    "segments_gpd = read_file(dl_path/'segments.parquet')\n",
    "nodes_gpd = read_file(dl_path/'connectors.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymap = segments_gpd.explore(\n",
    "    tiles='CartoDB dark_matter',\n",
    "    zoom_start=16,\n",
    "    column=\"class\",\n",
    "    location=segments_gpd.geometry.union_all().centroid.coords[0][::-1],\n",
    "    cmap='viridis')\n",
    "nodes_gpd.explore(m=mymap, color='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Fields\n",
    "\n",
    "Key missing field in all data we downloaded here was `lanes` which not only determines the number of lanes but also the link directionality.\n",
    "\n",
    "WARNING:  This data is **not suitable for use** yet because of this shortcoming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'subtype', 'names.primary', 'routes', 'class', 'geometry']\n",
    "segments_gpd.loc[segments_gpd[\"class\"]==\"primary\",cols ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Conversion\n",
    "\n",
    "Many of the fields are nested in stringified json (necessary in order to serialize as parquet).  This requires compute-intensive computation to grab values that you might need - although does provide a great deal of flexibility in how values are scoped.\n",
    "\n",
    "Scoped values include:\n",
    "- Speed limits: which seems to just be a lookup by facility class anyway (see plot below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# Convert stringified JSON to Python objects\n",
    "segments_gpd['speed_limits'] = segments_gpd['speed_limits'].apply(\n",
    "    lambda x: json.loads(x) if pd.notna(x) and isinstance(x, str) else x\n",
    ")\n",
    "# speed limits is an array, but I'm just taking the first value\n",
    "segments_gpd['max_speed'] = segments_gpd['speed_limits'].apply(\n",
    "    lambda x: x[0]['max_speed']['value'] if isinstance(x, list) and isinstance(x[0],dict) and x[0]['max_speed'] else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_gpd.dropna(subset=['max_speed', 'class']).plot(kind='scatter', x='max_speed', y='class')\n",
    "#not a lot of variety - looks like they were set by algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Wrangler Format\n",
    "\n",
    "Despite its shortcomings, we attempt to do what we can to re-format this data into network-wrangler's network format including:\n",
    "1. renaming columns that are directly comparible\n",
    "2. parsing columns into wrangler values (i.e. inferring \"drive_access\" from roadway class)\n",
    "3. reviewing topological inconsistencies (e.g. duplicate AB values)\n",
    "4. creating a duplicatable method to translate GERS_IDS (hex) --> wrangler ids (integers).  \n",
    "\n",
    "NOTE: This process is left incomplete, as the inability to determine which roadways should be two-way vs one-way is impossible without the `lanes` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_cols = {\n",
    "    \"id\":\"gers_id\",\n",
    "    \"names.primary\":\"name\",\n",
    "    \"class\": \"roadway\",\n",
    "    \"max_speed\": \"speed_limit\",\n",
    "    \"geometry\": \"geometry\"\n",
    "}\n",
    "links_df = segments_gpd.loc[:, list(rename_cols.keys())].rename(columns=rename_cols)\n",
    "links_df['waypoints'] = segments_gpd['connectors'].apply(\n",
    "    lambda x: json.loads(x) if pd.notna(x) and isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_gpd[['subtype','class']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df[\"rail_only\"] = segments_gpd.subtype.str.contains(\"rail\")\n",
    "links_df[\"walk_access\"] = segments_gpd['class'].isin(['footway','path','steps'])\n",
    "links_df[\"bike_access\"] = segments_gpd['class'].isin(['path','cycleway','service','tertiary','residential'])\n",
    "links_df[\"drive_access\"] = ~segments_gpd['class'].isin(['footway','path','steps','cycleway',None])\n",
    "links_df[\"truck_access\"] = links_df[\"drive_access\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IDS\n",
    "\n",
    "Process to convert IDS from hex to ints as required by wrangler.\n",
    "\n",
    "Note that we are forced to limit the number of hex digits converted to integers in order for the resulting integer to be able to be serialized in parquet which has an upper-bounds int limit based on c-int-large - but we do check that the values (at least within this dataset) are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDS from hex to ints as required by wrangler\n",
    "id_digits = 6 # a couple digits ...can't go BACK to gers id but should be relatively unique\n",
    "hex_to_int = lambda x: int(x[-id_digits:], 16)\n",
    "links_df[['A_h','B_h']] = links_df['waypoints'].apply(lambda x: pd.Series([x[0]['connector_id'], x[-1]['connector_id']]))\n",
    "links_df['A'] = links_df['A_h'].apply(hex_to_int)\n",
    "links_df['B'] = links_df['B_h'].apply(hex_to_int)\n",
    "links_df['model_link_id'] = links_df['gers_id'].apply(hex_to_int)\n",
    "assert links_df.model_link_id.nunique() == len(links_df)\n",
    "links_df[['model_link_id','A','B']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_cols = {\n",
    "    \"id\":\"gers_id\",\n",
    "    \"geometry\": \"geometry\"\n",
    "}\n",
    "nodes_df = nodes_gpd.loc[:, list(rename_cols.keys())].rename(columns=rename_cols)\n",
    "nodes_df['model_node_id'] = nodes_df['gers_id'].apply(hex_to_int)\n",
    "# find duplicated model_node_ids\n",
    "# nodes_df.loc[nodes_df.model_node_id.duplicated(keep=False), ['gers_id','model_node_id']]\n",
    "assert nodes_df.model_node_id.nunique() == len(nodes_df)\n",
    "nodes_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check uniqueness\n",
    "\n",
    "Find records with duplicated A-B, note that they are all footpaths and then keep the shortest of them since that is the path that would be taken anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find links that have duplicated A and B values\n",
    "links_df.loc[links_df.duplicated(subset=['A','B'], keep=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all sidewalks/stairs, so let's just take the one with the shortest length\n",
    "links_df = links_df.sort_values('geometry').drop_duplicates(subset=['A','B'], keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check topology\n",
    "\n",
    "- find links where AB doesn't have a corresponding BA\n",
    "\n",
    "Turns out that is basically all of them, thus we really do need \"lanes\" in order to know which of these to turn into two-way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\n",
    "ba_df = links_df.rename(columns={'A': 'B', 'B': 'A'})\n",
    "merged_df = pd.merge(links_df, ba_df, on=['A', 'B'], how='left', indicator=True)\n",
    "ab_without_ba = merged_df[merged_df['_merge'] == 'left_only']\n",
    "f\"Links without corresponding link the other direction: {len(ab_without_ba)}/{len(links_df)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_with_ba = merged_df[merged_df['_merge'] == 'both']\n",
    "ab_with_ba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load into Wrangler\n",
    "\n",
    "Note that this step will not work yet as it\n",
    "- fails validation for requiring lanes\n",
    "- doesn't have correct reversed links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df.to_parquet(dl_path/'nodes.parquet')\n",
    "links_df.to_parquet(dl_path/'links.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_wrangler import load_roadway_from_dir\n",
    "#if we try and load right away, there are clearly some issues. So we need to clean up the data more.\n",
    "road_net = load_roadway_from_dir(dl_path,file_format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrangler-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
