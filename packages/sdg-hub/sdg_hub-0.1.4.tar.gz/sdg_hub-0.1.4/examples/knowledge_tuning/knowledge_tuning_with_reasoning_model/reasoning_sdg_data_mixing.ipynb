{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Summary\n",
    "\n",
    "This notebook demonstrates how to the training dataset to train a student model. This notebook continues where left before. Recall in the reasoning notebook we saw how to modify current knowledge flow, swap a teacher model with reasoning model, and generate reasoning synthetic data. This notebook will focus on **data mixing and replay buffer strategies** to enhance the diversity and quality of training data. The notebook shows how to:\n",
    "\n",
    "* Show you how to create training mix using generated data and existing released instruct data\n",
    "* How instructlab data mixing for knowledge works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Nemotron Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "nemotron_ds = load_dataset(\"nvidia/Llama-Nemotron-Post-Training-Dataset-v1\", \"SFT\")\n",
    "nemotron_ds = nemotron_ds.filter(lambda x: x['used_in_training'] == 'yes')\n",
    "nemotron_ds = nemotron_ds.map(lambda x: {'question': x['input'][x['input'].find(\"user<|end_header_id|>\") + len(\"user<|end_header_id|>\") : x['input'].find(\"<|eot_id|><|start_header_id|>assistant\")].strip()})\n",
    "nemotron_ds = concatenate_datasets(nemotron_ds.values())\n",
    "nemotron_ds = nemotron_ds.shuffle(seed=894375).select(range(200000))\n",
    "nemotron_ds = nemotron_ds.add_column('unmask', [False]*nemotron_ds.num_rows)\n",
    "nemotron_ds = nemotron_ds.map(lambda x: {'messages': [{'role': 'system', 'content': 'detailed thinking on'}, {'role': 'user', 'content': x['question']}, {'role': 'assistant', 'content': x['output']}]})\n",
    "nemotron_ds = nemotron_ds.remove_columns(['input', 'output', 'category', 'license', 'reasoning', 'generator', 'used_in_training', 'question'])\n",
    "nemotron_ds.to_json(\"nemotron_replay_buffer_data.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for data mixng with conversation templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses utility functions for creating training data that combines generated reasoning examples.\n",
    "The functions handle tasks like:\n",
    "- Converting documents and Q&A pairs into chat format\n",
    "- Adding system prompts for controlling model behavior\n",
    "- Mixing in auxiliary data like summaries\n",
    "\n",
    "These functions are similar to instructlab.sdg's data mixing functions\n",
    "\n",
    "The core functions are defined in:\n",
    "- `sdg_hub/examples/reasoning_knowledge_generation/utils.py`\n",
    "- `sdg_hub/examples/knowledge_tuning/knowledge_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import sdg_hub\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "def generate_knowledge_qa_dataset(\n",
    "    generated_dataset: Dataset, keep_context_separate=False, keep_document_outline=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a knowledge QA dataset from the input dataset by transforming document/question/response pairs into a chat format.\n",
    "    \n",
    "    Args:\n",
    "        generated_dataset (Dataset): Input dataset containing documents, questions and responses\n",
    "        keep_context_separate (bool): If True, keeps context separate from the messages. If False, includes context in user message\n",
    "        keep_document_outline (bool): If True, includes document outline in user message when context is not separate\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: Transformed dataset with chat messages format\n",
    "    \"\"\"\n",
    "    def __create_qa_row(rec):\n",
    "        context = rec[\"document\"]\n",
    "        instruction = rec[\"question\"]\n",
    "        response = rec[\"response\"]\n",
    "        metadata = {\n",
    "            \"sdg_document\": rec[\"document\"],\n",
    "            \"domain\": rec[\"domain\"],\n",
    "            \"dataset\": \"document_knowledge_qa\",\n",
    "        }\n",
    "        if \"raw_document\" in rec and \"dataset_type\" in rec:\n",
    "            metadata.update(\n",
    "                {\n",
    "                    \"raw_document\": rec[\"raw_document\"],\n",
    "                    \"dataset_type\": rec[\"dataset_type\"],\n",
    "                }\n",
    "            )\n",
    "        metadata = json.dumps(metadata)\n",
    "        if keep_context_separate:\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"{instruction}\"},\n",
    "                {\"role\": \"assistant\", \"content\": response},\n",
    "            ]\n",
    "            return {\n",
    "                \"messages\": messages,\n",
    "                \"metadata\": metadata,\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"context\": context,\n",
    "            }\n",
    "        else:\n",
    "            if keep_document_outline:\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"{rec['document_outline']}\\n{context}\\n\\n{instruction}\",\n",
    "                    },\n",
    "                    {\"role\": \"assistant\", \"content\": response},\n",
    "                ]\n",
    "            else:\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": f\"{context}\\n\\n{instruction}\"},\n",
    "                    {\"role\": \"assistant\", \"content\": response},\n",
    "                ]\n",
    "            return {\"messages\": messages, \"metadata\": metadata, \"id\": str(uuid.uuid4())}\n",
    "\n",
    "    knowledge_ds = generated_dataset.map(\n",
    "        __create_qa_row, remove_columns=generated_dataset.column_names\n",
    "    )\n",
    "    return knowledge_ds\n",
    "\n",
    "\n",
    "def _conv_pretrain(rec, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert messages to pretraining format or set unmask flag based on tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        rec (dict): Record containing messages\n",
    "        tokenizer: Tokenizer object, if None uses pretraining format\n",
    "        \n",
    "    Returns:\n",
    "        dict: Modified record\n",
    "    \"\"\"\n",
    "    if tokenizer is not None:\n",
    "        rec['unmask'] = True\n",
    "        return rec\n",
    "    rec[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"pretraining\",\n",
    "            \"content\": f\"<|user|>\\n{rec['messages'][0]['content']}\\n<|assistant|>\\n{rec['messages'][1]['content']}\",\n",
    "        }\n",
    "    ]\n",
    "    return rec\n",
    "\n",
    "def create_auxiliary_dataset(generated_dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Create auxiliary dataset from non-base documents using predefined instructions.\n",
    "    \n",
    "    Args:\n",
    "        generated_dataset (Dataset): Input dataset containing documents and metadata\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: Auxiliary dataset with chat messages, or None if requirements not met\n",
    "    \"\"\"\n",
    "    if \"dataset_type\" not in generated_dataset.column_names:\n",
    "        return None\n",
    "\n",
    "    aux_inst_path = os.path.join(\n",
    "        os.path.dirname(sdg_hub.__file__),\n",
    "        \"configs/knowledge/auxilary_instructions.yaml\",\n",
    "    )\n",
    "    print(aux_inst_path)\n",
    "\n",
    "    if os.path.isfile(aux_inst_path):\n",
    "        with open(aux_inst_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "            auxiliary_inst = yaml.safe_load(fp)\n",
    "    else:\n",
    "        print(f\"auxiliary instructions file not found at {aux_inst_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Filter and process auxiliary documents\n",
    "    auxiliary_ds = generated_dataset.filter(\n",
    "        lambda x: x[\"dataset_type\"] != \"base_document\"\n",
    "    )\n",
    "    unique_document_auxiliary = auxiliary_ds.to_pandas().drop_duplicates(\n",
    "        subset=[\"document\"]\n",
    "    )\n",
    "    unique_document_auxiliary = Dataset.from_pandas(unique_document_auxiliary)\n",
    "    unique_document_auxiliary = unique_document_auxiliary.remove_columns(\n",
    "        [\n",
    "            col\n",
    "            for col in unique_document_auxiliary.column_names\n",
    "            if col\n",
    "            not in [\n",
    "                \"raw_document\",\n",
    "                \"document_outline\",\n",
    "                \"domain\",\n",
    "                \"dataset_type\",\n",
    "                \"document\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "    unique_document_auxiliary = unique_document_auxiliary.rename_columns(\n",
    "        {\"raw_document\": \"context\", \"document\": \"response\"}\n",
    "    )\n",
    "\n",
    "    def __create_auxiliary_ds(rec):\n",
    "        instruction = random.choice(auxiliary_inst[rec[\"dataset_type\"]])\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"{rec['context']}\\n\\n{instruction}\"},\n",
    "            {\"role\": \"assistant\", \"content\": rec[\"response\"]},\n",
    "        ]\n",
    "        metadata = json.dumps(\n",
    "            {\n",
    "                \"dataset_type\": rec[\"dataset_type\"],\n",
    "                \"raw_document\": rec[\"context\"],\n",
    "                \"dataset\": f\"document_{rec['dataset_type']}\",\n",
    "                \"domain\": rec[\"domain\"],\n",
    "            }\n",
    "        )\n",
    "        return {\"messages\": messages, \"metadata\": metadata, \"id\": str(uuid.uuid4())}\n",
    "\n",
    "    unique_document_auxiliary = unique_document_auxiliary.map(\n",
    "        __create_auxiliary_ds, remove_columns=unique_document_auxiliary.column_names\n",
    "    )\n",
    "    return unique_document_auxiliary\n",
    "\n",
    "    \n",
    "def create_training_mix(ds, tokenizer, thinking=\"on\", create_summary=True, nemotron_format=True, keep_context_separate=False, no_pretrain=False, keep_document_outline=False):\n",
    "    \"\"\"\n",
    "    Create a mixed training dataset combining knowledge QA and optional summary data.\n",
    "    \n",
    "    Args:\n",
    "        ds (Dataset): Input dataset\n",
    "        tokenizer: Tokenizer for pretraining format\n",
    "        thinking (str): Thinking mode for system message (\"on\"/\"off\")\n",
    "        create_summary (bool): Whether to include summary dataset\n",
    "        nemotron_format (bool): Whether to add system messages in nemotron format\n",
    "        keep_context_separate (bool): Whether to keep context separate in knowledge QA\n",
    "        no_pretrain (bool): Skip pretraining format conversion if True\n",
    "        keep_document_outline (bool): Include document outline in messages\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: Combined training dataset\n",
    "    \"\"\"\n",
    "    # Generate knowledge QA dataset\n",
    "    knowl_train = generate_knowledge_qa_dataset(ds, keep_context_separate=keep_context_separate, keep_document_outline=keep_document_outline)\n",
    "    \n",
    "    # Apply pretraining format if needed\n",
    "    if no_pretrain:\n",
    "        knowl_train_pretrain = knowl_train\n",
    "    else:\n",
    "        knowl_train_pretrain = knowl_train.map(_conv_pretrain, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=10)\n",
    "    \n",
    "    # Add system messages for nemotron format\n",
    "    if nemotron_format:\n",
    "        knowl_train_pretrain = knowl_train_pretrain.map(lambda x: {'messages': [{'content': f'detailed thinking {thinking}', 'role': 'system'}] + x['messages']})\n",
    "    \n",
    "    # Add summary dataset if requested\n",
    "    if create_summary:\n",
    "        summary_ds = create_auxiliary_dataset(ds)\n",
    "        if no_pretrain and summary_ds:\n",
    "            summary_ds_pretrain = summary_ds\n",
    "        else:\n",
    "            summary_ds_pretrain = summary_ds.map(_conv_pretrain, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=10)\n",
    "        if nemotron_format:\n",
    "            summary_ds_pretrain = summary_ds_pretrain.map(lambda x: {'messages': [{'content': 'detailed thinking off', 'role': 'system'}] + x['messages']})\n",
    "        return concatenate_datasets([knowl_train_pretrain, summary_ds_pretrain])\n",
    "    else:\n",
    "        return knowl_train_pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create quality training mix of: reasoning dataset, with non-reasoning dataset, nemotron replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For this tutorial, we will use the following document uids from the quality dataset:\n",
    "DOC_UIDS = [\n",
    "    ' Defining Decay Down by David Plotz',\n",
    "    ' Fight Clubbed by David Plotz',\n",
    "    ' I, Antichrist? by Jeffrey Goldberg',\n",
    "    \" It's Time To Keelhaul U-Haul! by Jeffrey Goldberg\",\n",
    "    \" My Father's Estate by Ben Stein\",\n",
    "    '\"Phone Me in Central Park\" by McConnell, James V.',\n",
    "    '...After a Few Words... by Garrett, Randall', \n",
    "    '...And It Comes Out Here by Del Rey, Lester',\n",
    "    'A Coffin for Jacob by Ludwig, Edward W.',\n",
    "    'A Fall of Glass by Lee, Stanley R.',\n",
    "    'A Filbert Is a Nut by Raphael, Rick',\n",
    "    'A Gift from Earth by Banister, Manly',\n",
    "    'A Gleeb for Earth by Schafhauser, Charles',\n",
    "    'A Good Year for the Roses? by David Edelstein',\n",
    "    'A Pail of Air by Leiber, Fritz',\n",
    "    'A Planet Named Joe by Hunter, Evan',\n",
    "    \"AI: what's the worst that could happen? by Harry Armstrong\",\n",
    "    'Accidental Death by Baily, Peter',\n",
    "    'All Day September by Kuykendall, Roger',\n",
    "    'Ambition by Bade, William L.',\n",
    "    'And Then the Town Took Off by Wilson, Richard',\n",
    "    'Atom Mystery [Young Atom Detective] by Coombs, Charles Ira',\n",
    "    'Beach Scene by King, Marshall',\n",
    "    'Big Ancestor by Wallace, F. L. (Floyd L.)',\n",
    "    'Birds of a Feather by Silverberg, Robert',\n",
    "    'Bodyguard by Gold, H. L. (Horace Leonard)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    " \n",
    "# Load tokenizer for pre-training formatting\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\")\n",
    "\n",
    "# Load non-reasoning dataset from nemotron super 49b\n",
    "nemotron_non_reasoning_ds = load_dataset(\"json\", data_dir=\"data/knowledge/quality/knowledge_nemotron/\", split=\"train\")\n",
    "nemotron_non_reasoning_ds = nemotron_non_reasoning_ds.filter(lambda x: x['score'] == '2' and x['judgment'] == 'YES')\n",
    "nemotron_non_reasoning_ds = nemotron_non_reasoning_ds.filter(lambda x: x['document_outline'] in DOC_UIDS)\n",
    "print(nemotron_non_reasoning_ds)\n",
    "\n",
    "# Load reasoning dataset from nemotron super 49b\n",
    "nemotron_reasoning_ds = load_dataset(\"json\", data_dir=\"data/knowledge/quality/synth_knowledge_reasoning/\", split=\"train\")\n",
    "nemotron_reasoning_ds = nemotron_reasoning_ds.filter(lambda x: x['score'] == '2' and x['judgment'] == 'YES')\n",
    "nemotron_reasoning_ds = nemotron_reasoning_ds.filter(lambda x: x['document_outline'] in DOC_UIDS)\n",
    "print(nemotron_reasoning_ds)\n",
    "\n",
    "# Load nemotron replay buffer. \n",
    "# Note: This is a replay buffer we created by sub-sampling nvidia/Llama-Nemotron-Post-Training-Dataset from huggingface.\n",
    "nemotron_ds_replay_buffer = load_dataset(\"json\", data_files=\"data/knowledge/quality/training_mix/replay_buffer.jsonl\", split=\"train\")\n",
    "\n",
    "\n",
    "# Create non-reasoning training mix\n",
    "nemotron_ds_training_mix = create_training_mix(nemotron_non_reasoning_ds, tokenizer, 'off').shuffle(seed=894375)\n",
    "\n",
    "# Create reasoning training mix\n",
    "nemotron_reasoning_ds = create_training_mix(nemotron_reasoning_ds, tokenizer, 'on')\n",
    "\n",
    "# Concatenate reasoning and non-reasoning training mixes\n",
    "quality_reasoning_ds = concatenate_datasets([nemotron_reasoning_ds, nemotron_ds_training_mix]).remove_columns(['metadata', 'id']) # .select(range(40000))\n",
    "print(quality_reasoning_ds)\n",
    "\n",
    "# Concatenate training mix with replay buffer\n",
    "training_mix = concatenate_datasets([quality_reasoning_ds, nemotron_ds_replay_buffer.shuffle(seed=894375).select(range(len(quality_reasoning_ds)))])\n",
    "\n",
    "print(training_mix)\n",
    "training_mix.to_json(\"data/knowledge/quality/training_mix/quality_knowledge_mix.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train student model\n",
    "- Setup the training by cloning `https://github.com/instructlab/training` and following the instructions in the README\n",
    "- The create `train.py` using below code\n",
    "    ```python\n",
    "    import argparse\n",
    "    from instructlab.training.config import TorchrunArgs,TrainingArgs,DistributedBackend,FSDPOptions\n",
    "    from instructlab.training.main_ds import run_training\n",
    "    import os\n",
    "    def parse_args():\n",
    "        parser = argparse.ArgumentParser(description='Training script with configurable paths')\n",
    "        parser.add_argument('--data_path', type=str, required=True,\n",
    "                        help='Path to the training data file')\n",
    "        parser.add_argument('--model_path', type=str, required=True,\n",
    "                        help='Path to the model or model identifier')\n",
    "        parser.add_argument('--chat_tmpl_path', type=str, required=True,\n",
    "                        help='Path to the chat template file')\n",
    "        parser.add_argument('--exp_dir', type=str, required=True,\n",
    "                        help='Path to the experiment directory')\n",
    "        parser.add_argument('--parent_exp_dir', type=str, required=True,\n",
    "                        help='Path to the parent experiment directory')\n",
    "        return parser.parse_args()\n",
    "\n",
    "    def main():\n",
    "        args = parse_args()\n",
    "        \n",
    "        torch_args = TorchrunArgs(\n",
    "            nproc_per_node=8,\n",
    "            nnodes=1,\n",
    "            node_rank=0,\n",
    "            rdzv_id=123,\n",
    "            rdzv_endpoint=\"0.0.0.0:8888\",\n",
    "        )\n",
    "        output_dir = os.path.join(args.parent_exp_dir, args.exp_dir)\n",
    "        train_args = TrainingArgs(\n",
    "            model_path=args.model_path,\n",
    "            data_path=args.data_path,\n",
    "            ckpt_output_dir=output_dir,\n",
    "            data_output_dir=\"data/processed-data\",\n",
    "            max_seq_len=20000,\n",
    "            max_batch_len=25000,\n",
    "            num_epochs=5,\n",
    "            effective_batch_size=256,\n",
    "            learning_rate=5e-6,\n",
    "            warmup_steps=25,\n",
    "            save_samples=0,\n",
    "            use_dolomite=False,\n",
    "            checkpoint_at_epoch = True,\n",
    "            accelerate_full_state_at_epoch = False,\n",
    "            process_data=True,\n",
    "            chat_tmpl_path=args.chat_tmpl_path,\n",
    "            distributed_backend=DistributedBackend.FSDP,\n",
    "            fsdp_options=FSDPOptions(cpu_offload_params=False),\n",
    "        )\n",
    "\n",
    "        run_training(torch_args=torch_args,train_args=train_args)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "    ```\n",
    "\n",
    "- Now create bash script with run command\n",
    "\n",
    "    ```shell\n",
    "    python train.py \\\n",
    "    --data_path \"quality_knowledge_1.25_nemotron_49b_first_24.jsonl\" \\\n",
    "    --model_path \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\" \\\n",
    "    --chat_tmpl_path \"<chat_template_path>\" \\\n",
    "    --exp_dir \"nano_customized_thinking_quality_model\" \\\n",
    "    --parent_exp_dir \"<parent_exp_dir>\"\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_sdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
