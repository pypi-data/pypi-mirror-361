# Ollama Flow

ä¸€å€‹å¼·å¤§ä¸”æ˜“ç”¨çš„ Python å‡½å¼åº«ï¼Œç”¨æ–¼èˆ‡ Ollama API äº’å‹•ã€‚

## åŠŸèƒ½ç‰¹è‰²

- ğŸš€ **ç°¡æ½”æ˜“ç”¨çš„ API** - æä¾›ç›´è§€çš„ Python ä»‹é¢
- ğŸ¯ **çµæ§‹åŒ–è¼¸å‡º** - æ”¯æ´ JSON Schema å’Œ Pydantic æ¨¡å‹
- ğŸŒŠ **ä¸²æµæ¨¡å¼** - å³æ™‚ç²å–ç”Ÿæˆå…§å®¹
- ğŸ’¬ **å®Œæ•´èŠå¤©æ”¯æ´** - æ”¯æ´å¤šè¼ªå°è©±å’Œå·¥å…·èª¿ç”¨
- ğŸ”¤ **åµŒå…¥å‘é‡** - ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
- âœ… **æ™ºèƒ½æ¨¡å‹é©—è­‰** - è‡ªå‹•æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œæä¾›å‹å¥½çš„éŒ¯èª¤æç¤º
- ğŸ’¾ **ç·©å­˜æ©Ÿåˆ¶** - æ™ºèƒ½ç·©å­˜æ¨¡å‹åˆ—è¡¨ï¼Œæå‡æ€§èƒ½
- ğŸ›¡ï¸ **é¡å‹å®‰å…¨** - å®Œæ•´çš„é¡å‹æç¤ºæ”¯æ´
- ğŸ“¦ **é›¶ä¾è³´** - åƒ…ä¾è³´ `requests` å’Œ `pydantic`

## æ”¯æ´çš„ API ç«¯é»

- `/api/generate` - ç”Ÿæˆå®Œæˆ
- `/api/chat` - èŠå¤©å®Œæˆ
- `/api/embed` - ç”ŸæˆåµŒå…¥å‘é‡

## å®‰è£

```bash
pip install ollama-flow
```

æˆ–è€…å¾æºç¢¼å®‰è£ï¼š

```bash
git clone https://github.com/your-username/ollama-flow.git
cd ollama-flow
pip install -e .
```

## å¿«é€Ÿé–‹å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from ollama_flow import OllamaClient

# å»ºç«‹å®¢æˆ¶ç«¯
client = OllamaClient(base_url="http://localhost:11434")

# ç”Ÿæˆæ–‡æœ¬
response = client.generate(
    model="llama3.2",
    prompt="è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
    stream=False
)
print(response.response)
```

### èŠå¤©å°è©±

```python
from ollama_flow import OllamaClient, ChatMessage

client = OllamaClient()

messages = [
    ChatMessage(role="system", content="ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹ã€‚"),
    ChatMessage(role="user", content="ä½ å¥½ï¼")
]

response = client.chat(
    model="llama3.2",
    messages=messages,
    stream=False
)
print(response.message.content)
```

### çµæ§‹åŒ–è¼¸å‡º

```python
from ollama_flow import OllamaClient
from pydantic import BaseModel, Field
from typing import List

class Product(BaseModel):
    name: str = Field(..., description="ç”¢å“åç¨±")
    price: float = Field(..., description="åƒ¹æ ¼")
    features: List[str] = Field(..., description="åŠŸèƒ½ç‰¹é»")

client = OllamaClient()

# ä½¿ç”¨ Pydantic æ¨¡å‹
response = client.generate_structured(
    model="llama3.2",
    prompt="å‰µå»ºä¸€å€‹æ™ºæ…§å‹æ‰‹æ©Ÿç”¢å“è³‡è¨Šï¼Œè«‹ç”¨ JSON æ ¼å¼å›æ‡‰ã€‚",
    schema=Product,
    stream=False
)

# è§£æçµæ§‹åŒ–å›æ‡‰
product = client.parse_structured_response(response.response, Product)
print(f"ç”¢å“åç¨±ï¼š{product.name}")
print(f"åƒ¹æ ¼ï¼š${product.price}")
print(f"åŠŸèƒ½ï¼š{', '.join(product.features)}")
```

### ä¸²æµæ¨¡å¼

```python
from ollama_flow import OllamaClient

client = OllamaClient()

response_stream = client.generate(
    model="llama3.2",
    prompt="å¯«ä¸€ç¯‡é—œæ–¼äººå·¥æ™ºæ…§çš„æ–‡ç« ã€‚",
    stream=True
)

print("ç”Ÿæˆä¸­ï¼š", end="", flush=True)
for chunk in response_stream:
    if chunk.get("done", False):
        print("\nç”Ÿæˆå®Œæˆï¼")
        break
    else:
        print(chunk.get("response", ""), end="", flush=True)
```

### ç”ŸæˆåµŒå…¥å‘é‡

```python
from ollama_flow import OllamaClient

client = OllamaClient()

response = client.embed(
    model="all-minilm",
    input="é€™æ˜¯è¦è½‰æ›ç‚ºåµŒå…¥å‘é‡çš„æ–‡æœ¬ã€‚"
)

print(f"åµŒå…¥ç¶­åº¦ï¼š{len(response.embeddings[0])}")
print(f"åµŒå…¥å‘é‡ï¼š{response.embeddings[0][:5]}...")  # é¡¯ç¤ºå‰5å€‹å€¼
```

## é€²éšåŠŸèƒ½

### æ¨¡å‹é©—è­‰

```python
# é è¨­é–‹å•Ÿæ¨¡å‹é©—è­‰
client = OllamaClient(check_models=True)

# ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
models = client.list_models()
print(f"å¯ç”¨æ¨¡å‹ï¼š{models}")

# åˆ·æ–°æ¨¡å‹ç·©å­˜
models = client.refresh_models_cache()

# é—œé–‰æ¨¡å‹é©—è­‰ï¼ˆä¸æ¨è–¦ï¼‰
client_no_check = OllamaClient(check_models=False)

# ç•¶ä½¿ç”¨ä¸å­˜åœ¨çš„æ¨¡å‹æ™‚ï¼Œæœƒæ‹‹å‡º ValueError
try:
    client.generate(model="nonexistent-model", prompt="Hello")
except ValueError as e:
    print(f"æ¨¡å‹é©—è­‰éŒ¯èª¤ï¼š{e}")
```

### JSON æ¨¡å¼

```python
# ä½¿ç”¨ JSON æ¨¡å¼
response = client.generate_json(
    model="llama3.2",
    prompt="åˆ—å‡ºä¸‰å€‹ç¨‹å¼è¨­è¨ˆèªè¨€åŠå…¶ç‰¹é»ã€‚è«‹ç”¨ JSON æ ¼å¼å›æ‡‰ã€‚",
    stream=False
)

import json
data = json.loads(response.response)
print(json.dumps(data, indent=2, ensure_ascii=False))
```

### è‡ªå®šç¾© JSON Schema

```python
# ä½¿ç”¨è‡ªå®šç¾© JSON Schema
person_schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "age": {"type": "integer"},
        "hobbies": {"type": "array", "items": {"type": "string"}}
    },
    "required": ["name", "age"]
}

response = client.generate_structured(
    model="llama3.2",
    prompt="å‰µå»ºä¸€å€‹è™›æ§‹è§’è‰²çš„è³‡è¨Šã€‚",
    schema=person_schema,
    stream=False
)
```

### ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªå‹•æ¸…ç†é€£ç·š
with OllamaClient() as client:
    response = client.generate(
        model="llama3.2",
        prompt="ä½ å¥½ï¼",
        stream=False
    )
    print(response.response)
```

## API åƒè€ƒ

### OllamaClient

ä¸»è¦çš„å®¢æˆ¶ç«¯é¡åˆ¥ï¼Œæä¾›èˆ‡ Ollama API çš„ä»‹é¢ã€‚

#### åˆå§‹åŒ–

```python
client = OllamaClient(
    base_url="http://localhost:11434",  # Ollama æœå‹™å™¨ URL
    timeout=30,  # è«‹æ±‚è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    check_models=True  # æ˜¯å¦åœ¨èª¿ç”¨å‰æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
)
```

#### æ–¹æ³•

- `generate()` - ç”Ÿæˆæ–‡æœ¬å®Œæˆ
- `chat()` - èŠå¤©å®Œæˆ
- `embed()` - ç”ŸæˆåµŒå…¥å‘é‡
- `generate_json()` - ç”Ÿæˆ JSON æ ¼å¼å›æ‡‰
- `generate_structured()` - ç”Ÿæˆçµæ§‹åŒ–å›æ‡‰
- `chat_json()` - èŠå¤©ç”Ÿæˆ JSON æ ¼å¼å›æ‡‰
- `chat_structured()` - èŠå¤©ç”Ÿæˆçµæ§‹åŒ–å›æ‡‰
- `parse_structured_response()` - è§£æçµæ§‹åŒ–å›æ‡‰
- `list_models()` - ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
- `refresh_models_cache()` - åˆ·æ–°æ¨¡å‹ç·©å­˜

### è³‡æ–™æ¨¡å‹

#### GenerateRequest
- `model`: æ¨¡å‹åç¨±
- `prompt`: æç¤ºæ–‡æœ¬
- `format`: å›æ‡‰æ ¼å¼ï¼ˆå¯é¸ï¼‰
- `stream`: æ˜¯å¦ä½¿ç”¨ä¸²æµæ¨¡å¼
- `options`: æ¨¡å‹åƒæ•¸ï¼ˆå¯é¸ï¼‰

#### ChatMessage
- `role`: è§’è‰²ï¼ˆsystem/user/assistant/toolï¼‰
- `content`: è¨Šæ¯å…§å®¹
- `images`: åœ–åƒåˆ—è¡¨ï¼ˆå¯é¸ï¼‰

#### ChatRequest
- `model`: æ¨¡å‹åç¨±
- `messages`: å°è©±è¨Šæ¯åˆ—è¡¨
- `format`: å›æ‡‰æ ¼å¼ï¼ˆå¯é¸ï¼‰
- `stream`: æ˜¯å¦ä½¿ç”¨ä¸²æµæ¨¡å¼
- `tools`: å·¥å…·å®šç¾©ï¼ˆå¯é¸ï¼‰

#### EmbedRequest
- `model`: æ¨¡å‹åç¨±
- `input`: è¼¸å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
- `truncate`: æ˜¯å¦æˆªæ–·é•·æ–‡æœ¬

## ç¯„ä¾‹

æŸ¥çœ‹ `examples/` ç›®éŒ„ä¸­çš„è©³ç´°ç¯„ä¾‹ï¼š

- `basic_usage.py` - åŸºæœ¬ä½¿ç”¨æ–¹æ³•
- `structured_output.py` - çµæ§‹åŒ–è¼¸å‡ºç¯„ä¾‹
- `streaming.py` - ä¸²æµæ¨¡å¼ç¯„ä¾‹
- `model_validation.py` - æ¨¡å‹é©—è­‰åŠŸèƒ½ç¯„ä¾‹

## éŒ¯èª¤è™•ç†

```python
from ollama_flow import OllamaClient

client = OllamaClient()

try:
    response = client.generate(
        model="llama3.2",
        prompt="ä½ å¥½ï¼",
        stream=False
    )
    print(response.response)
except Exception as e:
    print(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
```

## éœ€æ±‚

- Python 3.7+
- requests >= 2.31.0
- pydantic >= 2.0.0
- é‹è¡Œä¸­çš„ Ollama æœå‹™

## è¨±å¯è­‰

MIT License

## è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork é€™å€‹å°ˆæ¡ˆ
2. å»ºç«‹æ‚¨çš„åŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ‚¨çš„è®Šæ›´ (`git commit -m 'Add some amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. é–‹å•Ÿä¸€å€‹ Pull Request

## æ›´æ–°æ—¥èªŒ

### v0.1.0
- åˆå§‹ç™¼å¸ƒ
- æ”¯æ´ generate, chat, embed API
- çµæ§‹åŒ–è¼¸å‡ºæ”¯æ´
- ä¸²æµæ¨¡å¼æ”¯æ´
- æ™ºèƒ½æ¨¡å‹é©—è­‰åŠŸèƒ½
- æ¨¡å‹åˆ—è¡¨ç·©å­˜æ©Ÿåˆ¶
- å®Œæ•´çš„é¡å‹æç¤º

## ç›¸é—œé€£çµ

- [Ollama](https://ollama.com/) - æœ¬åœ° LLM é‹è¡Œå¹³å°
- [Ollama API æ–‡æª”](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Pydantic](https://docs.pydantic.dev/) - è³‡æ–™é©—è­‰å‡½å¼åº«
