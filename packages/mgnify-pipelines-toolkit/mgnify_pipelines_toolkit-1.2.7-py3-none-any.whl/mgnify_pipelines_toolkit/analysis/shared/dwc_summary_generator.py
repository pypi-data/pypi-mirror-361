#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright 2024-2025 EMBL - European Bioinformatics Institute
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
from collections import defaultdict
import pathlib
import logging
import requests

import pandas as pd
import pyfastx

logging.basicConfig(level=logging.DEBUG)

URL = "https://www.ebi.ac.uk/ena/portal/api/search?result"
RUNS_URL = f"{URL}=read_run&fields=secondary_study_accession,sample_accession&limit=10&format=json&download=false"
SAMPLES_URL = f"{URL}=sample&fields=lat,lon,collection_date,depth&limit=10&format=json&download=false"
HEADERS = {"Accept": "application/json"}


def parse_args():

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i",
        "--input_path",
        required=True,
        type=str,
        help="Directory where ASV files are.",
    )
    parser.add_argument(
        "-r",
        "--runs",
        required=True,
        type=str,
        help="Path to CSV file containing successful analyses generated by the pipeline (columns: `run, status`)",
    )
    parser.add_argument(
        "-o", "--output", required=True, type=str, help="Path to output directory."
    )

    args = parser.parse_args()

    input_path = args.input_path
    runs = args.runs
    output = args.output

    return input_path, runs, output


def get_metadata_from_run_acc(run_acc):

    query = f"{RUNS_URL}&includeAccessions={run_acc}"
    res_run = requests.get(query, headers=HEADERS)

    if res_run.status_code != 200:
        logging.error(f"Data not found for run {run_acc}")
        return False

    sample_acc = res_run.json()[0]["sample_accession"]

    query = f"{SAMPLES_URL}&includeAccessions={sample_acc}"
    res_sample = requests.get(query, headers=HEADERS)

    full_res_dict = res_run.json()[0] | res_sample.json()[0]

    fields_to_clean = ["lat", "lon", "depth"]

    for field in fields_to_clean:
        val = full_res_dict[field]
        if val == "":
            full_res_dict[field] = "NA"

    if full_res_dict["collection_date"] == "":
        full_res_dict["collectionDate"] = "NA"
    else:
        full_res_dict["collectionDate"] = full_res_dict["collection_date"]

    del full_res_dict["collection_date"]

    res_df = pd.DataFrame(full_res_dict, index=[0])
    res_df.columns = [
        "RunID",
        "SampleID",
        "StudyID",
        "decimalLongitude",
        "depth",
        "decimalLatitude",
        "collectionDate",
    ]

    return res_df


def get_all_metadata_from_runs(runs):

    run_metadata_dict = defaultdict(dict)

    for run in runs:
        res_df = get_metadata_from_run_acc(run)
        if res_df is not False:
            run_metadata_dict[run] = res_df

    return run_metadata_dict


def cleanup_taxa(df):

    df.pop("Kingdom")
    cleaned_df = df.rename(columns={"Superkingdom": "Kingdom", "asv": "ASVID"})

    ranks = ["Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"]

    for rank in ranks:
        cleaned_df[rank] = cleaned_df[rank].apply(
            lambda x: x.split("__")[1] if pd.notnull(x) else "NA"
        )

    for rank in ranks:
        cleaned_df[rank] = cleaned_df[rank].apply(lambda x: x if x != "" else "NA")

    cleaned_df = cleaned_df[
        [
            "ASVID",
            "StudyID",
            "SampleID",
            "RunID",
            "decimalLongitude",
            "decimalLatitude",
            "depth",
            "collectionDate",
            "Kingdom",
            "Phylum",
            "Class",
            "Order",
            "Family",
            "Genus",
            "Species",
            "ASVSeq",
        ]
    ]

    return cleaned_df


def get_asv_dict(runs_df, root_path):

    asv_dict = {}
    for i in range(0, len(runs_df)):
        run_acc = runs_df.loc[i, "run"]
        status = runs_df.loc[i, "status"]

        if status != "all_results":
            continue

        tax_file = sorted(
            list(
                (pathlib.Path(root_path) / run_acc / "asv").glob(
                    "*_DADA2-SILVA_asv_tax.tsv"
                )
            )
        )[0]
        count_files = sorted(
            list(pathlib.Path(f"{root_path}/{run_acc}/asv").glob("*S-V*/*.tsv"))
        )

        asv_fasta_file = sorted(
            list(pathlib.Path(f"{root_path}/{run_acc}/asv").glob("*_asv_seqs.fasta"))
        )[0]
        fasta = pyfastx.Fasta(str(asv_fasta_file), build_index=False)
        asv_fasta_dict = {name: seq for name, seq in fasta}
        asv_fasta_df = pd.DataFrame(asv_fasta_dict, index=["ASVSeq"]).transpose()
        asv_fasta_df["asv"] = asv_fasta_df.index
        run_tax_df = pd.read_csv(tax_file, sep="\t")

        count_dfs = []

        for count_file in count_files:
            count_df = pd.read_csv(count_file, sep="\t")
            count_dfs.append(count_df)

        all_ampregions_count_df = pd.concat(count_dfs)
        merged_df = all_ampregions_count_df.merge(
            run_tax_df, left_on="asv", right_on="ASV"
        )
        merged_df.pop("ASV")
        run_col = [run_acc] * len(merged_df)
        merged_df["RunID"] = run_col
        merged_df = merged_df.merge(asv_fasta_df, on="asv")
        asv_dict[run_acc] = merged_df

    return asv_dict


def main():

    input_path, runs, output = parse_args()

    root_path = pathlib.Path(input_path)

    if not root_path.exists():
        logging.error(f"Results path does not exist: {root_path}")
        exit(1)

    runs_df = pd.read_csv(runs, names=["run", "status"])

    all_runs = runs_df.run.to_list()
    run_metadata_dict = get_all_metadata_from_runs(all_runs)
    asv_dict = get_asv_dict(runs_df, root_path)

    all_merged_df = []

    for run in all_runs:
        if run in asv_dict.keys() and run in run_metadata_dict.keys():
            run_asv_data = asv_dict[run]
            run_metadata = run_metadata_dict[run]
            run_merged_result = run_metadata.merge(run_asv_data, on="RunID")
            all_merged_df.append(run_merged_result)

    final_df = pd.concat(all_merged_df, ignore_index=True)
    final_df = cleanup_taxa(final_df)

    final_df.to_csv(f"{output}_dwcready.csv", index=False, na_rep="NA")


if __name__ == "__main__":
    main()
