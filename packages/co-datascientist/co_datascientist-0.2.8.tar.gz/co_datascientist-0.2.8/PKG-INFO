Metadata-Version: 2.4
Name: co-datascientist
Version: 0.2.8
Summary: A tool for agentic recursive model improvement
Project-URL: Homepage, https://github.com/TropiFloAI/co-datascientist
Project-URL: Issues, https://github.com/TropiFloAI/co-datascientist/issues
Author-email: David Gedalevich <davidgdalevich7@gmail.com>
License: Copyright (c) 2018 The Python Packaging Authority
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
License-File: LICENSE
Requires-Python: >=3.12
Requires-Dist: click>=8.1.8
Requires-Dist: fastmcp>=2.2.5
Requires-Dist: httpx>=0.28.1
Requires-Dist: ipdb>=0.13.13
Requires-Dist: keyring>=25.6.0
Requires-Dist: keyrings-alt>=5.0.0
Requires-Dist: pydantic-settings>=2.9.1
Requires-Dist: yaspin>=3.1.0
Description-Content-Type: text/markdown

# ğŸ¤–âœ¨ Co-DataScientist

<div align="center">
  <img src="figures/Co-DataScientist.png" alt="Co-DataScientist Logo" width="420"/>
</div>

<p align="center">
  <img src="https://img.shields.io/badge/version-1.0.0-blue.svg" alt="Version"/>
  <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="License"/>
  <img src="https://img.shields.io/badge/license-EPICğŸ”¥-orange.svg" alt="Epic License"/>
  <img src="https://img.shields.io/badge/license-ML%20Beast-red.svg" alt="ML Beast License"/>
</p>

> **Kick back, relax, and tomorrow morning greet a shiny KPI you can parade at ML stand-up. ğŸ‰**

---

## ğŸš€ Why is everyone talking about the Co-DataScientist?

- ğŸ§ª **Idea Explosion** â€” Launches a swarm of models, feature recipes & hyper-parameters you never knew existed.
- ğŸŒŒ **Full-Map Exploration** â€” Charts the entire optimization galaxy so you can stop guessing and start winning.
- â˜• **Hands-Free Mode** â€” Hit *run*, kick back with a latte (or snooze) and let the search party work through the night.
- ğŸ“ˆ **KPI Fanatic** â€” Every evolutionary step is laser-focused on cranking that one number sky-high.
- ğŸ”’ **Data Stays Home** â€” Your training and testing data **never leaves your server**; everything runs 100 % locally.
- ğŸ¤‘ **Zero-Surprise Costs** â€” Live token & dollar tracking keeps the finance goblins happy.

Fast-track your ML pipelines from ğŸ˜© _painful_ to ğŸ† _heroic_
---

## ğŸ”§ Quickstart â€” â±ï¸ *30-Second Setup*

1. **Install**

```bash
pip install co-datascientist
```

2. **Write a tiny script** (e.g. `xor.py`). The _only_ rule: **print your KPI**! ğŸ·ï¸

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np

# XOR toy-set
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0,1,1,0])

# EVOLVE-BLOCK-START

pipe = Pipeline([
    ("scale", StandardScaler()),
    ("clf", LogisticRegression(random_state=0))
])

pipe.fit(X, y)
acc = accuracy_score(y, pipe.predict(X))

# EVOLVE-BLOCK-END


print(f"KPI: {acc:.4f}")  # ğŸ¯ Tag your metric!
```

3. **Run the magic!** âœ¨

```bash
co-datascientist run --script-path my/path/to/xor.py
```

Watch accuracy jump from `0.5` ğŸ«  to `1.0` ğŸ†!

<h2 align="center"><b>
Try it on <i>your</i> toughest problem and see how your KPI improves.<br>
<span style="font-size:2em;">ğŸ¯ğŸš€</span><br>
<b>Co-DataScientist helps you get better resultsâ€”no matter how big your challenge.</b>
</b></h2>

---

> **Important Notes About Your Input Script**



## ğŸ¯ KPI Tagging

Co-DataScientist scans your stdout for the pattern `KPI: <number>` â€” thatâ€™s the metric it maximizes. Use **anything**: accuracy, F1, revenue per click, unicorns-per-secondâ€¦ you name it!

---

## ğŸ§¬ Blocks to evolve

As you will see in the XOR exmaple, Co-DataScientist uses **# EVOLVE-BLOCK-START** and **# EVOLVE-BLOCK-END** tags to identify the parts of the system you want it to improve. Make sure to tag parts of your system you care about improving! It will help to Co-DataScientist stay focused on its job.

> **Other helpful stuff**

## ğŸ’° Cost Tracking

Stay on budget with one-liners:

```bash
co-datascientist costs            # summary
co-datascientist costs --detailed # per-run breakdown
```

Powered by **LiteLLM**â€™s real-time pricing.

---

## ğŸ“ Before vs After
<table>
<tr>
<th>ğŸ“¥ "Meh" Pipeline <br><sub>KPI â‰ˆ 0.50</sub></th>
<th>ğŸš€ Turbocharged by Co-DataScientist <br><sub>KPI ğŸš€ 1.00</sub></th>
</tr>
<tr>
<td>

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
import numpy as np

# XOR data
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 1, 1, 0])

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', RandomForestClassifier(n_estimators=10, random_state=0))
])

pipeline.fit(X, y)
preds = pipeline.predict(X)
accuracy = accuracy_score(y, preds)
print(f'Accuracy: {accuracy:.2f}')
print(f'KPI: {accuracy:.4f}')
```

</td>
<td>

```python
import numpy as np
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from tqdm import tqdm

class ChebyshevPolyExpansion(BaseEstimator, TransformerMixin):
    def __init__(self, degree=3):
        self.degree = degree
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        X = np.asarray(X)
        X_scaled = 2 * X - 1
        n_samples, n_features = X_scaled.shape
        features = []
        for f in tqdm(range(n_features), desc='Chebyshev features'):
            x = X_scaled[:, f]
            T = np.empty((self.degree + 1, n_samples))
            T[0] = 1
            if self.degree >= 1:
                T[1] = x
            for d in range(2, self.degree + 1):
                T[d] = 2 * x * T[d - 1] - T[d - 2]
            features.append(T.T)
        return np.hstack(features)

X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 1, 1, 0])

pipeline = Pipeline([
    ('cheb', ChebyshevPolyExpansion(degree=3)),
    ('scaler', StandardScaler()),
    ('clf', RandomForestClassifier(n_estimators=10, random_state=0))
])

pipeline.fit(X, y)
preds = pipeline.predict(X)
accuracy = accuracy_score(y, preds)
print(f'Accuracy: {accuracy:.2f}')
print(f'KPI: {accuracy:.4f}')
```

</td>
</tr>
</table>

---


## ğŸ™‹â€â™€ï¸ Need help?

Weâ€™d love to chat: [oz.kilim@tropiflo.io](mailto:oz.kilim@tropiflo.io)

---

<p align="center"><strong>All set? Ignite your pipelines and watch them soar! ğŸš€</strong></p>

<p align="center"><em>âš ï¸  Disclaimer: Co-DataScientist executes your scripts on your own machine. Make sure you trust the code you feed it!</em></p>

<p align="center">Made with â¤ï¸ by the Tropiflo team</p>
