Metadata-Version: 2.4
Name: rag_evaluation
Version: 0.2.0
Summary: A robust Python package for evaluating Retrieval-Augmented Generation (RAG) systems.
Author-email: Ola Akindele <akindele.ok@gmail.com>
License-Expression: MIT
Project-URL: Homepage, https://github.com/OlaAkindele/rag_evaluation
Keywords: rag,nlp,evaluation
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python :: 3.10
Classifier: Intended Audience :: Developers
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pandas>=1.0
Requires-Dist: openai>=0.27
Requires-Dist: python-dotenv>=0.21
Dynamic: license-file

# RAG Evaluation

**RAG Evaluation** is a Python package designed for evaluating Retrieval-Augmented Generation (RAG) systems. It provides a systematic way to score and analyze the quality of responses generated by RAG systems. This package is particularly suited for projects leveraging Large Language Models (LLMs) such as GPT, Gemini, etc.

It integrates easily with the OpenAI API via the `openai` package and automatically handles environment variable-based API key loading through `python-dotenv`.

## Features

- **Multi-Metric Evaluation:** Evaluate responses using the following metrics:
  - **Query Relevance**
  - **Factual Accuracy**
  - **Coverage**
  - **Coherence**
  - **Fluency**
- **Standardized Prompting:** Uses a well-defined prompt template to consistently assess responses.
- **Customizable:** Easily extendable to add new metrics or evaluation criteria.
- **Easy Integration:** Provides a high-level function to integrate evaluation into your RAG pipelines.

## Installation

```bash
pip install rag_evaluation
```

## Usage
### Open-Source Local Models (Ollama models; does not require external APIs)
**Currently, the package supports Llama, Mistral, and Qwen.**

```python
from openai import OpenAI

client = OpenAI(
    api_key='ollama',
    base_url="http://localhost:11434/v1"
)

# List all available models
models = client.models.list()
print(models.to_json())

```

### Usage with Open-Source Models (Ollama models)

```python
from rag_evaluation.evaluator import evaluate_response

# Define the inputs
query = "Which large language model is currently the largest and most capable?"

response_text = """The largest and most capable LLMs are the generative pretrained transformers (GPTs). These models are 
                designed to handle complex language tasks, and their vast number of parameters gives them the ability to 
                understand and generate human-like text."""
                 
document = """A large language model (LLM) is a type of machine learning model designed for natural language processing 
            tasks such as language generation. LLMs are language models with many parameters, and are trained with 
            self-supervised learning on a vast amount of text. The largest and most capable LLMs are 
            generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided 
            by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies 
            inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in."""

# Llama usage (ollama pull llama3.2:1b to download from terminal)
report = evaluate_response(
    query=query,
    response=response_text,
    document=document,
    model_type="ollama",
    model_name='llama3.2:1b',
)
print(report)

# Mistral usage (ollama pull mistral to download from terminal)
report = evaluate_response(
    query=query,
    response=response_text,
    document=document,
    model_type="ollama",
    model_name='mistral:latest',
)
print(report)

# Qwen usage (ollama pull qwen to download from terminal)
report = evaluate_response(
    query=query,
    response=response_text,
    document=document,
    model_type="ollama",
    model_name='qwen:latest',
)
print(report)

```

### For API-based Models (GPT and Gemini)
```python
import os
from rag_evaluation.config import get_api_key
from rag_evaluation.evaluator import evaluate_response

# Set the API key (either via environment variable or by providing a default)
os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"

# OR

api_key = get_api_key("OPENAI_API_KEY")

# Define your inputs (same as above)

# OpenAI usage (ensure API key is set manually or in environment variable)
report = evaluate_response(
    query=query,
    response=response_text,
    document=document,
    model_type="openai",
    model_name='gpt-4.1',
)
print(report)

# Gemini usage (ensure API key is set manually or in environment variable)
report = evaluate_response(
    query=query,
    response=response_text,
    document=document,
    model_type="gemini",
    model_name='gemini-2.5-flash',
)
print(report)

```

## Output

The `evaluate_response` function returns a pandas DataFrame with:
- **Metric Names:** Query Relevance, Factual Accuracy, Coverage, Coherence, Fluency.
- **Normalized Scores:** A 0â€“1 score for each metric.
- **Percentage Scores:** The normalized score expressed as a percentage.
- **Overall Accuracy:** A weighted average score across all metrics.
