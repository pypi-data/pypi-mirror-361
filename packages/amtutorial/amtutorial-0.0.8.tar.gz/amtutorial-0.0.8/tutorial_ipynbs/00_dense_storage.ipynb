{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Dense Storage\n",
    "> Improving the storage capacity of the Hopfield Network\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bhoov/amtutorial/blob/main/tutorial_ipynbs/00_dense_storage.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Uncomment to download dependencies to colab\n",
    "# !pip install amtutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import jax, jax.numpy as jnp, jax.random as jr, jax.tree_util as jtu, jax.lax as lax\n",
    "import equinox as eqx\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, Image, display, Video, Markdown\n",
    "import imageio\n",
    "import numpy as np\n",
    "from fastcore.basics import * \n",
    "from fastcore.test import *\n",
    "from fastcore.meta import *\n",
    "import os\n",
    "from jaxtyping import Float, Array, Scalar\n",
    "import matplotlib.gridspec as gridspec\n",
    "import functools as ft\n",
    "from pathlib import Path\n",
    "from moviepy.editor import ipython_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Notebook Execution Settings\"\n",
    "CACHE_DIR = \"cache/00_dense_storage\"\n",
    "CACHE_RECALL = True # If False, regenerate all saved results even if files exist.\n",
    "SHOW_FULL_ANIMATIONS = True # If True, render videos instead of gifs. This is slower than gifs and relies on `ffmpeg` to save the animation, but it lets us see the energy descent alongside the frame evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Associative Memory\n",
    "\n",
    "Our goal in this section is to build the smallest abstraction for Associative Memory, which at its core is just an *energy function* $E_\\Xi(x) \\in \\mathbb{R}$. where *query pattern* $x \\in \\{-1, 1\\}^D$ is a possibly noisy $D$-dimensional, binary pattern and *memory matrix* $\\Xi \\in \\{-1, 1\\}^{K \\times D}$ is our matrix of $K$ stored patterns. $E_\\Xi(x)$ stores patterns at low energies. To retrieve our stored patterns,we want to minimize $E_\\Xi(x)$.\n",
    "\n",
    "Let's assume an unimplemented, arbitrary energy function and setup a basic object for a binary AM. All we need to provide is an `energy` method, parameterized by $\\Xi$, that is a function of query $x$.\n",
    "\n",
    "Historically, the Hopfield Network minimizes energy using *asynchronous* update rules (where we minimize the query's energy one randomly selected bit at a time). We'll follow that precedent in this notebook since it makes for nicer visualizations, though fully *synchronous* update rules (where we minimize the energy by scanning through all bits sequentially) are also possible. The default `async_update` is simple: for a randomly sampled bit in the query pattern, compare the energy of that bit when it is flipped and not flipped. Keep the pattern whose energy is lower.\n",
    "\n",
    "$$\n",
    "x_i^{(t+1)} = \\underset{b \\in \\{-1, 1\\}}{\\mathrm{argmin}}\\left[E\\left(x_i = b, x_{j \\neq i} = x_j^{(t)}\\right)\\right]\n",
    "$${#eq-async-update}\n",
    "\n",
    "Converting a noisy query pattern into a stored pattern is a matter of repeatedly applying the `async_update` rule to minimize energy. Because this process, if run long enough, will \"recall\" a memory, we call this the `async_recall` method.\n",
    "\n",
    "We use `jax` primitives like `jax.lax.scan` and `jax.lax.cond` so we can JIT our code run quickly. A `scan` is just a glorified for loop, and a `cond` is a glorified if-else statement.\n",
    "\n",
    "With all this, we can fully encapsulate a basic, binary AM in the following object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryAM(eqx.Module):\n",
    "    Xi: Float[Array, \"K D\"] # matrix of stored patterns \n",
    "    def energy(\n",
    "        self, \n",
    "        x: Float[Array, \"D\"] # Possibly noisy query pattern\n",
    "        ): \n",
    "        ... # Left to implement later\n",
    "\n",
    "    def async_update(\n",
    "        self,\n",
    "        x: Float[Array, \"D\"], # Possibly noisy query pattern\n",
    "        idx:int,              # Index of bit to flip\n",
    "        ):                    # Return next state and its energy\n",
    "        \"Minimize the energy of `x[idx]`\"\n",
    "        xflipped = jnp.array(x).at[idx].multiply(-1)\n",
    "        energy_og = self.energy(x)\n",
    "        energy_flipped = self.energy(xflipped)\n",
    "        keep_flip = (energy_flipped - energy_og) < 0\n",
    "        return lax.cond(\n",
    "            keep_flip, \n",
    "            # Keep flipped bit if it has lower energy\n",
    "            lambda: (xflipped, energy_flipped), \n",
    "            # Otherwise keep original bit\n",
    "            lambda: (x, energy_og)\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def async_recall(\n",
    "        self, \n",
    "        x0: Float[Array, \"D\"], # Initial query pattern\n",
    "        nsteps:int=20000, # Number of bits to flip & check\n",
    "        key=jr.PRNGKey(0) # Random key for bit-flip choices\n",
    "        ):\n",
    "        \"Minimize energy of `x0` by repeatedly applying `async_update`\"\n",
    "        def update_step(x, idx):\n",
    "            x_new, energy_new = self.async_update(x, idx)\n",
    "            return x_new, (x_new, energy_new)\n",
    "        D = x0.shape[-1]\n",
    "\n",
    "        # Randomly sample `nsteps` bits to flip\n",
    "        bitflip_sequence = jr.choice(key, np.arange(D), shape=(nsteps,))\n",
    "\n",
    "        # Apply `async_update` to each bitflip in seq\n",
    "        final_x, (frames, energies) = lax.scan(update_step, x0, bitflip_sequence)\n",
    "\n",
    "        # Return final pattern and the trajectory\n",
    "        return final_x, (frames, energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    ":::{.callout-note}\n",
    "Feel free to skip this section. It's just loading data and setting up some fancy visualization functions.\n",
    ":::\n",
    "\n",
    "Let's build some helper functions to load and view our data: binarized pokemon sprites. While other fields like to work with $\\{0,1\\}$ binary data, Hopfield Networks like to work with bipolar data where each datapoint $x \\in \\{-1, 1\\}^D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "from amtutorial.data_utils import get_pokemon_data\n",
    "poke_pixels, poke_names = get_pokemon_data()\n",
    "data = poke_pixels\n",
    "\n",
    "pxh, pxw = data.shape[-2:]\n",
    "data = data.reshape(-1, pxh * pxw)\n",
    "\n",
    "def gridify(images, grid_h=None):\n",
    "    \"\"\"Convert list of images to a single grid image\"\"\"\n",
    "    images = np.array(images)  # Shape: (n_images, H*W)\n",
    "    if grid_h is None: grid_h = int(np.sqrt(len(images)))\n",
    "    grid_w = int(np.ceil(len(images) / grid_h))\n",
    "\n",
    "    # Pad if necessary\n",
    "    n_needed = grid_h * grid_w\n",
    "    if len(images) < n_needed:\n",
    "        padding_shape = (n_needed - len(images),) + images.shape[1:]\n",
    "        padding = np.zeros(padding_shape)\n",
    "        images = np.concatenate([images, padding], axis=0)\n",
    "    \n",
    "    # Reshape individual images and arrange in grid\n",
    "    grid = rearrange(images[:n_needed], '(gh gw) h w -> (gh h) (gw w)', gh=grid_h, gw=grid_w)\n",
    "    return grid\n",
    "\n",
    "def show_im(x, ax=None, do_gridify=True, grid_h=None, figsize=None):\n",
    "    \"\"\"Vector to figure\"\"\"\n",
    "    x = rearrange(x, \"... (h w) -> ... h w\", h=pxh, w=pxw)\n",
    "    if do_gridify and len(x.shape) == 3: x = gridify(x, grid_h)\n",
    "    empty_ax = ax is None\n",
    "    figsize = figsize or (8, 2.67) # Quarto aspect ratio\n",
    "    if empty_ax: fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(x, cmap=\"gray\", vmin=-1, vmax=1)\n",
    "    ax.axis(\"off\")\n",
    "    return None if not empty_ax else fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "sample_data = jr.choice(jr.PRNGKey(10), data, shape=(100,), replace=False)\n",
    "fig1, ax1 = show_im(sample_data, figsize=(6, 6));\n",
    "ax1.set_title(\"Example patterns (black=-1, white=1)\")\n",
    "plt.show()\n",
    "\n",
    "np.unique(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classical Hopfield Network\n",
    "\n",
    "Let's revisit our task to store $K$ binary patterns each of dimension $D$ into an energy function. Let's keep things simple and fast for the first part of this notebook and focus on storing and retrieving $K=2$ patterns: an eevee and pichu, where each `(48,48)` image is rasterized to a vector dimension of $D=2304$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_names = [\"eevee\", \"pichu\"]\n",
    "eevee_pichu_idxs = [poke_names.index(name) for name in desired_names]\n",
    "Xi = data[eevee_pichu_idxs]\n",
    "\n",
    "fig, ax = show_im(Xi, figsize=(6,3));\n",
    "ax.set_title(\"Stored patterns\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"K={Xi.shape[0]}, D={Xi.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Classical Hopfield Network (CHN) defines an energy function for this collection of patterns, putting the $\\mu$-th stored pattern $\\xi_\\mu$ at a *low* value of energy. The CHN energy is a quadratic function described by dot-product correlations:\n",
    "\n",
    "$$\n",
    "E_\\text{CHN}(x) = -\\frac{1}{2} \\left(\\sum_{\\mu} \\xi_{\\mu i} x_i\\right)^2 = -\\frac{1}{2} \\sum_{i,j} T_{ij} x_i x_j.\n",
    "$${#eq-chn-energy}\n",
    "\n",
    "We see the familiar equation for CHN energy on the RHS if we expand the quadratic function, where $T_{ij} := \\sum_{\\mu=1}^K \\xi_{\\mu i} \\xi_{\\mu_j}$ is the matrix of symmetric synapses. Learned patterns $\\xi_\\mu$ are stored in $T$ via a simple, Hebbian learning rule. \n",
    "\n",
    "The CHN can be easily implemented in code via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHN(BinaryAM):\n",
    "    def energy(\n",
    "        self, \n",
    "        x: Float[Array, \"D\"] # Possibly noisy query pattern\n",
    "        ): \n",
    "        \"Quadratic energy function for the CHN\"\n",
    "        return -0.5 * jnp.sum((self.Xi @ x)**2, axis=0)\n",
    "\n",
    "chn = CHN(Xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asynchronous update rule of @eq-async-update uses the energy difference of a flipped bit to determine whether to keep the flip or not. That update rule is equivalent to the following, arguably more familiar update rule, which describes the next state based on the sign of the *total input current* to the neuron $x_i$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_i^{(t+1)} &\\leftarrow \\text{sgn}\\left(\\sum_{\\mu} \\xi_{\\mu i} \\sum_{j \\neq i} \\left(\\xi_{\\mu j} x_j^{(t)}\\right) \\right)\\\\\n",
    "\\text{sgn}(x) &:= \\begin{cases}\n",
    "1 & \\text{if } x \\geq 0 \\\\\n",
    "-1 & \\text{if } x < 0\n",
    "\\end{cases}\\quad.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This update rule also ensures the network always moves toward lower energy states. Because the $E_\\text{CHN}$ is bounded from below, the network will eventually converge to a local minimum that (ideally) corresponds to one of the stored patterns.\n",
    "\n",
    "<!-- ```{python}\n",
    "@patch\n",
    "def async_update(self:BinaryCHN, x, idx):\n",
    "    new_xidx = ((self.Xi.T @ self.Xi @ x.at[idx].set(0))[idx] >= 0) * 2 - 1\n",
    "    xnew = x.at[idx].set(new_xidx)\n",
    "    energy = self.energy(xnew)\n",
    "    return xnew, energy\n",
    ".``` -->\n",
    "\n",
    "Let's observe the recall process! We'll start with a noisy version of the first pattern and see if we can recover it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_some_bits(key, x, p=0.1):\n",
    "    \"Flip `p` fraction of bits in `x`\"\n",
    "    prange = np.array([p, 1-p])\n",
    "    return x * jr.choice(key, np.array([-1, 1]), p=prange, shape=x.shape)\n",
    "\n",
    "x_og = Xi[0] \n",
    "x_noisy = flip_some_bits(jr.PRNGKey(0), x_og, 0.2)\n",
    "\n",
    "show_im(jnp.stack([x_og, x_noisy]), figsize=(6, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pedagogical purpose of this notebook, we'll cache the recall process and results so we don't have to run it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates(BinaryAM.async_recall)\n",
    "def cached_recall(am, cache_name, x_noisy, key=jr.PRNGKey(0), save=True, **kwargs):\n",
    "    \"Cache the recall process using key `cache_name`\"\n",
    "    npz_fname = Path(CACHE_DIR) / (cache_name + '.npz')\n",
    "    if npz_fname.exists() and CACHE_RECALL: \n",
    "        npz_data = np.load(npz_fname)\n",
    "        x_final, frames, energies = npz_data['x_final'], npz_data['frames'], npz_data['energies']\n",
    "        print(\"Loading cached recall data\")\n",
    "    else: \n",
    "        x_final, (frames, energies) = am.async_recall(x_noisy, key=key, **kwargs)\n",
    "        if save: jnp.savez(npz_fname, x_final=x_final, frames=frames, energies=energies)\n",
    "    return x_final, frames, energies\n",
    "\n",
    "cache_name = 'basic_hopfield_recovery'\n",
    "x_final, frames, energies = cached_recall(chn, cache_name, x_noisy, nsteps=12000, key=jr.PRNGKey(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "def show_recall_output(x_og, x_noisy, x_final, energies, show_original=True):\n",
    "    nimgs = 4 if show_original else 3\n",
    "    figsize = (8, 2.2) if nimgs == 4 else (8, 2.67)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = gridspec.GridSpec(1, nimgs, figure=fig, wspace=0.25) \n",
    "    axes = [fig.add_subplot(gs[0, i]) for i in range(nimgs)]\n",
    "\n",
    "    show_im(x_noisy, axes[0]); axes[0].set_title(\"Noisy Query\")\n",
    "    show_im(x_final, axes[1]); axes[1].set_title(\"Retrieved pattern\")\n",
    "    \n",
    "    # Add energy plot\n",
    "    axes[2].plot(energies)\n",
    "    axes[2].set_title(\"Energy during Recall\")\n",
    "    axes[2].set_xlabel(\"Iteration\")\n",
    "    axes[2].set_ylabel(\"Energy\")\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    if show_original:\n",
    "        show_im(x_og, axes[3])\n",
    "        axes[3].set_title(\"Original pattern\")\n",
    "    return fig, axes\n",
    "\n",
    "fig, axes = show_recall_output(x_og, x_noisy, x_final, energies, show_original=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can animate the recall process to view the \"thinking\" process of the CHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "def show_recall_animation(frames, cache_name, steps_per_sample=32):\n",
    "    images = list(frames[::steps_per_sample])\n",
    "    def frame_to_image(frame): return ((frame.reshape(pxh, pxw) + 1) * 127.5).astype(np.uint8)\n",
    "    images = [frame_to_image(frame) for frame in images]\n",
    "    fname = Path(CACHE_DIR) / (cache_name + '.gif')\n",
    "    imageio.mimsave(fname, images, duration=0.001, loop=0)\n",
    "    return Image(filename=fname, width=pxh * 5), fname\n",
    "\n",
    "def show_recall_with_energy_animation(frames, energies, cache_name, steps_per_sample=32, force_remake=False, fps=15):\n",
    "    \"\"\"Create animated video showing both frame evolution and energy descent\n",
    "    \n",
    "    You might need to install 'ffmpeg' to save as mp4: conda install -c conda-forge ffmpeg\n",
    "    \"\"\"\n",
    "    # Change file extension to mp4 for video output\n",
    "    video_fname = Path(CACHE_DIR) / (cache_name + \".mp4\")\n",
    "\n",
    "    if not os.path.exists(video_fname) or force_remake:\n",
    "        # Downsample frames and energies\n",
    "        sampled_frames = frames[::steps_per_sample]\n",
    "        sampled_energies = energies[::steps_per_sample]\n",
    "        sampled_steps = np.arange(0, len(energies), steps_per_sample)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Initialize image plot\n",
    "        im = ax1.imshow(sampled_frames[0].reshape(pxh, pxw), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "        ax1.set_title(\"Step 0\")\n",
    "        ax1.axis(\"off\")\n",
    "        \n",
    "        # Initialize energy plot\n",
    "        line, = ax2.plot([], [], 'b-', linewidth=2)\n",
    "        scatter = ax2.scatter([], [], color='red', s=50, zorder=5)\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Energy')\n",
    "        ax2.set_title('Energy During Recall')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xlim(0, sampled_steps[-1])\n",
    "        ax2.set_ylim(min(sampled_energies) * 1.1, max(sampled_energies) * 1.1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        def update(i):\n",
    "            # Update left subplot (frame evolution)\n",
    "            frame_img = sampled_frames[i].reshape(pxh, pxw)\n",
    "            im.set_data(frame_img)\n",
    "            ax1.set_title(f\"Step {sampled_steps[i]}\")\n",
    "            \n",
    "            # Update right subplot (energy descent)\n",
    "            current_steps = sampled_steps[:i+1]\n",
    "            current_energies = sampled_energies[:i+1]\n",
    "            line.set_data(current_steps, current_energies)\n",
    "            scatter.set_offsets(np.array([[sampled_steps[i], sampled_energies[i]]]))\n",
    "            \n",
    "            return im, line, scatter\n",
    "\n",
    "        anim = animation.FuncAnimation(fig, update, frames=len(sampled_frames), interval=100, blit=True)\n",
    "        \n",
    "        # Save as MP4\n",
    "        print(f\"Saving animated recall with energy to {video_fname}\")\n",
    "        anim.save(video_fname, writer='ffmpeg', fps=fps) \n",
    "        plt.close(fig)\n",
    "    \n",
    "    # return HTML(f'<video controls src=\"{video_fname}\" width=\"600\"></video>')\n",
    "    return Video(video_fname, width=600), video_fname\n",
    "\n",
    "def show_cached_recall_animation(cache_name, steps_per_sample=32):\n",
    "    npz_fname = Path(CACHE_DIR) / (cache_name + '.npz')\n",
    "    npz_data = np.load(npz_fname)\n",
    "    frames, energies = npz_data['frames'], npz_data['energies']\n",
    "\n",
    "    if SHOW_FULL_ANIMATIONS:\n",
    "        video, video_fname = show_recall_with_energy_animation(frames, energies, cache_name, steps_per_sample=steps_per_sample, force_remake=(not CACHE_RECALL))\n",
    "        return video, video_fname\n",
    "    else:\n",
    "        video_fname = Path(CACHE_DIR) / (cache_name + '.gif')\n",
    "        gif = show_recall_animation(frames, fname, steps_per_sample=steps_per_sample)\n",
    "        return gif, video_fname\n",
    "\n",
    "video, video_fname = show_cached_recall_animation(cache_name, steps_per_sample=32)\n",
    "Markdown(f\"![]({video_fname})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# For google colab only\n",
    "ipython_display(str(video_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving \"inverted\" images {#sec-inverted-retrievals}\n",
    "\n",
    "If we initialize a query with *too much* noise, it's possible to retrieve the negative of a stored pattern or an \"inverted image\". Because the energy is quadratic, both $x$ and $-x$ produce the same small value of energy. Whether we retrieve the original $x$ or the inverted $-x$ is dependent on whether we initialize our query closer to the original or inverted pattern.\n",
    "\n",
    "$$\n",
    "E_\\text{CHN}(-x) = -\\frac{1}{2} \\left(\\sum_{\\mu} \\xi_{\\mu i} (-x_i)\\right)^2 = E_\\text{CHN}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "x_og = Xi[0] \n",
    "x_noisy = flip_some_bits(jr.PRNGKey(1), x_og, 0.6)\n",
    "\n",
    "cache_name = 'hopfield_recovery_inverted'\n",
    "x_final, frames, energies = cached_recall(chn, cache_name, x_noisy, nsteps=15000, key=jr.PRNGKey(5))\n",
    "fig, axes = show_recall_output(x_og, x_noisy, x_final, energies, show_original=False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Accidentally retrieved the inverted pattern!\")\n",
    "video, video_fname = show_cached_recall_animation(cache_name, steps_per_sample=32)\n",
    "Markdown(f\"![]({video_fname})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# For google colab only\n",
    "ipython_display(str(video_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory retrieval failure \n",
    "\n",
    "Unfortunately, the CHN is terrible at storing and retrieving multiple patterns. If we add even four more patterns into the synaptic memory, our network will fail to retrieve our eevee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = data[eevee_pichu_idxs]\n",
    "Xi = jnp.concatenate([Xi, jr.choice(jr.PRNGKey(10), data, shape=(4,), replace=False)])\n",
    "fig, ax = show_im(Xi, figsize=(6, 4));\n",
    "ax.set_title(f\"Stored patterns (K={Xi.shape[0]})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "x_og = Xi[0]\n",
    "x_noisy = flip_some_bits(jr.PRNGKey(0), x_og, 0.2)\n",
    "\n",
    "chn = CHN(Xi)\n",
    "\n",
    "fname = 'hopfield_recovery_fail'\n",
    "x_final, frames, energies = cached_recall(chn, fname, x_noisy, nsteps=15000, key=jr.PRNGKey(5))\n",
    "fig2, axes2 = show_recall_output(x_og, x_noisy, x_final, energies, show_original=False)\n",
    "plt.show()\n",
    "\n",
    "print(\"CHN failed to retrieve the correct pattern!\")\n",
    "video, video_fname = show_cached_recall_animation(fname, steps_per_sample=32)\n",
    "Markdown(f\"![]({video_fname})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# For google colab only\n",
    "ipython_display(str(video_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Associative Memory\n",
    "\n",
    "The CHN has a *quadratic* energy, which is a special case of a more general class of models called **Dense Associative Memory** (DenseAM). If we increase the degree of the polynomial used in the energy function, we strengthen the coupling between neurons and can store more patterns into the same synaptic matrix.\n",
    "\n",
    "The new energy function, written in terms of polynomials of degree $n$ and using the same notation for stored patterns $\\xi_{\\mu i}$, is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E_\\text{DAM}(x) &= -\\sum_{\\mu=1}^K F_n\\left(\\sum_{i=1}^D \\xi_{\\mu i} x_i\\right),\\\\\n",
    "\\text{where}\\;F_n(x) &= \\begin{cases} \\frac{x^n}{n} & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}.\n",
    "\\end{align*}\n",
    "$${#eq-dam-energy}\n",
    "\n",
    ":::{.callout-note}\n",
    "We need $F_n$ to be convex for all $n$, which is why we perform the rectification. We could alternatively limit ourselves to only even values of $n$.\n",
    "\n",
    "Fun fact, rectified polynomials remove the \"inverted\" retrieval phenomenon seen in @sec-inverted-retrievals.\n",
    ":::\n",
    "\n",
    "@eq-dam-energy admits the following manual update rule for a single neuron $i$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_i^{(t+1)} &\\leftarrow \\text{sgn}\\left( \\sum_{\\mu} \\xi_{\\mu i} f_n\\left( \\sum_{j \\neq i} \\xi_{\\mu j} x_j^{(t)}\\right)\\right)\\\\\n",
    "\\end{align*}.\n",
    "$${#eq-dam-update}\n",
    "\n",
    "Here we introduced an activation function $f_n(\\cdot) = F_n'(\\cdot)$ that is the derivative of the rectified polynomial used to define the energy.\n",
    "This update can be viewed as the negative gradient of the energy function, ensuring that the network always moves toward lower energy states. Like before, this energy is bounded from below and we will eventually converge to a local minimum that corresponds to one of the stored patterns.\n",
    "\n",
    "Let's implement the DenseAM model. The primary difference from the CHN is that now we generalize the quadratic energy to a (possibly rectified) polynomial energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialDenseAM(BinaryAM):\n",
    "    Xi: jax.Array # (K, D) Memory patterns \n",
    "    n: int # Power of polynomial F\n",
    "    rectified: bool = True # Whether to rectify inputs to F\n",
    "\n",
    "    def F_n(self, sims): \n",
    "        \"\"\"Rectified polynomial of degree `n` for energy\"\"\"\n",
    "        sims = sims.clip(0) if self.rectified else sims\n",
    "        return 1 / self.n * sims ** self.n\n",
    "\n",
    "    def energy(self, x): \n",
    "        return -jnp.sum(self.F_n(self.Xi @ x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple change to using a polynomial of degree $6$ instead of the CHN's quadratic energy function allows us to store and retrieve our desired eevee even with up to $K=100$ patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of stored patterns!\n",
    "Xi = data[eevee_pichu_idxs]\n",
    "Xi = jnp.concatenate([Xi, jr.choice(jr.PRNGKey(10), data, shape=(98,), replace=False)])\n",
    "fig1, ax1 = show_im(Xi, figsize=(7,7));\n",
    "ax1.set_title(\"Stored patterns\")\n",
    "dam = PolynomialDenseAM(Xi, n=6, rectified=True)\n",
    "\n",
    "fname = f'dam_recovery_n_{dam.n}_K_{Xi.shape[0]}'\n",
    "\n",
    "x_og = Xi[0]\n",
    "x_noisy = flip_some_bits(jr.PRNGKey(0), x_og, 0.2)\n",
    "x_final, frames, energies = cached_recall(dam, fname, x_noisy, nsteps=20000, key=jr.PRNGKey(5))\n",
    "\n",
    "fig2, axes2 = show_recall_output(x_og, x_noisy, x_final, energies, show_original=False)\n",
    "fig2.suptitle(f\"DenseAM(n={dam.n}, K={Xi.shape[0]})\")\n",
    "plt.subplots_adjust(top=0.75)\n",
    "plt.show()\n",
    "\n",
    "video, video_fname = show_cached_recall_animation(fname, steps_per_sample=32)\n",
    "Markdown(f\"![]({video_fname})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# For google colab only\n",
    "ipython_display(str(video_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A higher degree polynomial gives us more **storage capacity**, which means that it is easier to retrieve the patterns we have stored in the network. Note that the higher the degree $n$, the narrower the basins of attraction, which makes it easier to pack more patterns into the energy landscape.\n",
    "\n",
    "## Gotta catch 'em all! {#sec-gotta-catch-em-all}\n",
    "\n",
    "Let's try to store and retrieve all `1024` pokemon patterns into our network (though we will only show retrieval for a subset of them for computational reasons). To do this, we'll need very large values of $n$, which is bad for numeric overflow (computers don't like working in really really large numbers i.e., `inf` energy regimes).\n",
    "\n",
    "We'll implement an exponential version of the DenseAM [@demicirgil2017model] using the numerically stable `logsumexp` function. This form of the DenseAM energy was introduced by [@ramsauer2021hopfield] and is also referred to as the \"Modern Hopfield Network\" (MHN).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E_\\text{MHN}(x) &= -\\log \\sum_{\\mu=1}^K \\exp \\left(\\beta \\sum_{i=1}^D \\xi_{\\mu i} x_i\\right)\n",
    "\\end{align*}\n",
    "$${#eq-dam-energy-logsumexp}\n",
    "\n",
    "where increasing the inverse temperature $\\beta$ has a similar effect to increasing $n$ in the DenseAM polynomial energy function. Because the `log` is a monotonically increasing function, the energy minima of the original energy function are preserved, while simultaneously making the energy function more numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialDenseAM(BinaryAM):\n",
    "    Xi: jax.Array # (K, D) Memory patterns \n",
    "    beta: float = 1.0 # Temperature parameter\n",
    "\n",
    "    def energy(self, x):\n",
    "        return -jax.nn.logsumexp(self.beta * self.Xi @ x, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show larger batch retrieval\n",
    "Xi = data[:1024]\n",
    "Nshow = 255\n",
    "Xi_show = jnp.concatenate([data[eevee_pichu_idxs], jr.choice(jr.PRNGKey(10), Xi, shape=(Nshow - len(eevee_pichu_idxs),), replace=False)])\n",
    "fig1, ax1 = show_im(Xi_show, figsize=(8,8));\n",
    "ax1.set_title(f\"Random sample of {Nshow} stored patterns\")\n",
    "print(f\"Storing {Xi.shape[0]} patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-caution collapse=\"true\"}\n",
    "## Memory usage warning\n",
    "\n",
    "**Depending on your RAM availability, the following cell may crash your session**. Decrease to e.g., `nh = nw = 5` to avoid this (or upgrade your runtime on Colab for more resources).\n",
    "\n",
    "![](assets/figs/ResourceExhaustedWarning.png)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "key1, key2 = jr.split(jr.PRNGKey(3))\n",
    "nh = nw = 10\n",
    "N = nh * nw # Sample N patterns to show in grid\n",
    "x_og = jnp.concatenate([\n",
    "    data[eevee_pichu_idxs], \n",
    "    jr.choice(jr.PRNGKey(10), data, shape=(N - len(eevee_pichu_idxs),), replace=False)])\n",
    "x_noisy = flip_some_bits(key2, x_og, 0.25)\n",
    "\n",
    "mhn = ExponentialDenseAM(Xi, beta=50.)\n",
    "\n",
    "cache_name = \"logsumexp_batched\"\n",
    "keys = jr.split(key2, x_noisy.shape[0])\n",
    "npz_fname = Path(CACHE_DIR) / (cache_name + \".npz\")\n",
    "if os.path.exists(npz_fname) and CACHE_RECALL:\n",
    "    npz_data = np.load(npz_fname)\n",
    "    x_final, frames, energies = npz_data['x_final'], npz_data['frames'], npz_data['energies']\n",
    "else:\n",
    "    x_final, frames, energies = jax.vmap(ft.partial(cached_recall, nsteps=16000, save=False), in_axes=(None, None, 0,0))(mhn, cache_name, x_noisy, keys)\n",
    "    np.savez(npz_fname, x_final=x_final, frames=frames, energies=energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, what's the fun if we can't animate the retrieval process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "def show_batched_recall_output(x_og, x_noisy, x_final, energies):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    \n",
    "    # Show grids of images\n",
    "    show_im(x_noisy, axes[0]); axes[0].set_title(\"Noisy queries\") \n",
    "    show_im(x_final, axes[1]); axes[1].set_title(\"Retrieved patterns\")\n",
    "    \n",
    "    # Plot all energy curves\n",
    "    for i in range(energies.shape[0]):\n",
    "        axes[2].plot(energies[i], alpha=0.7)\n",
    "    axes[2].set_title(\"Energy during recall\")\n",
    "    axes[2].set_xlabel(\"Iteration\")\n",
    "    axes[2].set_ylabel(\"Energy\")\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "# Redundant\n",
    "# show_batched_recall_output(x_og, x_noisy, x_final, energies);\n",
    "# plt.show()\n",
    "\n",
    "def show_batched_recall_animation(frames, energies, cache_name, steps_per_sample=64):\n",
    "    # Sample frames for animation. Only gif\n",
    "    sampled_frames = frames[:, ::steps_per_sample]  # (16, n_samples)\n",
    "    sampled_energies = energies[:, ::steps_per_sample]\n",
    "    \n",
    "    images = []\n",
    "    for t in range(sampled_frames.shape[1]):\n",
    "        # Convert frame at time t to grid image\n",
    "        frame_t = sampled_frames[:, t]\n",
    "        frame_t = rearrange(frame_t, '... (h w) -> ... h w', h=pxh, w=pxw)\n",
    "        frame_grid = gridify(frame_t)\n",
    "        frame_img = ((frame_grid + 1) * 127.5).astype(np.uint8)\n",
    "        images.append(frame_img)\n",
    "    \n",
    "    fname_gif = Path(CACHE_DIR) / (cache_name + '.gif')\n",
    "    imageio.mimsave(fname_gif, images, duration=0.1, loop=0)\n",
    "    \n",
    "    return Image(filename=fname_gif, width=400), fname_gif\n",
    "\n",
    "if (Path(CACHE_DIR) / (cache_name + '.gif')).exists() and CACHE_RECALL:\n",
    "    gif_fname = str(Path(CACHE_DIR) / (cache_name + '.gif'))\n",
    "    gif = Image(filename=gif_fname, width=400)\n",
    "else:\n",
    "    gif, gif_fname = show_batched_recall_animation(frames, energies, cache_name)\n",
    "\n",
    "display(gif)\n",
    "Markdown(f\"![]({gif_fname})\")\n",
    "# Markdown(f'<img src=\"{gif_fname}\" width=\"700px\">')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "path": "nbs/tutorial/00_dense_storage.qmd"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
