A deviously simple eval library. Supports all your [favorite tasks](./minieval/tasks/).

### Quick Start

```sh
pip install minieval # we ü§ç uv
```

**CLI Usage**

```sh
# eval with a random solver
minieval -t minerva --task.limit 10

# eval with vLLM, like Qwen
minieval -m Qwen/Qwen3-4B -t minerva --task.limit 10 --model.backend vllm

# eval with an API, like GPT-4o
minieval -m gpt-4.1-nano -t gpqa --task.limit 10 --model.backend lightllm
```

---

### Usage

**Beaker Usage**

Internally at AI2, `minieval` supports Beaker using Gantry.

```sh
# eval with vLLM on Beaker
minieval \
    --model allenai/OLMo-2-0425-1B \
    --model.revision stage2-ingredient2-step23852-tokens51B \
    --task arc_challenge \
    --task.limit 10 \
    --beaker.workspace ai2/lm-eval \
    --beaker.budget ai2/oe-eval \
    --beaker.priority normal \
    --beaker.retries 3 \
    --beaker.image XXXX \
    --beaker.secrets.hf DAVIDH_HF_TOKEN \
    --beaker.secrets.openai DAVIDH_OPENAI_API_KEY \
    --beaker.secrets.aws_lambda_access_key_id LAMBDA_AWS_ACCESS_KEY_ID \
    --beaker.secrets.aws_lambda_access_key_secret LAMBDA_AWS_ACCESS_KEY_SECRET \
    --beaker.follow
```

**Python Usage**

```python
from minieval.tasks.minerva import Math500
from minieval.formats import CoT
from minieval.metrics import passAtK

from vllm import LLM, CompletionOutput, RequestOutput, SamplingParams

llm = LLM(model_path)
sampling_params = SamplingParams(
    temperature=0, 
    max_tokens=1024,
)

task = Math500() # e.g., MMLU, HumanEval
formatter = CoT() # e.g., RC, MC, BPB, CoT, Gen
metric = passAtK(ks=[1, 2, 4]) # e.g., majAtK

# build hf dataset into standard instance format
instances = task.requests

# apply chat template
messages = formatter.build_messages(instances)
requests = formatter.build_requests(messages)

# generate responses
generations = llm.generate(requests, sampling_params)

# extract answers (if applicable, e.g., CoT)
generations = task.extract_outputs(generations)

# compile responses
responses = []
for inst, msg, req, gen in zip(instances, messages, requests, generations):
    responses += [Response(input=inst, messages=msg, request=req, output=gen)]

# grade respones
instance_scores = metric.grade_responses(responses)
dataset_scores  = metric.compute_metric()
```

**Local Install**

```sh
pip install -e ".[all]"
```

```sh
# For development, run without a model to use a random solver
minieval -t minerva
```

---

### About

Design principles are based on OAI's [nanoeval](https://github.com/openai/preparedness/tree/main/project/nanoeval):

- **Minimal indirection.** You should be able to implement and understand an eval in 100 lines.
- **Separation of concerns.** Keep data loading away from completions/parsing/different ways of running an eval.
- **Fast iteration and testability.** minievals should import in less than a second and be testable without a live LLM backend.
- **High performance.** Minieval should max out the compute resources available to it.

Primitives:

- `Eval` - Enumerates a set of tasks, and (typically) uses a "Solver" to solve them and then records the results. Can be configured in code or on the CLI using a chz entrypoint.
- `EvalSpec` - An eval to run and runtime characteristics of how to run it (i.e. concurrency, recording, other administrivia)
- `Task` - A single scoreable unit of work.
- `Solver` - A strategy (usually involving sampling a model) to go from a task to a result that can be scored. For example, there may be different ways to prompt a model to answer a multiple-choice question (i.e. looking at logits, few-shot prompting, etc)